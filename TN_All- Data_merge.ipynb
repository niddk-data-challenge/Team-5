{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e5a201",
   "metadata": {},
   "source": [
    "# 0. Try approaches to find an effective way to merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b4327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1578 entries, 0 to 1577\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   header              1578 non-null   object \n",
      " 1   Missing Percentage  1578 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 24.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 262 entries, 0 to 261\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   header              262 non-null    object \n",
      " 1   Missing Percentage  262 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 4.2+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 875 entries, 0 to 874\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   header              875 non-null    object \n",
      " 1   Missing Percentage  875 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 13.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 917 entries, 0 to 916\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   header              917 non-null    object \n",
      " 1   Missing Percentage  917 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 14.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#TN_01\n",
    "tn01_col = pd.read_csv('TN_01_merged_data_summary.csv', usecols=[0, 1])\n",
    "print(tn01_col.info())\n",
    "\n",
    "#TN_16\n",
    "tn16_col = pd.read_csv('TN_16_merged_data_summary.csv', usecols=[0, 1])\n",
    "#rename the column 'Unnamed: 0'\n",
    "tn16_col.rename(columns={'Unnamed: 0': 'header'}, inplace=True)\n",
    "print(tn16_col.info())\n",
    "\n",
    "#TN_19\n",
    "tn19_col = pd.read_csv('TN_19_merged_data_summary.csv', usecols=[0, 1])\n",
    "tn19_col.rename(columns={'Unnamed: 0': 'header'}, inplace=True)\n",
    "print(tn19_col.info())\n",
    "\n",
    "#TN_20\n",
    "tn20_col = pd.read_csv('TN_20_merged_data_summary.csv', usecols=[0, 1])\n",
    "tn20_col.rename(columns={'Unnamed: 0': 'header'}, inplace=True)\n",
    "print(tn20_col.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36742203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in TN_01 with missing values less than 99%:  296\n",
      "Number of columns in TN_16 with missing values less than 99%:  205\n",
      "Number of columns in TN_19 with missing values less than 99%:  295\n",
      "Number of columns in TN_20 with missing values less than 99%:  652\n"
     ]
    }
   ],
   "source": [
    "# filter out columns with missing values larger than 99%\n",
    "#TN_01\n",
    "filtered_tn_01_col_99 = tn01_col[tn01_col['Missing Percentage'] < 99]\n",
    "# Extract the values from the first column and convert to list\n",
    "tn_01_col_99 = filtered_tn_01_col_99['header'].tolist()\n",
    "print('Number of columns in TN_01 with missing values less than 99%: ', len(tn_01_col_99))\n",
    "\n",
    "#TN_16\n",
    "filtered_tn_16_col_99 = tn16_col[tn16_col['Missing Percentage'] < 99]\n",
    "tn_16_col_99 = filtered_tn_16_col_99['header'].tolist()\n",
    "print('Number of columns in TN_16 with missing values less than 99%: ', len(tn_16_col_99))\n",
    "\n",
    "#TN_19\n",
    "filtered_tn_19_col_99 = tn19_col[tn19_col['Missing Percentage'] < 99]\n",
    "tn_19_col_99 = filtered_tn_19_col_99['header'].tolist()\n",
    "print('Number of columns in TN_19 with missing values less than 99%: ', len(tn_19_col_99))\n",
    "\n",
    "#TN_20\n",
    "filtered_tn_20_col_99 = tn20_col[tn20_col['Missing Percentage'] < 99]\n",
    "tn_20_col_99 = filtered_tn_20_col_99['header'].tolist()\n",
    "print('Number of columns in TN_20 with missing values less than 99%: ', len(tn_20_col_99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a82805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 296 entries, MaskID to LabID\n",
      "dtypes: string(296)"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "df_tn01_99 = dd.read_csv('TN_01_Merged_Data.csv', usecols=tn_01_col_99, dtype=str, assume_missing=True)\n",
    "df_tn01_99.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd02b34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 875 entries, 0 to 874\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   header              875 non-null    object \n",
      " 1   Missing Percentage  875 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 13.8+ KB\n"
     ]
    }
   ],
   "source": [
    "tn19_col.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc50bbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 205 entries, MaskID to LabID\n",
      "dtypes: string(205)"
     ]
    }
   ],
   "source": [
    "df_tn16_99 = dd.read_csv('TN_16_merged_data.csv', usecols=tn_16_col_99, dtype=str, assume_missing=True)\n",
    "df_tn16_99.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c0e142e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 295 entries, MaskID to ExperiencedAnyHypoglycemicEven\n",
      "dtypes: string(295)"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "df_tn19_99 = dd.read_csv('TN_19_merged_data.csv', usecols=tn_19_col_99, dtype=str, assume_missing=True)\n",
    "df_tn19_99.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9de7267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 652 entries, MaskID to howmanydoseshastheparticipantm\n",
      "dtypes: string(652)"
     ]
    }
   ],
   "source": [
    "df_tn20_99 = dd.read_csv('TN_20_merged_data.csv', usecols=tn_20_col_99, dtype=str, assume_missing=True)\n",
    "df_tn20_99.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feaddb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging DataFrame computed successfully.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5369309 entries, 0 to 64561\n",
      "Columns: 470 entries, MaskID to Visit_dup12\n",
      "dtypes: string(470)\n",
      "memory usage: 11.7 GB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaskID</th>\n",
       "      <th>Visit_Dt</th>\n",
       "      <th>Visit</th>\n",
       "      <th>_15Aug2011VisitType</th>\n",
       "      <th>StorageOfResidualsDuringTrialN</th>\n",
       "      <th>_15Aug2011ConsentForResidualSa</th>\n",
       "      <th>_15Aug2011ConsentDateDay</th>\n",
       "      <th>_15Aug2011ConsentDateMonth</th>\n",
       "      <th>_15Aug2011ConsentDateYear</th>\n",
       "      <th>Fall2017ConsentDateDay</th>\n",
       "      <th>...</th>\n",
       "      <th>Visit_Dt_dup9</th>\n",
       "      <th>Visit_dup9</th>\n",
       "      <th>TN16ProtocolDeviation</th>\n",
       "      <th>ProtocolDeviationOtherSubcateg</th>\n",
       "      <th>IfMiscellaneousSpecify</th>\n",
       "      <th>SubjectRandomizedInelgibilityA</th>\n",
       "      <th>Visit_Dt_dup10</th>\n",
       "      <th>Visit_dup10</th>\n",
       "      <th>SelectTheVersionOfTN16</th>\n",
       "      <th>Visit_dup12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>968661</td>\n",
       "      <td>09/04/2012</td>\n",
       "      <td>PRN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>969347</td>\n",
       "      <td>10/08/2009</td>\n",
       "      <td>PRN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>971693</td>\n",
       "      <td>12/11/2019</td>\n",
       "      <td>PRN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>972762</td>\n",
       "      <td>02/28/2019</td>\n",
       "      <td>PRN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>972892</td>\n",
       "      <td>01/22/2007</td>\n",
       "      <td>PRN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 470 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MaskID    Visit_Dt Visit _15Aug2011VisitType  \\\n",
       "0  968661  09/04/2012   PRN                <NA>   \n",
       "1  969347  10/08/2009   PRN                <NA>   \n",
       "2  971693  12/11/2019   PRN                <NA>   \n",
       "3  972762  02/28/2019   PRN                <NA>   \n",
       "4  972892  01/22/2007   PRN                <NA>   \n",
       "\n",
       "  StorageOfResidualsDuringTrialN _15Aug2011ConsentForResidualSa  \\\n",
       "0                           <NA>                           <NA>   \n",
       "1                           <NA>                           <NA>   \n",
       "2                           <NA>                           <NA>   \n",
       "3                           <NA>                           <NA>   \n",
       "4                           <NA>                           <NA>   \n",
       "\n",
       "  _15Aug2011ConsentDateDay _15Aug2011ConsentDateMonth  \\\n",
       "0                     <NA>                       <NA>   \n",
       "1                     <NA>                       <NA>   \n",
       "2                     <NA>                       <NA>   \n",
       "3                     <NA>                       <NA>   \n",
       "4                     <NA>                       <NA>   \n",
       "\n",
       "  _15Aug2011ConsentDateYear Fall2017ConsentDateDay  ... Visit_Dt_dup9  \\\n",
       "0                      <NA>                   <NA>  ...          <NA>   \n",
       "1                      <NA>                   <NA>  ...          <NA>   \n",
       "2                      <NA>                   <NA>  ...          <NA>   \n",
       "3                      <NA>                   <NA>  ...          <NA>   \n",
       "4                      <NA>                   <NA>  ...          <NA>   \n",
       "\n",
       "  Visit_dup9 TN16ProtocolDeviation ProtocolDeviationOtherSubcateg  \\\n",
       "0       <NA>                  <NA>                           <NA>   \n",
       "1       <NA>                  <NA>                           <NA>   \n",
       "2       <NA>                  <NA>                           <NA>   \n",
       "3       <NA>                  <NA>                           <NA>   \n",
       "4       <NA>                  <NA>                           <NA>   \n",
       "\n",
       "  IfMiscellaneousSpecify SubjectRandomizedInelgibilityA Visit_Dt_dup10  \\\n",
       "0                   <NA>                           <NA>           <NA>   \n",
       "1                   <NA>                           <NA>           <NA>   \n",
       "2                   <NA>                           <NA>           <NA>   \n",
       "3                   <NA>                           <NA>           <NA>   \n",
       "4                   <NA>                           <NA>           <NA>   \n",
       "\n",
       "  Visit_dup10 SelectTheVersionOfTN16 Visit_dup12  \n",
       "0        <NA>                   <NA>        <NA>  \n",
       "1        <NA>                   <NA>        <NA>  \n",
       "2        <NA>                   <NA>        <NA>  \n",
       "3        <NA>                   <NA>        <NA>  \n",
       "4        <NA>                   <NA>        <NA>  \n",
       "\n",
       "[5 rows x 470 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the common headers\n",
    "common_headers = set(df_tn01_99.columns).intersection(df_tn16_99.columns)\n",
    "\n",
    "# Merge the DataFrames\n",
    "df_tn01_tn16 = dd.merge(df_tn01_99, df_tn16_99, on=list(common_headers), how='outer', \n",
    "                        suffixes=('_dup_tn01', '_dup_tn16'))\n",
    "\n",
    "# Compute the concatenated DataFrame to get the result\n",
    "try:\n",
    "    df_tn01_tn16_merge = df_tn01_tn16.compute()\n",
    "    print(\"Merging DataFrame computed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in merging DataFrame: {e}\")\n",
    "    \n",
    "print(df_tn01_tn16_merge.info())\n",
    "df_tn01_tn16_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57ed6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d5e8189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tn01_tn16_merge.to_feather('tn01_tn16_merge.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b7848cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file.feather' with the path to your Feather file\n",
    "feather_file = 'tn01_tn16_merge.feather'\n",
    "\n",
    "# Read the Feather file into a Pandas DataFrame\n",
    "pandas_df = pd.read_feather(feather_file)\n",
    "\n",
    "# Convert the Pandas DataFrame to a Dask DataFrame\n",
    "df_tn01_tn16_merge = dd.from_pandas(pandas_df, npartitions=50) # You can adjust npartitions based on your dataset and system\n",
    "\n",
    "# Now dask_df is a Dask DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aed1518f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 470 entries, MaskID to Visit_dup12\n",
      "dtypes: string(470)"
     ]
    }
   ],
   "source": [
    "df_tn01_tn16_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e43da5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_tn01_tn16_merge.columns:\n",
    "    df_tn01_tn16_merge[column] = df_tn01_tn16_merge[column].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0e52448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 470 entries, MaskID to Visit_dup12\n",
      "dtypes: category(470)"
     ]
    }
   ],
   "source": [
    "df_tn01_tn16_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79f072a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 295 entries, MaskID to ExperiencedAnyHypoglycemicEven\n",
      "dtypes: category(295)"
     ]
    }
   ],
   "source": [
    "for column in df_tn19_99.columns:\n",
    "    df_tn19_99[column] = df_tn19_99[column].astype('category')\n",
    "df_tn19_99.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4abd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc66baab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfe5735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad88baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate the percentage of missing values\n",
    "missing_percentage = df_tn01_tn16_merge.isnull().mean() * 100\n",
    "\n",
    "# Initialize a list to hold the data for the new DataFrame\n",
    "data = []\n",
    "\n",
    "# Iterate through each column in the original DataFrame\n",
    "for column in df_tn01_tn16_merge.columns:\n",
    "    # Find the top 5 most frequent values (excluding NaN) and their counts\n",
    "    top_5_values_counts = df_tn01_tn16_merge[column].value_counts(dropna=True).head(5)\n",
    "    top_5_values = top_5_values_counts.index.tolist()\n",
    "    top_5_counts = top_5_values_counts.tolist()\n",
    "\n",
    "    # Ensure the lists have 5 elements by padding with None if necessary\n",
    "    top_5_values += [None] * (5 - len(top_5_values))\n",
    "    top_5_counts += [None] * (5 - len(top_5_counts))\n",
    "\n",
    "    # Determine the data type of the column\n",
    "    column_dtype = str(df_tn01_tn16_merge[column].dtype)\n",
    "\n",
    "    # Append the data for this column to the list\n",
    "    data.append([missing_percentage[column], column_dtype] + top_5_values + top_5_counts)\n",
    "\n",
    "# Define the columns for the new DataFrame\n",
    "columns = ['Missing Percentage', 'Value Type', \n",
    "           'Top 1', 'Top 2', 'Top 3', 'Top 4', 'Top 5',\n",
    "           'Top 1 Value Count', 'Top 2 Value Count', 'Top 3 Value Count', \n",
    "           'Top 4 Value Count', 'Top 5 Value Count']\n",
    "\n",
    "# Create the new DataFrame\n",
    "summary_df = pd.DataFrame(data, index=df_tn01_tn16_merge.columns, columns=columns)\n",
    "\n",
    "# Sort the DataFrame by 'Missing Percentage' in descending order\n",
    "summary_df = summary_df.sort_values(by='Missing Percentage', ascending=False)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "summary_df.to_csv('TN_01_16_summary.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e232bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_headers_01_16_19:  99\n"
     ]
    }
   ],
   "source": [
    "# Identify the common headers\n",
    "common_headers_01_16_19= set(df_tn01_tn16_merge.columns).intersection(df_tn19_99.columns)\n",
    "\n",
    "print('common_headers_01_16_19: ', len(common_headers_01_16_19))\n",
    "\n",
    "# Merge the DataFrames\n",
    "df_tn01_tn16_tn19 = dd.merge(df_tn01_tn16_merge, df_tn19_99, on=list(common_headers_01_16_19), how='outer', \n",
    "                        suffixes=('','_dup_tn19'))\n",
    "\n",
    "# Compute the concatenated DataFrame to get the result\n",
    "try:\n",
    "    df_tn01_tn16_tn19_merge = df_tn01_tn16_tn19.compute()\n",
    "    print(\"Merging DataFrame computed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in merging DataFrame: {e}\")\n",
    "    \n",
    "print(df_tn01_tn16_tn19_merge.info())\n",
    "df_tn01_tn16_tn19_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccfcf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tn19_99.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd2283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2968199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = 'TN_01_Merged_Data.csv'\n",
    "\n",
    "# Open the file and read the first line\n",
    "with open(file_path, 'r') as file:\n",
    "    first_line = file.readline().strip()\n",
    "\n",
    "# Split the first line into column headers\n",
    "column_headers = first_line.split(',')\n",
    "\n",
    "for i in tn_01_col_99:\n",
    "    if i not in column_headers:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fec2361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4a985b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c4f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e0a014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac78480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25590ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf9d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780edca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27266751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d5924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3504b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07981cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Names of the Excel files\n",
    "file_names = ['TN_01_column_headers.csv', 'TN_16_column_headers.csv', 'TN_19_column_headers.csv', 'TN_20_column_headers.csv']\n",
    "column_name = 'Column Header'  # Replace 'header' with the actual name of your column\n",
    "\n",
    "# Reading the specified column from each file and storing it in a list\n",
    "all_values = []\n",
    "for file in file_names:\n",
    "    df = pd.read_csv(file)\n",
    "    if column_name in df.columns:\n",
    "        all_values.extend(df[column_name].tolist())\n",
    "    else:\n",
    "        print(f\"Column '{column_name}' not found in {file}\")\n",
    "\n",
    "# Removing duplicates\n",
    "unique_values = list(set(all_values))\n",
    "\n",
    "print(len(unique_values))\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fcb206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968c4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a701ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c87be8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68872f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a0cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b9c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72e5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e95262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251e7fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d74a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c8614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656853d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2de9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9bf2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef853c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3065cc45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd95b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61bc6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eb24ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee03da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5336ea6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0354f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a45c838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8604040a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5415dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef4586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cda44c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c6eac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248f608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2924b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41eb747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286a3145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a81484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4fbe29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea80595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dad6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb946b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76df09db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4831594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b33a8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "327bc753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def prepare_and_save_to_feather(df, file_name):\n",
    "    \"\"\"\n",
    "    Prepares a DataFrame for saving to a Feather file by ensuring consistent data types\n",
    "    and handling NaN values, then saves it to the specified file.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be saved.\n",
    "    file_name (str): The name of the Feather file to save.\n",
    "    \"\"\"\n",
    "    # Ensure all columns are of type 'str' to avoid mixed types.\n",
    "    # You can adjust this logic based on your specific data type needs.\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            df[col] = df[col].astype(str)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting column {col}: {e}\")\n",
    "            # Handle specific errors or fill NaN values if necessary\n",
    "            df[col] = df[col].fillna('Unknown').astype(str)\n",
    "\n",
    "    # Try saving the DataFrame to a Feather file\n",
    "    try:\n",
    "        df.to_feather(file_name)\n",
    "        print(f\"DataFrame successfully saved to {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving DataFrame to Feather: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e403cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_01_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbce2a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d5ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4461aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3573b72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d3ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e6061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fdec3bf",
   "metadata": {},
   "source": [
    "# 1. Load merged datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1809d4",
   "metadata": {},
   "source": [
    "## 1.1 TN_01: Merging into one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a85ac",
   "metadata": {},
   "source": [
    "### 1.1.1 Data type transformation: string into category, float64 into float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f247857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# The following code is to merge the two datasets for TN_01, but will cause error or dead kernel. \n",
    "# Convert the 'ProtocolVersion' column to string in both DataFrames\n",
    "file_path_1 = \"TN_01_merged_data_1.csv\"\n",
    "file_path_2 = \"TN_01_merged_data_23.csv\"\n",
    "\n",
    "merged_first_part = pd.read_csv(file_path_1)\n",
    "merged_data_23 = pd.read_csv(file_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "303719c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully saved to TN_01_merged_data_23.feather\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "prepare_and_save_to_feather(merged_data_23, \"TN_01_merged_data_23.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad2e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "prepare_and_save_to_feather(merged_first_part, \"TN_01_merged_data_1.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8179191",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_data_23' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmerged_data_23\u001b[49m\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_data_23' is not defined"
     ]
    }
   ],
   "source": [
    "merged_data_23.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52051089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 519750 entries, 0 to 519749\n",
      "Columns: 1563 entries, MaskID to SignedConsentFor2017Protocol\n",
      "dtypes: float64(665), int64(1), object(897)\n",
      "memory usage: 6.1+ GB\n",
      "None\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1968807 entries, 0 to 1968806\n",
      "Data columns (total 17 columns):\n",
      " #   Column                         Dtype  \n",
      "---  ------                         -----  \n",
      " 0   MaskID                         int64  \n",
      " 1   Date_of_Registration           object \n",
      " 2   Status                         object \n",
      " 3   Date_of_Study_Start            object \n",
      " 4   Date_of_Draw                   object \n",
      " 5   Event_Title                    object \n",
      " 6   Spec_Name                      object \n",
      " 7   SampleMaskID                   object \n",
      " 8   Test_Name                      object \n",
      " 9   Result                         object \n",
      " 10  Result_Type                    object \n",
      " 11  Visit                          object \n",
      " 12  Date_at_Test_Results_Reported  object \n",
      " 13  Date_at_Evaluation             object \n",
      " 14  Date_Received                  object \n",
      " 15  Date_Shipped                   object \n",
      " 16  LabID                          float64\n",
      "dtypes: float64(1), int64(1), object(15)\n",
      "memory usage: 255.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(merged_first_part.info())\n",
    "print()\n",
    "print(merged_data_23.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "658146c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17883089368\n",
      "\n",
      "1802078130\n"
     ]
    }
   ],
   "source": [
    "print(merged_first_part.memory_usage(deep=True).sum())\n",
    "print()\n",
    "print(merged_data_23.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac341b0",
   "metadata": {},
   "source": [
    "##### By changing some columns from float64 to float 16. I checked the accuracy loss, which is minimal and negligible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb4bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def transform_data_types(df):\n",
    "    for column in df.columns:\n",
    "        # Check for NaN values in the column\n",
    "        has_nan = df[column].isnull().any()\n",
    "\n",
    "        if df[column].dtype == 'float64':\n",
    "            # Convert to int16 if all values are integers and there are no NaN values\n",
    "            if not has_nan and all(df[column].dropna().apply(float.is_integer)):\n",
    "                df[column] = df[column].astype('int16')\n",
    "            else:\n",
    "                # Convert to float16 otherwise\n",
    "                df[column] = df[column].astype('float16')\n",
    "\n",
    "        elif df[column].dtype == 'object':\n",
    "            # Convert string columns to category\n",
    "            if all(df[column].apply(lambda x: isinstance(x, str) or pd.isnull(x))):\n",
    "                df[column] = df[column].astype('category')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3db14310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1311209169\n",
      "\n",
      "245206343\n"
     ]
    }
   ],
   "source": [
    "merged_first_part = transform_data_types(merged_first_part)\n",
    "merged_second_part = transform_data_types(merged_data_23)\n",
    "\n",
    "print(merged_first_part.memory_usage(deep=True).sum())\n",
    "print()\n",
    "print(merged_second_part.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ade88be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 519750 entries, 0 to 519749\n",
      "Columns: 1563 entries, MaskID to SignedConsentFor2017Protocol\n",
      "dtypes: category(888), float16(665), int64(1), object(9)\n",
      "memory usage: 1.1+ GB\n",
      "None\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1968807 entries, 0 to 1968806\n",
      "Data columns (total 17 columns):\n",
      " #   Column                         Dtype   \n",
      "---  ------                         -----   \n",
      " 0   MaskID                         int64   \n",
      " 1   Date_of_Registration           category\n",
      " 2   Status                         category\n",
      " 3   Date_of_Study_Start            category\n",
      " 4   Date_of_Draw                   category\n",
      " 5   Event_Title                    category\n",
      " 6   Spec_Name                      category\n",
      " 7   SampleMaskID                   category\n",
      " 8   Test_Name                      category\n",
      " 9   Result                         object  \n",
      " 10  Result_Type                    category\n",
      " 11  Visit                          category\n",
      " 12  Date_at_Test_Results_Reported  category\n",
      " 13  Date_at_Evaluation             category\n",
      " 14  Date_Received                  category\n",
      " 15  Date_Shipped                   category\n",
      " 16  LabID                          float16 \n",
      "dtypes: category(14), float16(1), int64(1), object(1)\n",
      "memory usage: 101.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(merged_first_part.info())\n",
    "print()\n",
    "print(merged_second_part.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf7a4ab",
   "metadata": {},
   "source": [
    "### 1.2.2 Data merge into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248445da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2923848 entries, 0 to 2923847\n",
      "Columns: 1578 entries, MaskID to LabID\n",
      "dtypes: category(900), float16(666), int64(1), object(11)\n",
      "memory usage: 6.4+ GB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaskID</th>\n",
       "      <th>Visit_Dt</th>\n",
       "      <th>Visit</th>\n",
       "      <th>_22Jul2009VisitType</th>\n",
       "      <th>_15Aug2011VisitType</th>\n",
       "      <th>StorageOfResidualsDuringTrialN</th>\n",
       "      <th>_15Aug2011ConsentForResidualSa</th>\n",
       "      <th>_22Jul2009ConsentSignedDay</th>\n",
       "      <th>_22Jul2009ConsentSignedMonth</th>\n",
       "      <th>_22Jul2009ConsentSignedYear</th>\n",
       "      <th>...</th>\n",
       "      <th>Spec_Name</th>\n",
       "      <th>SampleMaskID</th>\n",
       "      <th>Test_Name</th>\n",
       "      <th>Result</th>\n",
       "      <th>Result_Type</th>\n",
       "      <th>Date_at_Test_Results_Reported</th>\n",
       "      <th>Date_at_Evaluation</th>\n",
       "      <th>Date_Received</th>\n",
       "      <th>Date_Shipped</th>\n",
       "      <th>LabID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200023</td>\n",
       "      <td>07/01/2015</td>\n",
       "      <td>PRN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rescreen/Confirmatory Screening Visit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Serum - Autoantibodies</td>\n",
       "      <td>S3827078</td>\n",
       "      <td>GAD65H</td>\n",
       "      <td>0</td>\n",
       "      <td>RPTD</td>\n",
       "      <td>07/20/2015</td>\n",
       "      <td>07/14/2015</td>\n",
       "      <td>07/14/2015</td>\n",
       "      <td>07/08/2015</td>\n",
       "      <td>547.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200023</td>\n",
       "      <td>07/01/2015</td>\n",
       "      <td>PRN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rescreen/Confirmatory Screening Visit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Serum - Autoantibodies</td>\n",
       "      <td>S3827078</td>\n",
       "      <td>MIAA</td>\n",
       "      <td>0</td>\n",
       "      <td>RPTD</td>\n",
       "      <td>07/20/2015</td>\n",
       "      <td>07/14/2015</td>\n",
       "      <td>07/14/2015</td>\n",
       "      <td>07/08/2015</td>\n",
       "      <td>547.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200023</td>\n",
       "      <td>07/01/2015</td>\n",
       "      <td>PRN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rescreen/Confirmatory Screening Visit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Serum - Autoantibodies</td>\n",
       "      <td>S3827078</td>\n",
       "      <td>IA-2H</td>\n",
       "      <td>0</td>\n",
       "      <td>RPTD</td>\n",
       "      <td>07/20/2015</td>\n",
       "      <td>07/14/2015</td>\n",
       "      <td>07/14/2015</td>\n",
       "      <td>07/08/2015</td>\n",
       "      <td>547.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200071</td>\n",
       "      <td>01/24/2018</td>\n",
       "      <td>PRN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rescreen/Confirmatory Screening Visit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200071</td>\n",
       "      <td>01/14/2019</td>\n",
       "      <td>PRN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1578 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MaskID    Visit_Dt Visit _22Jul2009VisitType  \\\n",
       "0  200023  07/01/2015   PRN                 NaN   \n",
       "1  200023  07/01/2015   PRN                 NaN   \n",
       "2  200023  07/01/2015   PRN                 NaN   \n",
       "3  200071  01/24/2018   PRN                 NaN   \n",
       "4  200071  01/14/2019   PRN                 NaN   \n",
       "\n",
       "                     _15Aug2011VisitType StorageOfResidualsDuringTrialN  \\\n",
       "0  Rescreen/Confirmatory Screening Visit                            NaN   \n",
       "1  Rescreen/Confirmatory Screening Visit                            NaN   \n",
       "2  Rescreen/Confirmatory Screening Visit                            NaN   \n",
       "3  Rescreen/Confirmatory Screening Visit                            NaN   \n",
       "4                                    NaN                            NaN   \n",
       "\n",
       "  _15Aug2011ConsentForResidualSa  _22Jul2009ConsentSignedDay  \\\n",
       "0                            Yes                         NaN   \n",
       "1                            Yes                         NaN   \n",
       "2                            Yes                         NaN   \n",
       "3                            Yes                         NaN   \n",
       "4                            NaN                         NaN   \n",
       "\n",
       "  _22Jul2009ConsentSignedMonth  _22Jul2009ConsentSignedYear  ...  \\\n",
       "0                          NaN                          NaN  ...   \n",
       "1                          NaN                          NaN  ...   \n",
       "2                          NaN                          NaN  ...   \n",
       "3                          NaN                          NaN  ...   \n",
       "4                          NaN                          NaN  ...   \n",
       "\n",
       "                Spec_Name SampleMaskID  Test_Name  Result Result_Type  \\\n",
       "0  Serum - Autoantibodies     S3827078     GAD65H       0        RPTD   \n",
       "1  Serum - Autoantibodies     S3827078       MIAA       0        RPTD   \n",
       "2  Serum - Autoantibodies     S3827078      IA-2H       0        RPTD   \n",
       "3                     NaN          NaN        NaN     NaN         NaN   \n",
       "4                     NaN          NaN        NaN     NaN         NaN   \n",
       "\n",
       "   Date_at_Test_Results_Reported Date_at_Evaluation Date_Received  \\\n",
       "0                     07/20/2015         07/14/2015    07/14/2015   \n",
       "1                     07/20/2015         07/14/2015    07/14/2015   \n",
       "2                     07/20/2015         07/14/2015    07/14/2015   \n",
       "3                            NaN                NaN           NaN   \n",
       "4                            NaN                NaN           NaN   \n",
       "\n",
       "  Date_Shipped  LabID  \n",
       "0   07/08/2015  547.0  \n",
       "1   07/08/2015  547.0  \n",
       "2   07/08/2015  547.0  \n",
       "3          NaN    NaN  \n",
       "4          NaN    NaN  \n",
       "\n",
       "[5 rows x 1578 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_headers_123 = set.intersection(set(merged_first_part.columns), set(merged_second_part.columns))\n",
    "\n",
    "# Merge the first two DataFrames on common headers using outer join\n",
    "TN_01_merged_data = pd.merge(merged_first_part, merged_second_part, on=list(common_headers_123), how='outer', \n",
    "                            suffixes=('_dup_x', '_dup_y'))\n",
    "\n",
    "print(TN_01_merged_data.info())\n",
    "TN_01_merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d20188f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TN_01_merged_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mTN_01_merged_data\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TN_01_merged_data' is not defined"
     ]
    }
   ],
   "source": [
    "TN_01_merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53df3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def prepare_and_save_to_feather(df, file_name):\n",
    "    \"\"\"\n",
    "    Prepares a DataFrame for saving to a Feather file by ensuring consistent data types\n",
    "    and handling NaN values, then saves it to the specified file.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be saved.\n",
    "    file_name (str): The name of the Feather file to save.\n",
    "    \"\"\"\n",
    "    # Ensure all columns are of type 'str' to avoid mixed types.\n",
    "    # You can adjust this logic based on your specific data type needs.\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            df[col] = df[col].astype(str)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting column {col}: {e}\")\n",
    "            # Handle specific errors or fill NaN values if necessary\n",
    "            df[col] = df[col].fillna('Unknown').astype(str)\n",
    "\n",
    "    # Try saving the DataFrame to a Feather file\n",
    "    try:\n",
    "        df.to_feather(file_name)\n",
    "        print(f\"DataFrame successfully saved to {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving DataFrame to Feather: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "prepare_and_save_to_feather(TN_01_merged_data, \"TN_01_merged_data.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39acce6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowTypeError",
     "evalue": "(\"Expected bytes, got a 'float' object\", 'Conversion failed for column TestingPlaceForAutoAntibodyOut with type object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m TN_01_merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDPT1ParticipantIDNumber\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m TN_01_merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDPT1ParticipantIDNumber\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m      3\u001b[0m TN_01_merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSelectStudyForPreventionOfT1D\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m TN_01_merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSelectStudyForPreventionOfT1D\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mTN_01_merged_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_feather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTN_01_merged_data.feather\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/frame.py:2797\u001b[0m, in \u001b[0;36mDataFrame.to_feather\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m   2769\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary Feather format.\u001b[39;00m\n\u001b[1;32m   2771\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2793\u001b[0m \u001b[38;5;124;03m>>> df.to_feather(\"file.feather\")  # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m   2794\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2795\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeather_format\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_feather\n\u001b[0;32m-> 2797\u001b[0m \u001b[43mto_feather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/io/feather_format.py:69\u001b[0m, in \u001b[0;36mto_feather\u001b[0;34m(df, path, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeather only support IO with DataFrames\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m     67\u001b[0m     path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     68\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m---> 69\u001b[0m     \u001b[43mfeather\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_feather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyarrow/feather.py:156\u001b[0m, in \u001b[0;36mwrite_feather\u001b[0;34m(df, dest, compression, compression_level, chunksize, version)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVersion value should either be 1 or 2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 156\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreserve_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Version 1 does not chunking\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(table\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mnames):\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyarrow/table.pxi:3762\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyarrow/pandas_compat.py:624\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[0;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, maybe_fut \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_fut, futures\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m--> 624\u001b[0m             arrays[i] \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_fut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    626\u001b[0m types \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyarrow/pandas_compat.py:598\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[1;32m    594\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    595\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    596\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    597\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(col\u001b[38;5;241m.\u001b[39mname, col\u001b[38;5;241m.\u001b[39mdtype),)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m field_nullable \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mnull_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mField \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m was non-nullable but pandas column \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    601\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhad \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m null values\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(field),\n\u001b[1;32m    602\u001b[0m                                                  result\u001b[38;5;241m.\u001b[39mnull_count))\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyarrow/pandas_compat.py:592\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    589\u001b[0m     type_ \u001b[38;5;241m=\u001b[39m field\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 592\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[1;32m    594\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    595\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    596\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    597\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(col\u001b[38;5;241m.\u001b[39mname, col\u001b[38;5;241m.\u001b[39mdtype),)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyarrow/array.pxi:323\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyarrow/array.pxi:83\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pyarrow/error.pxi:123\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowTypeError\u001b[0m: (\"Expected bytes, got a 'float' object\", 'Conversion failed for column TestingPlaceForAutoAntibodyOut with type object')"
     ]
    }
   ],
   "source": [
    "TN_01_merged_data['OnlineScreeningReferralSource'] = TN_01_merged_data['OnlineScreeningReferralSource'].astype(str)\n",
    "TN_01_merged_data['DPT1ParticipantIDNumber'] = TN_01_merged_data['DPT1ParticipantIDNumber'].astype(str)\n",
    "TN_01_merged_data['SelectStudyForPreventionOfT1D'] = TN_01_merged_data['SelectStudyForPreventionOfT1D'].astype(str)\n",
    "\n",
    "TN_01_merged_data.to_feather(\"TN_01_merged_data.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f78d57e",
   "metadata": {},
   "source": [
    "#### The merged dataset is too large to be saved in the working directory. We are to turn columns with numerical values and large portion of missing values into sparse data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2cf6e3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summary_df_tn_01 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerged_data_summary.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m summary_df_tn_01\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "summary_df_tn_01 = pd.read_csv('merged_data_summary.csv')\n",
    "summary_df_tn_01.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beeeca0",
   "metadata": {},
   "source": [
    "### 1.2.4 Numerical data structure transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beab085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter based on Missing Percentage and Value Type\n",
    "filtered_columns = summary_df_tn_01[(summary_df_tn_01['Missing Percentage'] > 85) & \n",
    "                             (summary_df_tn_01['Value Type'].isin(['float16', 'int64']))]\n",
    "\n",
    "# Get the list of headers from the filtered DataFrame\n",
    "headers_with_missing_85 = filtered_columns['header'].tolist()\n",
    "\n",
    "# Print the count of columns\n",
    "print(\"Count of columns with missing Percentage > 85% and Value Type 'float16' or 'int64':\", len(headers_with_missing_85))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deccfde",
   "metadata": {},
   "source": [
    "#### Tried many times and found that: 1) for columns with very high missing values, sparse data structure significantly save memory; 2) when missing value less than 85%, sparse data structure does have advantage over float16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86249d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TN_01_merged_data.info())\n",
    "print()\n",
    "print(TN_01_merged_data.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dffb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_columns = []  # List to store columns that raised errors\n",
    "\n",
    "for col in headers_with_missing_85:\n",
    "    try:\n",
    "        print(\"Before transformation of\", col, \":\", TN_01_merged_data[col].memory_usage(deep=True), \"bytes\")\n",
    "        \n",
    "        # Check data type of the column\n",
    "        col_dtype = TN_01_merged_data[col].dtype\n",
    "        print(col_dtype)\n",
    "        if col_dtype == 'float16' or col_dtype == 'int64':\n",
    "            # Convert numeric columns to default sparse type\n",
    "            TN_01_merged_data[col] = pd.to_numeric(TN_01_merged_data[col], errors='coerce').astype('Sparse[float16]')\n",
    "        # No need to replace the column in the original DataFrame\n",
    "        # Just printing out the memory usage after transformation\n",
    "        print(\"After transformation to Sparse of\", col, \":\", TN_01_merged_data[col].memory_usage(deep=True), \"bytes\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error converting column {col}: {e}\")\n",
    "        error_columns.append(col)\n",
    "\n",
    "    print()\n",
    "\n",
    "# Print out columns that had errors\n",
    "if error_columns:\n",
    "    print(\"Columns with errors:\", error_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff51b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TN_01_merged_data.info())\n",
    "print()\n",
    "print(TN_01_merged_data.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894db31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in TN_01_merged_data.columns:\n",
    "    if TN_01_merged_data[column].dtype == 'object':\n",
    "        TN_01_merged_data[column] = TN_01_merged_data[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ee916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TN_01_merged_data = TN_01_merged_data.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TN_01_merged_data.info())\n",
    "print()\n",
    "print(TN_01_merged_data.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c20f1d2",
   "metadata": {},
   "source": [
    "#### change MaskID from int64 to int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_col = TN_01_merged_data['MaskID']\n",
    "print(test_col.memory_usage(deep=True))\n",
    "print(test_col.head(30))\n",
    "print()\n",
    "test_col_16 = test_col.astype('int16')\n",
    "print(test_col_16.memory_usage(deep=True))\n",
    "print(test_col_16.head(30))\n",
    "print()\n",
    "test_col_32 = test_col.astype('Int32')\n",
    "print(test_col_32.memory_usage(deep=True))\n",
    "print(test_col_32.head(30))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "TN_01_merged_data['MaskID'] = TN_01_merged_data['MaskID'].astype('Int32')\n",
    "\n",
    "print(TN_01_merged_data.info())\n",
    "print()\n",
    "print(TN_01_merged_data.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc49e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = TN_01_merged_data['D5MeterReadingResult4_1'].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b86b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcb10bd2",
   "metadata": {},
   "source": [
    "#### turn columns as Sparse[float64] to Sparse[float16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb20a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each column and convert if it's of type Sparse[float64]\n",
    "for column in TN_01_merged_data.columns:\n",
    "    if pd.api.types.is_sparse(TN_01_merged_data[column].dtype):\n",
    "        if TN_01_merged_data[column].dtype.subtype == np.float64:\n",
    "            TN_01_merged_data[column] = TN_01_merged_data[column].astype('Sparse[float16]')\n",
    "\n",
    "print(TN_01_merged_data.info())\n",
    "print()\n",
    "print(TN_01_merged_data.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85b728",
   "metadata": {},
   "source": [
    "#### turn columns as float16 to category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540c11d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_col = TN_01_merged_data['Race_White']\n",
    "\n",
    "print(\"memory used as float16: \",test_col.memory_usage(deep=True))\n",
    "print()\n",
    "\n",
    "# Convert to float32 or float64 before converting to category\n",
    "test_col_float32 = test_col.astype('float32')\n",
    "print(\"memory used as float32: \",test_col_float32.memory_usage(deep=True))\n",
    "print()\n",
    "\n",
    "test_col_category = test_col_float32.astype('category')\n",
    "print(\"memory used as category: \", test_col_category.memory_usage(deep=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b870fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each column and convert if it's of type float16\n",
    "for column in TN_01_merged_data.columns:\n",
    "    if TN_01_merged_data[column].dtype == np.float16:\n",
    "        print(\"memory used as float16: \", TN_01_merged_data[column].memory_usage(deep=True))\n",
    "        #convert to float32 first\n",
    "        TN_01_merged_data[column] = TN_01_merged_data[column].astype('float32')\n",
    "        print(\"memory used as float32: \", TN_01_merged_data[column].memory_usage(deep=True))\n",
    "        # Convert the column to 'category'\n",
    "        TN_01_merged_data[column] = TN_01_merged_data[column].astype('category')\n",
    "        print(\"memory used as category: \", TN_01_merged_data[column].memory_usage(deep=True))\n",
    "        print()\n",
    "\n",
    "# Display DataFrame info and memory usage\n",
    "print(TN_01_merged_data.info())\n",
    "print()\n",
    "print(TN_01_merged_data.memory_usage(deep=True).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8c623",
   "metadata": {},
   "source": [
    "#### Turn SampleMaskID into integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8db06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SampleMaskID has the biggest memory\n",
    "# To turn it into interger, we need to check whether all values are starting with 'S', and the rest of values are integer.\n",
    "column = TN_01_merged_data['SampleMaskID']\n",
    "\n",
    "print(\"memory as category: \", column.memory_usage(deep=True))\n",
    "\n",
    "test_col_obj = test_col.astype('object')\n",
    "print(\"memory as object: \",test_col_obj.memory_usage(deep=True))\n",
    "\n",
    "# Filter out missing values\n",
    "non_missing_column = column[column.notna()]\n",
    "\n",
    "# Check if values start with 'S'\n",
    "starts_with_s = non_missing_column.str.startswith('S')\n",
    "\n",
    "# Check if the rest part of the values are integers\n",
    "rest_is_integer = non_missing_column.str.lstrip('S').apply(lambda x: x.isdigit())\n",
    "\n",
    "# Combine the conditions\n",
    "all_conditions_met = starts_with_s & rest_is_integer\n",
    "\n",
    "# Check if all non-missing values in the column meet the conditions\n",
    "all_values_meet_conditions = all_conditions_met.all()\n",
    "\n",
    "print(\"Do all non-missing values start with 'S' and the rest part are integers?\", all_values_meet_conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae353f5",
   "metadata": {},
   "source": [
    "##### No.1 change in column values: remove 'S' for 'SampleMaskID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b558ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip 's' from the start of each string in the column\n",
    "TN_01_merged_data['SampleMaskID'] = TN_01_merged_data['SampleMaskID'].str.lstrip('S')\n",
    "print(\"original memory usage: \", TN_01_merged_data['SampleMaskID'].memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "TN_01_merged_data['SampleMaskID'].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d463e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SexOfRelativeWithT1D14_1 as category in the dataframe with only one value: Male.  \n",
    "test_col = TN_01_merged_data['SampleMaskID'] \n",
    "\n",
    "print(\"original memory usage: \", test_col.memory_usage(deep=True))\n",
    "test_col_32 = test_col.astype('Int32')\n",
    "print(\"memory usage as Int32: \", test_col_32.memory_usage(deep=True))\n",
    "print(test_col_32.head(30))\n",
    "\n",
    "test_col_16 = test_col.astype('Int16')\n",
    "print(\"memory usage as Int16: \", test_col_16.memory_usage(deep=True))\n",
    "print(test_col_16.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce34203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column to nullable integer type Int32\n",
    "TN_01_merged_data['SampleMaskID'] = TN_01_merged_data['SampleMaskID'].astype('Int32')\n",
    "print(\"memory as Int32: \", TN_01_merged_data['SampleMaskID'].memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3f88a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display DataFrame info and memory usage\n",
    "print(TN_01_merged_data.info())\n",
    "print()\n",
    "print(TN_01_merged_data.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_values = TN_01_merged_data['SampleMaskID'].value_counts().nlargest(5)\n",
    "print(top_5_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f53fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b9e3566",
   "metadata": {},
   "source": [
    "#### turn some category into Sparse[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da401c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SexOfRelativeWithT1D14_1 as category in the dataframe with only one value: Male.  \n",
    "test_col = TN_01_merged_data['SexOfRelativeWithT1D14_1']\n",
    "\n",
    "print(test_col.memory_usage(deep=True))\n",
    "print()\n",
    "\n",
    "test_col_category = test_col.astype('Sparse[str]')\n",
    "print(test_col_category.memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af264e",
   "metadata": {},
   "source": [
    "#### It is an unefficient way to turn string values into sparse even when the column with nearly 100% missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55df392d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "642c151c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb2840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc72e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626d0fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2394bdf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3806ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8105429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef9d7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411f87a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ecb55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28c3fe52",
   "metadata": {},
   "source": [
    "#### Function to check memory usage of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming merged_data_all is your DataFrame\n",
    "\n",
    "def column_memory_usage(df):\n",
    "    # List to store memory usage and data types\n",
    "    memory_info = []\n",
    "\n",
    "    # Iterating through each column to get memory usage and data type\n",
    "    for col in df.columns:\n",
    "        memory = df[col].memory_usage(deep=True)\n",
    "        dtype = df[col].dtype\n",
    "        memory_info.append({'Column': col, 'Memory Usage (Bytes)': memory, 'Data Type': dtype})\n",
    "\n",
    "    # Create a DataFrame from the list\n",
    "    memory_df = pd.DataFrame(memory_info)\n",
    "\n",
    "    # Sorting by memory usage in descending order\n",
    "    memory_df = memory_df.sort_values(by='Memory Usage (Bytes)', ascending=False)\n",
    "\n",
    "    return memory_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d716543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and store the result\n",
    "column_memory_info = column_memory_usage(TN_01_merged_data)\n",
    "\n",
    "# Display the result\n",
    "print(column_memory_info.info())\n",
    "\n",
    "column_memory_info.to_csv('TN_01_memory_usage_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b530c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2d523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3708b021",
   "metadata": {},
   "source": [
    "## 1.2 TN_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffccbb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63b3161c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2445461 entries, 0 to 2445460\n",
      "Columns: 262 entries, MaskID to LabID\n",
      "dtypes: float64(99), object(163)\n",
      "memory usage: 4.8+ GB\n",
      "None\n",
      "19758580453\n"
     ]
    }
   ],
   "source": [
    "# The following code is to load \n",
    "# load the merged data of TN_16\n",
    "file_path_16 = \"TN_16_merged_data.csv\"\n",
    "\n",
    "TN_16_merged_data = pd.read_csv(file_path_16)\n",
    "\n",
    "print(TN_16_merged_data.info())\n",
    "print(TN_16_merged_data.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c62c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def prepare_and_save_to_feather(df, file_name):\n",
    "    \"\"\"\n",
    "    Prepares a DataFrame for saving to a Feather file by ensuring consistent data types\n",
    "    and handling NaN values, then saves it to the specified file.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be saved.\n",
    "    file_name (str): The name of the Feather file to save.\n",
    "    \"\"\"\n",
    "    # Ensure all columns are of type 'str' to avoid mixed types.\n",
    "    # You can adjust this logic based on your specific data type needs.\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            df[col] = df[col].astype(str)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting column {col}: {e}\")\n",
    "            # Handle specific errors or fill NaN values if necessary\n",
    "            df[col] = df[col].fillna('Unknown').astype(str)\n",
    "\n",
    "    # Try saving the DataFrame to a Feather file\n",
    "    try:\n",
    "        df.to_feather(file_name)\n",
    "        print(f\"DataFrame successfully saved to {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving DataFrame to Feather: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "prepare_and_save_to_feather(TN_16_merged_data, \"TN_16_merged_data.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4202a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f13e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17cbfe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1658ac0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes={'AECausalityByReviewer': 'object',\n",
    "       'AEOccurDtDay': 'float64',\n",
    "       'AEOccurDtMonth': 'float64',\n",
    "       'AEOccurDtYear': 'float64',\n",
    "       'AEReasonForFollowup': 'object',\n",
    "       'AEReportDtDay': 'float64',\n",
    "       'AEReportDtMonth': 'float64',\n",
    "       'AEReportDtYear': 'float64',\n",
    "       'AEReporterStaffCode': 'float64',\n",
    "       'AEResolveDtDay': 'float64',\n",
    "       'AEResolveDtMonth': 'float64',\n",
    "       'AEResolveDtYear': 'float64',\n",
    "       'AEReviewPriorSimilarSAEs': 'object',\n",
    "       'AEReviewReportingTimeframe': 'object',\n",
    "       'AEReviewTreatmentUnblindedForC': 'object',\n",
    "       'AdverseEventID': 'float64',\n",
    "       'AreThere3GlucoseValuesAvailabl': 'object',\n",
    "       'AssessDateDay': 'float64',\n",
    "       'AssessDateYear': 'float64',\n",
    "       'AvgAllRecordedGlucosesMmoll': 'object',\n",
    "       'AvgRecordedFastingGlucosesmmol': 'object',\n",
    "       'ConMedsFrequencyOtherDetail': 'object',\n",
    "       'ConMedsIntervalOtherDetails': 'object',\n",
    "       'DateRecordedMM1': 'object',\n",
    "       'DateRecordedMM2': 'object',\n",
    "       'DateRecordedMM3': 'object',\n",
    "       'DateStatusChangeDay': 'float64',\n",
    "       'DateStatusChangeYear': 'float64',\n",
    "       'DoWDay': 'float64',\n",
    "       'DoWYear': 'float64',\n",
    "       'Dose': 'float64',\n",
    "       'HighestGlucosemmol': 'object',\n",
    "       'IfMiscellaneousSpecify': 'object',\n",
    "       'InsulDoseInfoAvailableFor3Days': 'object',\n",
    "       'InsulinDoseLast24Hrs': 'float64',\n",
    "       'InsulinRoutineDaily': 'object',\n",
    "       'LastHbA1cDay': 'float64',\n",
    "       'LastHbA1cYear': 'float64',\n",
    "       'LowestGlucosemmoll': 'object',\n",
    "       'MMTTCollected': 'object',\n",
    "       'OGTTCollected': 'object',\n",
    "       'PInotified': 'object',\n",
    "       'ProtcolDeviationMonth': 'object',\n",
    "       'ProtocolDeviationOtherSubcateg': 'object',\n",
    "       'ProtocolVersion': 'object',\n",
    "       'RequiredSpecimensCollected': 'object',\n",
    "       'Result': 'object',\n",
    "       'StartDateDay': 'float64',\n",
    "       'StartDateYear': 'float64',\n",
    "       'StopDateMonth': 'object',\n",
    "       'SubjectRandomizedInelgibilityA': 'object',\n",
    "       'SubjectUsingCGMS': 'object',\n",
    "       'TN16ProtocolDeviation': 'object',\n",
    "       'Visit_Dt_dup1': 'object',\n",
    "       'Visit_Dt_dup9': 'object',\n",
    "       'Visit_dup1': 'object',\n",
    "       'Visit_dup9': 'object',\n",
    "       'WeightKg': 'float64',\n",
    "       'severity': 'float64',\n",
    "       'AECategory': 'object',\n",
    "       'LabID': 'float64',\n",
    "       'Eventabateafterstoppingdrug': 'object',\n",
    "       'Date_of_Study_Start': 'object',\n",
    "        'AECausalityByReporter' : 'object',\n",
    "        'Current_Site_Number' : 'object',\n",
    "        'ReasonForWithdrawal': 'object',\n",
    "        'Eventreappearafterreintroducti' : 'object',\n",
    "        'AETerm' : 'object',\n",
    "        'AEExpected1_1' : 'object',\n",
    "        'AEPrimarySecondary1_1' : 'object',\n",
    "        'AEReportType' : 'object',\n",
    "        'AEPatientOutcome1_1': 'object',\n",
    "        'WillingToBeContacted': 'object',\n",
    "        'Old_Site_Number': 'object',\n",
    "        'Date_of_Registration': 'object',\n",
    "        'ChangeOfStatusOccuring': 'object',\n",
    "        'AETreatLocation': 'object',\n",
    "        'DateStatusChangeMonth': 'object',\n",
    "        'Reason_for_Transfer': 'object',\n",
    "        'Status': 'object',\n",
    "        'DoWMonth': 'object'    \n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a15ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype={'AEAdditionalInfoRequested': 'object',\n",
    "       'AESerious': 'object',\n",
    "       'AETreatLocationOther': 'object',\n",
    "       'AddisonsDiseaseWithinLast6Months': 'object',\n",
    "       'AddisonsDiseaseWithinLast6Months_dup16': 'object',\n",
    "       'AdmissionMonth': 'object',\n",
    "       'AllergiesWithinLast6Months': 'object',\n",
    "       'AllergiesWithinLast6Months_dup16': 'object',\n",
    "       'AlopeciaWithinLast6Months': 'object',\n",
    "       'AlopeciaWithinLast6Months_dup16': 'object',\n",
    "       'AlphaGlucosidaseInhibitors': 'object',\n",
    "       'AlphaGlucosidaseInhibitors_dup16': 'object',\n",
    "       'AnionGapMonth': 'object',\n",
    "       'AnionGapUnits': 'object',\n",
    "       'AntibodySampleCollected_dup15': 'object',\n",
    "       'AntibodySampleCollected_dup16': 'object',\n",
    "       'AsthmaWithinLast6Months': 'object',\n",
    "       'AsthmaWithinLast6Months_dup16': 'object',\n",
    "       'BasophilsResultsNormal': 'object',\n",
    "       'BetaBlockers_dup15': 'object',\n",
    "       'BetaBlockers_dup16': 'object',\n",
    "       'BiweeklyInjectionVisitSchedule': 'object',\n",
    "       'BllodForDNASampleCollected_dup15': 'object',\n",
    "       'BllodForDNASampleCollected_dup16': 'object',\n",
    "       'BloodForCellsSampleCollected_dup15': 'object',\n",
    "       'BloodForCellsSampleCollected_dup16': 'object',\n",
    "       'BloodForRNASampleCollected_dup15': 'object',\n",
    "       'BloodForRNASampleCollected_dup16': 'object',\n",
    "       'CancerWithinLast6Months': 'object',\n",
    "       'CancerWithinLast6Months_dup16': 'object',\n",
    "       'CeliacDiseaseWithinLast6Months': 'object',\n",
    "       'CeliacDiseaseWithinLast6Months_dup16': 'object',\n",
    "       'CentralIRBOrlocalIRB': 'object',\n",
    "       'CentralIRBOrlocalIRB_dup26': 'object',\n",
    "       'ColitisOrColonProblemsWithinLa_dup15': 'object',\n",
    "       'ColitisOrColonProblemsWithinLa_dup16': 'object',\n",
    "       'ConcernsAboutReceivingPlaceboR': 'object',\n",
    "       'ConflictingResponsibilitiesRan': 'object',\n",
    "       'CongenitalHeartDiseaseWithinLa_dup15': 'object',\n",
    "       'CongenitalHeartDiseaseWithinLa_dup16': 'object',\n",
    "       'ConsenToContinuedStorageOfBloo_dup15': 'object',\n",
    "       'ConsenToContinuedStorageOfBloo_dup16': 'object',\n",
    "       'Corticosteroids_dup15': 'object',\n",
    "       'Corticosteroids_dup16': 'object',\n",
    "       'CriteriaParticipantDiagnosewit': 'object',\n",
    "       'CurrentPhase': 'object',\n",
    "       'D1FastingPlasmaGlucMM1_1': 'object',\n",
    "       'D1FastingPlasmaGlucUnits1_1': 'object',\n",
    "       'D2b2HourGlucMM1_1': 'object',\n",
    "       'D2b2HourGlucUnits1_1': 'object',\n",
    "       'D3RandomPlasmaGlucMM1_1': 'object',\n",
    "       'D3RandomPlasmaGlucUnits1_1': 'object',\n",
    "       'D4HbA1cMM1_1': 'object',\n",
    "       'D5MeterReadingLoc1_1': 'object',\n",
    "       'D5MeterReadingMM1_1': 'object',\n",
    "       'D5MeterReadingUnit1_1': 'object',\n",
    "       'D5OtherTestResultMM1_1': 'object',\n",
    "       'D5OtherTestResultMM2_1': 'object',\n",
    "       'D6OtherTestResultUnit2_1': 'object',\n",
    "       'DateCompletionMM': 'object',\n",
    "       'DateConsentSignedMonth_dup15': 'object',\n",
    "       'DateControlBecameAutoantibodyP2': 'object',\n",
    "       'DateOfBloodDrawMonth': 'object',\n",
    "       'DateParticipantwascontacted1_2': 'object',\n",
    "       'DateReportedMonth': 'object',\n",
    "       'DateSignedNEWInformedConsentFo2_dup15': 'object',\n",
    "       'DateSignedNEWInformedConsentFo2_dup16': 'object',\n",
    "       'DateofAssessmentMM': 'object',\n",
    "       'DatestudyendpointmetMM': 'object',\n",
    "       'DiagnosisBy': 'object',\n",
    "       'DiagnosisByOthersSpecify': 'object',\n",
    "       'DiagnosisMonth': 'object',\n",
    "       'Didtheparticipantindicatetheym': 'object',\n",
    "       'Diphenylhydantoin_dup15': 'object',\n",
    "       'Diphenylhydantoin_dup16': 'object',\n",
    "       'DischargeMonth': 'object',\n",
    "       'DoRMonth': 'object',\n",
    "       'DoesNotTolerateOGTTRank': 'object',\n",
    "       'DoesTheParticipantWishToContin': 'object',\n",
    "       'E1aDKA': 'object',\n",
    "       'E1bDateOfOnsetMM': 'object',\n",
    "       'E1bPolyuria': 'object',\n",
    "       'E1cDateofOnsetMM': 'object',\n",
    "       'E1cPolydipsia': 'object',\n",
    "       'E1dPolyphagia': 'object',\n",
    "       'E1eUnexplainedWeightLoss': 'object',\n",
    "       'EosinophilsResultsNormal': 'object',\n",
    "       'EpilepsyConvulsionsSeizuresWit_dup15': 'object',\n",
    "       'EpilepsyConvulsionsSeizuresWit_dup16': 'object',\n",
    "       'F1DateStudyEndpointMetMM': 'object',\n",
    "       'F2ClinicalDiagnosisDateMM': 'object',\n",
    "       'FamilyWishesToWaitUntilOlderRa': 'object',\n",
    "       'Fatigue': 'object',\n",
    "       'FatigueMonth': 'object',\n",
    "       'FearOfStudyDrugRisksRank': 'object',\n",
    "       'FinalContactAttempt1_1': 'object',\n",
    "       'Followupstatuscode': 'object',\n",
    "       'GlucoseMonth1_1': 'object',\n",
    "       'GlucoseMonth2_1': 'object',\n",
    "       'GlucoseMonth3_1': 'object',\n",
    "       'GlucoseMonth4_1': 'object',\n",
    "       'GlucoseType1_1': 'object',\n",
    "       'GlucoseType2_1': 'object',\n",
    "       'GlucoseType3_1': 'object',\n",
    "       'GlucoseType4_1': 'object',\n",
    "       'H1OtherSymptoms': 'object',\n",
    "       'H2OvernightHospitalization': 'object',\n",
    "       'H3AdmissionToICU': 'object',\n",
    "       'H4DKA': 'object',\n",
    "       'H5StartedInsulin': 'object',\n",
    "       'Hastheparticipantbeentestedfor': 'object',\n",
    "       'Hastheparticipantbeentoahealth': 'object',\n",
    "       'Hastheparticipantexperiencedvo': 'object',\n",
    "       'Hastheparticipanthadacough': 'object',\n",
    "       'Hastheparticipanthadafever': 'object',\n",
    "       'Hastheparticipanthadalossreduc': 'object',\n",
    "       'Hastheparticipanthadanydifficu': 'object',\n",
    "       'Hastheparticipanthadanypersist': 'object',\n",
    "       'Hastheparticipanthadchillsasso': 'object',\n",
    "       'Hastheparticipanthadknownconta': 'object',\n",
    "       'Hastheparticipanthadknownconta2': 'object',\n",
    "       'Hasthesubjectexperiencedblurre': 'object',\n",
    "       'Hasthesubjectexperiencedpolydy': 'object',\n",
    "       'Hasthesubjectexperiencedpolyur': 'object',\n",
    "       'Hasthesubjectexperienceduninte': 'object',\n",
    "       'Hasthesubjectprovidedcontinued': 'object',\n",
    "       'Havetherebeenanychangesinthepa': 'object',\n",
    "       'HbA1cMonth': 'object',\n",
    "       'HbA1cSamplesCollected_dup15': 'object',\n",
    "       'HbA1cSamplesCollected_dup16': 'object',\n",
    "       'Height': 'object',\n",
    "       'Height_dup16': 'object',\n",
    "       'HematocritResultsNormal': 'object',\n",
    "       'HemoglobinResultAbormalClinica': 'object',\n",
    "       'HemoglobinResultsNormal': 'object',\n",
    "       'HepatitisOrLiverDiseaseWithinL_dup15': 'object',\n",
    "       'HepatitisOrLiverDiseaseWithinL_dup16': 'object',\n",
    "       'HighBPwithinLast6Months': 'object',\n",
    "       'HighBPwithinLast6Months_dup16': 'object',\n",
    "       'HighCholesterolWithinLast6Months': 'object',\n",
    "       'HighCholestrolWithinLast6Months': 'object',\n",
    "       'HospitalizedAtDiagnosisTime': 'object',\n",
    "       'IfYesOutcomeofCOVID19Test': 'object',\n",
    "       'IfyesDateCOVID19TestAdminister2': 'object',\n",
    "       'ImmunosuppressiveTherapies_dup15': 'object',\n",
    "       'ImmunosuppressiveTherapies_dup16': 'object',\n",
    "       'IndicateTheStudyParticipantWas': 'object',\n",
    "       'IndicateTheStudyParticipantWas_dup9': 'object',\n",
    "       'InfectiousMononucleosisWithinL_dup15': 'object',\n",
    "       'InfectiousMononucleosisWithinL_dup16': 'object',\n",
    "       'InflammatoryBowelDiseaseWithin_dup15': 'object',\n",
    "       'InflammatoryBowelDiseaseWithin_dup16': 'object',\n",
    "       'Informationgatheredfrom': 'object',\n",
    "       'InsulinTreatmentStartedMonth': 'object',\n",
    "       'IsParticipantCurrentlyEligible': 'object',\n",
    "       'IsParticipantPregnant': 'object',\n",
    "       'IsParticipantPregnant_dup16': 'object',\n",
    "       'IsTheParticipantInterested': 'object',\n",
    "       'Istheparticipantinterestedinth': 'object',\n",
    "       'Ketoacidosis': 'object',\n",
    "       'LimitedLifeExpectancyOrImmunos': 'object',\n",
    "       'LimitedLifeExpectancyOrImmunos_dup16': 'object',\n",
    "       'LymphocytesResultsNormal': 'object',\n",
    "       'MCHCResultAbormalClinicallySig': 'object',\n",
    "       'MCHCresultsNormal': 'object',\n",
    "       'MCHResultAbormalClinicallySign': 'object',\n",
    "       'MCHresultsNormal': 'object',\n",
    "       'MCVResultAbormalClinicallySign': 'object',\n",
    "       'MCVresultsNormal': 'object',\n",
    "       'MeasuredBy1_1': 'object',\n",
    "       'MeasuredBy2_1': 'object',\n",
    "       'MeasuredBy3_1': 'object',\n",
    "       'MeasuredBy4_1': 'object',\n",
    "       'MechanisticSampleCollectionMon_dup15': 'object',\n",
    "       'MechanisticSampleCollectionMon_dup16': 'object',\n",
    "       'Meglinitides': 'object',\n",
    "       'Meglitinides': 'object',\n",
    "       'Metformin': 'object',\n",
    "       'Metformin_dup16': 'object',\n",
    "       'MethodofContact': 'object',\n",
    "       'MonocytesResultAbormalClinical': 'object',\n",
    "       'MonocytesResultsNormal': 'object',\n",
    "       'MonthlyInfusionVisitScheduleRa': 'object',\n",
    "       'MultipleSclerosisWithinLast6Mo': 'object',\n",
    "       'MultipleSclerosisWithinLast6Mo_dup16': 'object',\n",
    "       'NeutrophilsResultsNormal': 'object',\n",
    "       'Niacin_dup15': 'object',\n",
    "       'Niacin_dup16': 'object',\n",
    "       'NotificationStatus': 'object',\n",
    "       'OGTTSampleCollected_dup15': 'object',\n",
    "       'OGTTSampleCollected_dup16': 'object',\n",
    "       'OtherAutoimmuneDiseasesWithinL_dup15': 'object',\n",
    "       'OtherAutoimmuneDiseasesWithinL_dup16': 'object',\n",
    "       'OtherGlucoseLoweringAgents': 'object',\n",
    "       'OtherGlucoseLoweringAgents_dup16': 'object',\n",
    "       'OtherRank': 'object',\n",
    "       'ParticipantAgreeToContinue': 'object',\n",
    "       'ParticipantContacted': 'object',\n",
    "       'ParticipantDiagnosedWithT1D_dup15': 'object',\n",
    "       'ParticipantDiagnosedWithT1D_dup16': 'object',\n",
    "       'ParticipantExpectedToContinue': 'object',\n",
    "       'ParticipantPregnant_dup15': 'object',\n",
    "       'ParticipantPregnant_dup16': 'object',\n",
    "       'ParticipantUsedInsulinOrOtherG_dup15': 'object',\n",
    "       'ParticipantUsedInsulinOrOtherG_dup16': 'object',\n",
    "       'ParticipantWasContactedByOther': 'object',\n",
    "       'ParticipantWillingToBeContacte': 'object',\n",
    "       'Participantisahotleadwarmleado': 'object',\n",
    "       'PermissionForSampleStorage_dup15': 'object',\n",
    "       'PerniciousAnemiaWithinLast6Mo': 'object',\n",
    "       'PerniciousAnemiaWithinLast6Mo_dup16': 'object',\n",
    "       'PlasmaGlucoseMonth': 'object',\n",
    "       'PlasmaGlucoseUnits': 'object',\n",
    "       'PlasmaSampleCollected_dup15': 'object',\n",
    "       'PlasmaSampleCollected_dup16': 'object',\n",
    "       'PlateletCountResultsNormal2': 'object',\n",
    "       'Pleaseselecttypeofcontactattem': 'object',\n",
    "       'Polydipsia': 'object',\n",
    "       'PolydipsiaMonth': 'object',\n",
    "       'Polyphagia': 'object',\n",
    "       'PolyphagiaMonth': 'object',\n",
    "       'Polyuria': 'object',\n",
    "       'PolyuriaMonth': 'object',\n",
    "       'PotassiumDepletingDiurectics_dup15': 'object',\n",
    "       'PotassiumDepletingDiurectics_dup16': 'object',\n",
    "       'PregnancyCompletionMonth': 'object',\n",
    "       'ProtocolDeviation': 'object',\n",
    "       'PsoriasisWithinLast6Months': 'object',\n",
    "       'PsoriasisWithinLast6Months_dup16': 'object',\n",
    "       'RBCResultAbormalClinicallySign': 'object',\n",
    "       'RBCResultsNormal': 'object',\n",
    "       'RecommendConsentChange': 'object',\n",
    "       'RecommendProtocolChange': 'object',\n",
    "       'RheumatologicWithinLast6Months': 'object',\n",
    "       'RheumatologicWithinLast6Months_dup16': 'object',\n",
    "       'SamplesCollectionMonth_dup15': 'object',\n",
    "       'SamplesCollectionMonth_dup16': 'object',\n",
    "       'SerumForProteomicsSampleCollec_dup15': 'object',\n",
    "       'SerumForProteomicsSampleCollec_dup16': 'object',\n",
    "       'SerumMechanisticSerumSampleCol': 'object',\n",
    "       'SignedMonitoringConsentProtoco': 'object',\n",
    "       'SignedMonitoringConsentProtoco_dup16': 'object',\n",
    "       'SiteInvestigatorAckStudyEndPoi': 'object',\n",
    "       'Spring2019ConsentDateMonth': 'object',\n",
    "       'TN01ParticipantStatusSectionB': 'object',\n",
    "       'TN16TN19TN20EligibilityStatus': 'object',\n",
    "       'TN18Exclusion12bDoesthiscondit': 'object',\n",
    "       'Thiazolidinediones': 'object',\n",
    "       'Thiazolidinediones_dup16': 'object',\n",
    "       'ThyroidDiseaseWithinLast6Months': 'object',\n",
    "       'ThyroidDiseaseWithinLast6Months_dup16': 'object',\n",
    "       'TimeCommitmentRank': 'object',\n",
    "       'TypeofTestAdministered': 'object',\n",
    "       'UlcerWithinLast6Months': 'object',\n",
    "       'UlcerWithinLast6Months_dup16': 'object',\n",
    "       'UnableUnwillingToTravelRank': 'object',\n",
    "       'UnexplainedWeightLoss': 'object',\n",
    "       'Units1_1': 'object',\n",
    "       'Units2_1': 'object',\n",
    "       'Units3_1': 'object',\n",
    "       'Units4_1': 'object',\n",
    "       'UnwillingToTakeInvestigational': 'object',\n",
    "       'UrineKetonesMonth': 'object',\n",
    "       'Visit_Type_dup15': 'object',\n",
    "       'VistMissed': 'object',\n",
    "       'VitiligoWithinLast6Months': 'object',\n",
    "       'VitiligoWithinLast6Months_dup16': 'object',\n",
    "       'WBCresultsNormal': 'object',\n",
    "       'WasTheParticipantNotified': 'object',\n",
    "       'WastheCOVID19diagnosissuspecte': 'object',\n",
    "       'Wastheparticipanthospitalizedd': 'object',\n",
    "       'WastheparticipantplacedintheIC': 'object',\n",
    "       'Wastheparticipantplacedonavent': 'object',\n",
    "       'Wastheparticipanttreatedwithme': 'object',\n",
    "       'Weight': 'object',\n",
    "       'Weight_dup16': 'object',\n",
    "       'Whatwastheseverity': 'object',\n",
    "       'Whendidthecoughoccur': 'object',\n",
    "       'WhendidthesesymptomsOccur': 'object',\n",
    "       'WhichProtocolVersion2': 'object',\n",
    "       'WholeBloodPBMCPlasmaSampleColl': 'object',\n",
    "       'WillingToContinueAfterPregnanc': 'object',\n",
    "       '_22Jul2009ConsentSignedMonth': 'object',\n",
    "       '_22Jul2009ScreeningInformedCon': 'object',\n",
    "       '_22Jul2009VisitType': 'object',\n",
    "       '_2WeekIfusionPeriodRank': 'object',\n",
    "       '_2aIfyes': 'object',\n",
    "       '_2dMailedLetter': 'object',\n",
    "       '_3aIfyes': 'object',\n",
    "       '_8Hastheparticipantbeendiagnos': 'object',\n",
    "       'centralIRBscriteria': 'object',\n",
    "       'centralIRBscriteria_dup26': 'object',\n",
    "       'f1availableWeightMM': 'object',\n",
    "       'f1availableWeightUnits': 'object',\n",
    "       'f2availableWeightMM': 'object',\n",
    "       'f2availableWeightUnits': 'object',\n",
    "       'hyperglycemia': 'object',\n",
    "       'hyperglycemia_dup16': 'object',\n",
    "       'immunomodulatory': 'object',\n",
    "       'immunomodulatory_dup16': 'object',\n",
    "       'participantdiagnosedT1D': 'object',\n",
    "       'participantdiagnosedT1D_dup16': 'object',\n",
    "       'systemicsteroids': 'object',\n",
    "       'systemicsteroids_dup16': 'object',\n",
    "       'testName1_1': 'object',\n",
    "       'testName2_1': 'object',\n",
    "       'willingToConsiderAtALaterTime': 'object'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e28e8cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "ddf_16 = dd.read_csv(\"TN_16_merged_data.csv\", dtype=str, assume_missing=True)\n",
    "\n",
    "# pick the first 1 million rows\n",
    "#ddf_16_1m = ddf_16.head(n=1000000, npartitions=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b80488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1ae627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 262 entries, MaskID to LabID\n",
      "dtypes: string(262)"
     ]
    }
   ],
   "source": [
    "\n",
    "ddf_16.to_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "212cbdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_01 = dd.read_csv(\"TN_01_Merged_Data.csv\", dtype=str, assume_missing=True)\n",
    "# pick the first 1 million rows\n",
    "#ddf_01_1m = ddf_01.head(n=1000000, npartitions=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e85fd45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ddf_01' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mddf_01\u001b[49m\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ddf_01' is not defined"
     ]
    }
   ],
   "source": [
    "ddf_01.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb703e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e16ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ff4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f4325d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39a9ae2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmerged_result\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_result' is not defined"
     ]
    }
   ],
   "source": [
    "merged_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7295882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Assuming ddf_01 and ddf_16 are your original Dask DataFrames\n",
    "\n",
    "# Convert all columns in both DataFrames to dtype 'object'\n",
    "ddf_01_obj = ddf_01.astype(str)\n",
    "ddf_16_obj = ddf_16.astype(str)\n",
    "\n",
    "concatenated_ddf = dd.concat([ddf_01_obj, ddf_16_obj], axis=0)\n",
    "\n",
    "# Compute the concatenated DataFrame to get the result\n",
    "try:\n",
    "    concatenated_result = concatenated_ddf.compute()\n",
    "    print(\"Concatenated DataFrame computed successfully.\")\n",
    "    print(concatenated_result.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error computing the concatenated DataFrame: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0725feff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a580ccf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741428ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38191a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f378d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b0710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a046ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ce3e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59606475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a89012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common columns between the two DataFrames\n",
    "common_columns = set(ddf_01.columns).intersection(set(ddf_16.columns))\n",
    "print(len(common_columns))\n",
    "print(common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6717a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200eea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_column = list(common_columns)\n",
    "\n",
    "# Merge the datasets using an outer join on the common columns\n",
    "merged_ddf = dd.merge(ddf_01, ddf_16, on=common_column, how='outer')\n",
    "\n",
    "# Compute the merged DataFrame to get the result\n",
    "merged_result = merged_ddf.compute()\n",
    "\n",
    "# Show the first few rows of the merged DataFrame\n",
    "print(merged_result.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c8106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43707daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825d1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ea50a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a5f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d851c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa1a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ddc92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb145a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a073674",
   "metadata": {},
   "source": [
    "#### Change some columns from object to category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa3f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def transform_data_types(df):\n",
    "    for column in df.columns:\n",
    "        # Check for NaN values in the column\n",
    "        has_nan = df[column].isnull().any()\n",
    "\n",
    "        if df[column].dtype == 'float64':\n",
    "            # Convert to int16 if all values are integers and there are no NaN values\n",
    "            if not has_nan and all(df[column].dropna().apply(float.is_integer)):\n",
    "                df[column] = df[column].astype('int16')\n",
    "            else:\n",
    "                # Convert to float16 otherwise\n",
    "                df[column] = df[column].astype('float16')\n",
    "\n",
    "        elif df[column].dtype == 'object':\n",
    "            # Convert string columns to category\n",
    "            if all(df[column].apply(lambda x: isinstance(x, str) or pd.isnull(x))):\n",
    "                df[column] = df[column].astype('category')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TN_16_merged_data = transform_data_types(TN_16_merged_data)\n",
    "\n",
    "print(TN_16_merged_data.info())\n",
    "print(TN_16_merged_data.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb44ab",
   "metadata": {},
   "source": [
    "#### change some columns from float64 to sparse[float16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97852f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df_16 = pd.read_csv('TN_16_merged_data_summary.csv')\n",
    "summary_df_16.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d70dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the column 'Unnamed: 0'\n",
    "summary_df_16.rename(columns={'Unnamed: 0': 'header'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d89530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter based on Missing Percentage and Value Type\n",
    "filtered_columns = summary_df_16[(summary_df_16['Missing Percentage'] > 85) & \n",
    "                             (summary_df_16['Value Type'].isin(['float64']))]\n",
    "\n",
    "# Get the list of headers from the filtered DataFrame\n",
    "tn_16_headers_with_missing_85 = filtered_columns['header'].tolist()\n",
    "\n",
    "# Print the count of columns\n",
    "print(\"Count of columns with missing Percentage > 85% and Value Type 'float64':\", len(tn_16_headers_with_missing_85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2d19fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_columns = []  # List to store columns that raised errors\n",
    "\n",
    "for col in tn_16_headers_with_missing_85:\n",
    "    try:\n",
    "        print(\"Before transformation of\", col, \":\", TN_16_merged_data[col].memory_usage(deep=True), \"bytes\")\n",
    "        \n",
    "        # Check data type of the column\n",
    "        col_dtype = TN_16_merged_data[col].dtype\n",
    "        print(col_dtype)\n",
    "        if col_dtype == 'float16' or col_dtype == 'float64':\n",
    "            # Convert numeric columns to default sparse type\n",
    "            TN_16_merged_data[col] = pd.to_numeric(TN_16_merged_data[col], errors='coerce').astype('Sparse[float16]')\n",
    "        # No need to replace the column in the original DataFrame\n",
    "        # Just printing out the memory usage after transformation\n",
    "        print(\"After transformation to Sparse of\", col, \":\", TN_16_merged_data[col].memory_usage(deep=True), \"bytes\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error converting column {col}: {e}\")\n",
    "        error_columns.append(col)\n",
    "\n",
    "    print()\n",
    "\n",
    "# Print out columns that had errors\n",
    "if error_columns:\n",
    "    print(\"Columns with errors:\", error_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aef8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \n",
    "test_col = TN_16_merged_data['LowestGlucosemgdl'] \n",
    "\n",
    "print(\"original memory usage: \", test_col.memory_usage(deep=True))\n",
    "test_col_32 = test_col.astype('Sparse[float16]')\n",
    "print(\"memory usage as Int32: \", test_col_32.memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e640b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TN_16_merged_data.info())\n",
    "print(TN_16_merged_data.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ae47f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and store the result\n",
    "column_memory_tn_16= column_memory_usage(TN_16_merged_data)\n",
    "\n",
    "# Display the result\n",
    "print(column_memory_tn_16.info())\n",
    "\n",
    "column_memory_tn_16.to_csv('TN_16_memory_usage_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b0ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c1f1d45",
   "metadata": {},
   "source": [
    "#### change MaskID into int32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe425520",
   "metadata": {},
   "source": [
    "##### There are some misfilled values in MaskID columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50738a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a value can be converted to a numeric type\n",
    "def is_not_numeric(val):\n",
    "    try:\n",
    "        float(val)  # Try converting to float\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return True\n",
    "\n",
    "# Apply the function to filter out non-numeric values\n",
    "MaskID_non_numeric_values = TN_16_merged_data['MaskID'][TN_16_merged_data['MaskID'].apply(is_not_numeric)]\n",
    "print(MaskID_non_numeric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679d7ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a value is not an integer\n",
    "def is_not_integer(val):\n",
    "    try:\n",
    "        float_val = float(val)  # Convert to float first\n",
    "        int_val = int(float_val)\n",
    "        return float_val != int_val  # Check if float value is not equal to its integer conversion\n",
    "    except ValueError:\n",
    "        return True  # Non-numeric values are also considered as not integer\n",
    "\n",
    "# Apply the function to filter out non-integer values\n",
    "MaskID_non_integer_values = TN_16_merged_data['MaskID'][TN_16_merged_data['MaskID'].apply(is_not_integer)]\n",
    "print(MaskID_non_integer_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de6fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TN_16_merged_data.info())\n",
    "TN_16_merged_data = TN_16_merged_data.drop(MaskID_non_integer_values.index)\n",
    "print(TN_16_merged_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8850c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming TN_16_merged_data is your DataFrame and 'MaskID' is the column you're working with\n",
    "test_col = TN_16_merged_data['MaskID']\n",
    "\n",
    "print(test_col.memory_usage(deep=True))\n",
    "print(test_col.head(30))\n",
    "\n",
    "# Convert to numeric, coercing errors to NaN\n",
    "test_col_numeric = pd.to_numeric(test_col, errors='coerce')\n",
    "test_col_32 = test_col_numeric.astype('Int32')\n",
    "\n",
    "print(test_col_32.memory_usage(deep=True))\n",
    "print(test_col_32.head(30))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c7ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TN_16_merged_data.memory_usage(deep=True).sum())\n",
    "\n",
    "test_col_numeric = pd.to_numeric(TN_16_merged_data['MaskID'], errors='coerce')\n",
    "TN_16_merged_data['MaskID'] = test_col_numeric.astype('Int32')\n",
    "\n",
    "print(TN_16_merged_data.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f7ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TN_16_merged_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8409bc3",
   "metadata": {},
   "source": [
    "#### Change columns as object into category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f9c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \n",
    "test_col = TN_16_merged_data['AssessDateYear'] \n",
    "\n",
    "print(\"original memory usage as object: \", test_col.memory_usage(deep=True))\n",
    "test_col_category = test_col.astype('category')\n",
    "print(\"memory usage as category: \", test_col_category.memory_usage(deep=True))\n",
    "\n",
    "test_col_numeric = pd.to_numeric(test_col, errors='coerce')\n",
    "test_col_16 = test_col_numeric.astype('Int16')\n",
    "print(\"memory usage as int16: \", test_col_16.memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f3d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in TN_16_merged_data.columns:\n",
    "    if TN_16_merged_data[column].dtype == 'object':\n",
    "        print(\"original memory usage as object: \", TN_16_merged_data[column].memory_usage(deep=True))\n",
    "        TN_16_merged_data[column] = TN_16_merged_data[column].astype('category')\n",
    "        print(\"memory usage as category: \", TN_16_merged_data[column].memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e48035",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TN_16_merged_data.memory_usage(deep=True).sum())\n",
    "TN_16_merged_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6941a76",
   "metadata": {},
   "source": [
    "#### Turn Sparse[float64] into Sparse[float16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fa25d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each column and convert if it's of type Sparse[float64]\n",
    "for column in TN_16_merged_data.columns:\n",
    "    if pd.api.types.is_sparse(TN_16_merged_data[column].dtype):\n",
    "        if TN_16_merged_data[column].dtype.subtype == np.float64:\n",
    "            TN_16_merged_data[column] = TN_16_merged_data[column].astype('Sparse[float16]')\n",
    "\n",
    "print(TN_16_merged_data.info())\n",
    "print()\n",
    "print(TN_16_merged_data.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223fe8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa78f17c",
   "metadata": {},
   "source": [
    "#### Turn SampleMaskID into int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0ae13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SampleMaskID has the biggest memory\n",
    "# To turn it into interger, we need to check whether all values are starting with 'S', and the rest of values are integer.\n",
    "column = TN_16_merged_data['SampleMaskID']\n",
    "\n",
    "print(\"memory as category: \", column.memory_usage(deep=True))\n",
    "\n",
    "test_col_obj = test_col.astype('object')\n",
    "print(\"memory as object: \",test_col_obj.memory_usage(deep=True))\n",
    "\n",
    "# Filter out missing values\n",
    "non_missing_column = column[column.notna()]\n",
    "\n",
    "# Check if values start with 'S'\n",
    "starts_with_s = non_missing_column.str.startswith('S')\n",
    "\n",
    "# Check if the rest part of the values are integers\n",
    "rest_is_integer = non_missing_column.str.lstrip('S').apply(lambda x: x.isdigit())\n",
    "\n",
    "# Combine the conditions\n",
    "all_conditions_met = starts_with_s & rest_is_integer\n",
    "\n",
    "# Check if all non-missing values in the column meet the conditions\n",
    "all_values_meet_conditions = all_conditions_met.all()\n",
    "\n",
    "print(\"Do all non-missing values start with 'S' and the rest part are integers?\", all_values_meet_conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbacc56c",
   "metadata": {},
   "source": [
    "##### No.1 change in column values: remove 'S' for 'SampleMaskID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e02b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip 's' from the start of each string in the column\n",
    "TN_16_merged_data['SampleMaskID'] = TN_16_merged_data['SampleMaskID'].str.lstrip('S')\n",
    "print(\"original memory usage: \", TN_16_merged_data['SampleMaskID'].memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463ef1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column to nullable integer type Int32\n",
    "TN_16_merged_data['SampleMaskID'] = TN_16_merged_data['SampleMaskID'].astype('Int32')\n",
    "print(\"memory as Int32: \", TN_16_merged_data['SampleMaskID'].memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f15d1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4a1b77e",
   "metadata": {},
   "source": [
    "#### turn float16 into category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b94e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each column and convert if it's of type float16\n",
    "for column in TN_16_merged_data.columns:\n",
    "    if TN_16_merged_data[column].dtype == np.float16:\n",
    "        print(\"memory used as float16: \", TN_16_merged_data[column].memory_usage(deep=True))\n",
    "        #convert to float32 first\n",
    "        TN_16_merged_data[column] = TN_16_merged_data[column].astype('float32')\n",
    "        print(\"memory used as float32: \", TN_16_merged_data[column].memory_usage(deep=True))\n",
    "        # Convert the column to 'category'\n",
    "        TN_16_merged_data[column] = TN_16_merged_data[column].astype('category')\n",
    "        print(\"memory used as category: \", TN_16_merged_data[column].memory_usage(deep=True))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b3229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display DataFrame info and memory usage\n",
    "print(TN_16_merged_data.info())\n",
    "print()\n",
    "print(TN_16_merged_data.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475bd19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bc74a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b818023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c346d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b89e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc80141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034cd54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_columns = TN_16_merged_data.columns.intersection(TN_01_merged_data.columns)\n",
    "print(len(shared_columns))\n",
    "print(shared_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for inconsistencies in data types and only report those\n",
    "inconsistencies = {}\n",
    "for col in shared_columns:\n",
    "    dtype_df1 = TN_01_merged_data[col].dtype\n",
    "    dtype_df2 = TN_16_merged_data[col].dtype\n",
    "\n",
    "    if dtype_df1 != dtype_df2:\n",
    "        print(f\"Column '{col}' has inconsistent data types: {dtype_df1} (DataFrame 1) != {dtype_df2} (DataFrame 2)\")\n",
    "    else:\n",
    "        print(f\"Column '{col}' has consistent data type: {dtype_df1}\")\n",
    "\n",
    "# Print only the columns with inconsistent data types\n",
    "for col, dtypes in inconsistencies.items():\n",
    "    print(f\"Column '{col}' has inconsistent data types: {dtypes[0]} (DataFrame 1) vs {dtypes[1]} (DataFrame 2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b264f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a328477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_columns = TN_16_merged_data.columns.intersection(TN_01_merged_data.columns)\n",
    "print(len(shared_columns))\n",
    "print(shared_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a3034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a91f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to convert float16 columns to float32\n",
    "def convert_float16_to_float32(df):\n",
    "    float16_cols = df.select_dtypes(include=['float16']).columns\n",
    "    for col in float16_cols:\n",
    "        df[col] = df[col].astype('float32')\n",
    "    return df\n",
    "\n",
    "# Convert float16 columns to float32 for both DataFrames\n",
    "TN_01_merged_data_float32 = convert_float16_to_float32(TN_01_merged_data)\n",
    "TN_16_merged_data_float32 = convert_float16_to_float32(TN_16_merged_data)\n",
    "\n",
    "# Find common headers\n",
    "common_headers_01_16 = set.intersection(set(TN_01_merged_data_float32.columns), set(TN_16_merged_data_float32.columns))\n",
    "\n",
    "# Merge the DataFrames on common headers using outer join\n",
    "merged_data_01_16 = pd.merge(TN_01_merged_data_float32, TN_16_merged_data_float32, on=list(common_headers_01_16), how='outer', \n",
    "                            suffixes=('_dup_01', '_dup_16'))\n",
    "\n",
    "print(merged_data_01_16.info())\n",
    "merged_data_01_16.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed3482",
   "metadata": {},
   "outputs": [],
   "source": [
    "TN_01_merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b804185",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_headers_01_16 = set.intersection(set(TN_01_merged_data.columns), set(TN_16_merged_data.columns))\n",
    "\n",
    "# Merge the first two DataFrames on common headers using outer join\n",
    "merged_data_01_16 = pd.merge(TN_01_merged_data, TN_16_merged_data, on=list(common_headers_01_16), how='outer', \n",
    "                            suffixes=('_duplication_x', '_duplication_y'))\n",
    "\n",
    "print(merged_data_01_16.info())\n",
    "merged_data_01_16.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c6630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb805dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc490f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6496a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f01fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f2dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07cd9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4acbc69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d3841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935946bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4b349b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f44f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa3966e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7035021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e75819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e7f496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc220bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f92339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d97a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b95652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462e07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3991041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb63a965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff13d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6777f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907d67b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e9010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1b6edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5272a753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26773ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9554eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d32449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff472a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f3952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72197fae",
   "metadata": {},
   "source": [
    "## 1.3 TN_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ef5df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e4c66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc15dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336438db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee70dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6083b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af80d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb784d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d7638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d0cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33df8734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edad7477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07741563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf9739a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8bd18c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427a807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f53f5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e7afa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b7a053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ff2700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768284f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f25c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a8cdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1660ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f495333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b33d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eaade5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e42d6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a10d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fa67c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af236ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7300e1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e55ffad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a3567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46cfe72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba6faf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4657f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c83b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02354f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28449d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ee54055",
   "metadata": {},
   "source": [
    "## 1.4 TN_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d732a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9a6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9999b6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf4ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb7380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d09004d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef2005d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa02a857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7540ca36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad22bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424af488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c958725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59911fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c5bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00edbcfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad62f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a45a608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde6aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636309c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561de3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526336d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da5f417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836520cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a984f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a9959f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b8529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34ae3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c7fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63958e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd208f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39962f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba35b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558490b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f556c25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e2bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3371bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69631633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f6306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "236d4f01",
   "metadata": {},
   "source": [
    "#### test of merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5ae980",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"D5OtherTestResultDD\"  # change this to the string you're searching for\n",
    "# Convert the search string to lower case (or upper case)\n",
    "search_string_lower = string.lower()\n",
    "\n",
    "# Find columns that contain the search string, case-insensitive\n",
    "matching_columns = [col for col in transformed_merged_data.columns if search_string_lower in col.lower()]\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "filtered_df = transformed_merged_data[matching_columns].dropna(how='all')\n",
    "print(\"DataFrame with filtered columns (rows with all missing values dropped):\")\n",
    "print()\n",
    "print(filtered_df.info())\n",
    "#print(filtered_df.head(30))\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "for col in matching_columns:\n",
    "    print(f\"Top 20 values for column {col}:\")\n",
    "    print(transformed_merged_data[col].value_counts().nlargest(30))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41362f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fac697",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.read_csv('merged_first_part_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31309d69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777775a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb8998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c3131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff86229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96878128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c5c69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800ca53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_23.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_23['Date_of_Registration'] = pd.to_datetime(merged_data_23['Date_of_Registration'], format='%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_23.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_23.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5181ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930be531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique value counts. \n",
    "unique_counts = merged_data_23.nunique()\n",
    "unique_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5075e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8c8a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0f5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540fadae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331418d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699e326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c7f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e53f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14ea147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68239f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0bcd1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea9821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e878a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7850ab48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9cb0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2070292f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd66bb49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f0055de",
   "metadata": {},
   "source": [
    "# 1.4 Data query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb254312",
   "metadata": {},
   "source": [
    "## 1.4.1 check column names and their values containing a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is to load the first part of merged dataset\n",
    "import pandas as pd\n",
    "from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "merged_first_part = pd.read_csv('TN_01_merged_data_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc61a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7ac3486",
   "metadata": {},
   "source": [
    "Strings in columns that could be merged:\n",
    "1. testName: 23\n",
    "2. \n",
    "3. D5OtherTestResultDD, D5OtherTestResultMM, D5OtherTestResultYYYY: 3*23\n",
    "4. \n",
    "5. \n",
    "6. \n",
    "7. \n",
    "8. Heightcm, heightin\n",
    "9. Weightkg, Weightlbs\n",
    "10.'TimeOfCollectionHH', 'TimeOfCollectionMM'\n",
    "11.\n",
    "12.\n",
    "13. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_first_part.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3893b928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9adb46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_first_part.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18df9017",
   "metadata": {},
   "source": [
    "#### 1) Case insensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d84de",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"D6OtherTestResultUnit\"  # change this to the string you're searching for\n",
    "# Convert the search string to lower case (or upper case)\n",
    "search_string_lower = string.lower()\n",
    "\n",
    "# Find columns that contain the search string, case-insensitive\n",
    "matching_columns = [col for col in merged_first_part.columns if search_string_lower in col.lower()]\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "filtered_df = merged_first_part[matching_columns].dropna(how='all')\n",
    "print(\"DataFrame with filtered columns (rows with all missing values dropped):\")\n",
    "print()\n",
    "print(filtered_df.info())\n",
    "#print(filtered_df.head(30))\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "for col in matching_columns:\n",
    "    print(f\"Top 20 values for column {col}:\")\n",
    "    print(merged_first_part[col].value_counts().nlargest(30))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c83ff6",
   "metadata": {},
   "source": [
    "#### 2) Case sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48e80ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"D5OtherTestResult\"  # Change this to the string you're searching for\n",
    "\n",
    "# Find columns that contain the search string, case-sensitive\n",
    "matching_columns = [col for col in merged_first_part.columns if string in col]\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "filtered_df = merged_first_part[matching_columns].dropna(how='all')\n",
    "print(\"DataFrame with filtered columns (rows with all missing values dropped):\")\n",
    "print(filtered_df.info())\n",
    "#print(filtered_df.head(30))\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "for col in matching_columns:\n",
    "    print(f\"Top 20 values for column {col}:\")\n",
    "    print(merged_first_part[col].value_counts().nlargest(20))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac3a908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74ffddc9",
   "metadata": {},
   "source": [
    "## 1.4.2 check column names and their values containing two strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2753f51",
   "metadata": {},
   "source": [
    "Filtered columns:\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5b8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strings to search for: AND\n",
    "string1 = \"\"  # First string\n",
    "string2 = \"heightin\"  # Second string, change this to your second string\n",
    "\n",
    "# Convert the search strings to lower case\n",
    "search_string1_lower = string1.lower()\n",
    "search_string2_lower = string2.lower()\n",
    "\n",
    "# Find columns that contain both search strings, case-insensitive\n",
    "matching_columns = [col for col in merged_first_part.columns \n",
    "                    if search_string1_lower in col.lower() and search_string2_lower in col.lower()]\n",
    "\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "for col in matching_columns:\n",
    "    print(f\"Top 20 values for column {col}:\")\n",
    "    print(merged_first_part[col].value_counts().nlargest(20))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd4d2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf85f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strings to search for: OR\n",
    "string1 = \"heightcm\"  # First string\n",
    "string2 = \"heightin\"  # Second string, change this to your second string\n",
    "\n",
    "# Convert the search strings to lower case\n",
    "search_string1_lower = string1.lower()\n",
    "search_string2_lower = string2.lower()\n",
    "\n",
    "# Find columns that contain both search strings, case-insensitive\n",
    "matching_columns = [col for col in merged_first_part.columns \n",
    "                    if search_string1_lower in col.lower() or search_string2_lower in col.lower()]\n",
    "\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "filtered_df = merged_first_part[matching_columns].dropna(how='all')\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "for col in matching_columns:\n",
    "    print(f\"Top 20 values for column {col}:\")\n",
    "    print(merged_first_part[col].value_counts().nlargest(20))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b80c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f545f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0a88bc6",
   "metadata": {},
   "source": [
    "## 1.4.3 check column headers in one shared CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e1fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
    "\n",
    "#file_path = \".././studies/TN_01/TrialNet_01_Data/TN01_ADVERSEEVENTS.CSV\"\n",
    "file_path = \"/home/ec2-user/SageMaker/studies/TN_01/TrialNet_01_Data/TN01_nh08_diabonset_current.csv\"\n",
    "\n",
    "df1 = pd.read_csv(file_path)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7613e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"MM\"  # change this to the string you're searching for\n",
    "# Convert the search string to lower case (or upper case)\n",
    "search_string_lower = string.lower()\n",
    "\n",
    "# Find columns that contain the search string, case-insensitive\n",
    "matching_columns = [col for col in df1.columns if search_string_lower in col.lower()]\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "\n",
    "matching_columns2 = ['MaskID', 'Visit_Dt'] + matching_columns\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "filtered_df = df1[matching_columns2].dropna(how='all', subset=matching_columns)\n",
    "print(\"DataFrame with filtered columns (rows with all missing values dropped):\")\n",
    "#print(filtered_df.info())\n",
    "#print(filtered_df.head(30))\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "for col in matching_columns:\n",
    "    print(f\"Top 20 values for column {col}:\")\n",
    "    print(df1[col].value_counts().nlargest(20))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6159cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the selected rows\n",
    "filtered_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e6100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "#filtered_df.to_csv('D5MeterReading_TN01_nh08_diabonset_current.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d2cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe based on the condition\n",
    "filtered_rows = filtered_df[filtered_df['D5MeterReadingLoc1_1'] == 'Unknown']\n",
    "\n",
    "# Display the first 20 rows of the filtered dataframe\n",
    "filtered_rows.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7cd6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f30c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eebb3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
