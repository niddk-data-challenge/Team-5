{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f89be239",
   "metadata": {},
   "source": [
    "# 1. Date merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016a3809",
   "metadata": {},
   "source": [
    "## 1.1 Date merging in TN01_nh08_diabonset_current"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6687a9c2",
   "metadata": {},
   "source": [
    "1. All data are from \"TN01_nh08_diabonset_current.csv\".\n",
    "2. Collected headers with DD indicating day, MM indicating month, and YYYY indicating year; 58 sets in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada7196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaskID</th>\n",
       "      <th>Visit_Dt</th>\n",
       "      <th>Visit</th>\n",
       "      <th>CriteriaParticipantDiagnosewit</th>\n",
       "      <th>D1FastingPlasmaGlucDD1_1</th>\n",
       "      <th>D1FastingPlasmaGlucMM1_1</th>\n",
       "      <th>D1FastingPlasmaGlucYYYY1_1</th>\n",
       "      <th>D1FastingPlasmaGlucResult1_1</th>\n",
       "      <th>D1FastingPlasmaGlucUnits1_1</th>\n",
       "      <th>D1FastingPlasmaGlucDD2_1</th>\n",
       "      <th>...</th>\n",
       "      <th>D5OtherTestResultYYYY14_1</th>\n",
       "      <th>D5OtherTestResultYYYY15_1</th>\n",
       "      <th>D5OtherTestResultYYYY16_1</th>\n",
       "      <th>D5OtherTestResultYYYY17_1</th>\n",
       "      <th>D5OtherTestResultYYYY18_1</th>\n",
       "      <th>D5OtherTestResultYYYY19_1</th>\n",
       "      <th>D5OtherTestResultYYYY20_1</th>\n",
       "      <th>D5OtherTestResultYYYY21_1</th>\n",
       "      <th>D5OtherTestResultYYYY22_1</th>\n",
       "      <th>D5OtherTestResultYYYY23_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202411</td>\n",
       "      <td>11/06/2018</td>\n",
       "      <td>PRN</td>\n",
       "      <td>b.DKA or unequivocally symptomatic AND one dia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202455</td>\n",
       "      <td>10/08/2019</td>\n",
       "      <td>PRN</td>\n",
       "      <td>e.Diagnosis made by criteria other than listed...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>mmol/L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203313</td>\n",
       "      <td>05/19/2020</td>\n",
       "      <td>PRN</td>\n",
       "      <td>e.Diagnosis made by criteria other than listed...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>203899</td>\n",
       "      <td>09/15/2020</td>\n",
       "      <td>PRN</td>\n",
       "      <td>b.DKA or unequivocally symptomatic AND one dia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>207308</td>\n",
       "      <td>02/04/2019</td>\n",
       "      <td>PRN</td>\n",
       "      <td>a.Two consecutive OGTT clinical alerts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 312 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MaskID    Visit_Dt Visit  \\\n",
       "0  202411  11/06/2018   PRN   \n",
       "1  202455  10/08/2019   PRN   \n",
       "2  203313  05/19/2020   PRN   \n",
       "3  203899  09/15/2020   PRN   \n",
       "4  207308  02/04/2019   PRN   \n",
       "\n",
       "                      CriteriaParticipantDiagnosewit  \\\n",
       "0  b.DKA or unequivocally symptomatic AND one dia...   \n",
       "1  e.Diagnosis made by criteria other than listed...   \n",
       "2  e.Diagnosis made by criteria other than listed...   \n",
       "3  b.DKA or unequivocally symptomatic AND one dia...   \n",
       "4             a.Two consecutive OGTT clinical alerts   \n",
       "\n",
       "   D1FastingPlasmaGlucDD1_1 D1FastingPlasmaGlucMM1_1  \\\n",
       "0                       NaN                      NaN   \n",
       "1                      16.0                      Sep   \n",
       "2                       NaN                      NaN   \n",
       "3                       NaN                      NaN   \n",
       "4                       NaN                      NaN   \n",
       "\n",
       "   D1FastingPlasmaGlucYYYY1_1  D1FastingPlasmaGlucResult1_1  \\\n",
       "0                         NaN                           NaN   \n",
       "1                      2019.0                          11.6   \n",
       "2                         NaN                           NaN   \n",
       "3                         NaN                           NaN   \n",
       "4                         NaN                           NaN   \n",
       "\n",
       "  D1FastingPlasmaGlucUnits1_1  D1FastingPlasmaGlucDD2_1  ...  \\\n",
       "0                         NaN                       NaN  ...   \n",
       "1                      mmol/L                       NaN  ...   \n",
       "2                         NaN                       NaN  ...   \n",
       "3                         NaN                       NaN  ...   \n",
       "4                         NaN                       NaN  ...   \n",
       "\n",
       "  D5OtherTestResultYYYY14_1  D5OtherTestResultYYYY15_1  \\\n",
       "0                       NaN                        NaN   \n",
       "1                       NaN                        NaN   \n",
       "2                       NaN                        NaN   \n",
       "3                       NaN                        NaN   \n",
       "4                       NaN                        NaN   \n",
       "\n",
       "   D5OtherTestResultYYYY16_1 D5OtherTestResultYYYY17_1  \\\n",
       "0                        NaN                       NaN   \n",
       "1                        NaN                       NaN   \n",
       "2                        NaN                       NaN   \n",
       "3                        NaN                       NaN   \n",
       "4                        NaN                       NaN   \n",
       "\n",
       "   D5OtherTestResultYYYY18_1 D5OtherTestResultYYYY19_1  \\\n",
       "0                        NaN                       NaN   \n",
       "1                        NaN                       NaN   \n",
       "2                        NaN                       NaN   \n",
       "3                        NaN                       NaN   \n",
       "4                        NaN                       NaN   \n",
       "\n",
       "   D5OtherTestResultYYYY20_1  D5OtherTestResultYYYY21_1  \\\n",
       "0                        NaN                        NaN   \n",
       "1                        NaN                        NaN   \n",
       "2                        NaN                        NaN   \n",
       "3                        NaN                        NaN   \n",
       "4                        NaN                        NaN   \n",
       "\n",
       "  D5OtherTestResultYYYY22_1  D5OtherTestResultYYYY23_1  \n",
       "0                       NaN                        NaN  \n",
       "1                       NaN                        NaN  \n",
       "2                       NaN                        NaN  \n",
       "3                       NaN                        NaN  \n",
       "4                       NaN                        NaN  \n",
       "\n",
       "[5 rows x 312 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
    "\n",
    "#file_path = \".././studies/TN_01/TrialNet_01_Data/TN01_ADVERSEEVENTS.CSV\"\n",
    "file_path = \"/home/ec2-user/SageMaker/studies/TN_01/TrialNet_01_Data/TN01_nh08_diabonset_current.csv\"\n",
    "\n",
    "df1 = pd.read_csv(file_path)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1986ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  0 columns:  []\n",
      "DataFrame with filtered columns (rows with all missing values dropped):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 0 entries\n",
      "Empty DataFrame\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#CASE SENSITIVE\n",
    "string = \"Time\"  # Change this to the string you're searching for\n",
    "#missmatch of E1aDateOfOnsetYYYY and E1aDateofOnsetYYYY\n",
    "\n",
    "# Find columns that contain the search string, case-sensitive\n",
    "matching_columns = [col for col in df1.columns if string in col]\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "filtered_df = df1[matching_columns].dropna(how='all')\n",
    "print(\"DataFrame with filtered columns (rows with all missing values dropped):\")\n",
    "print(filtered_df.info())\n",
    "#print(filtered_df.head(30))\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "for col in matching_columns:\n",
    "    print(f\"Top 20 values for column {col}:\")\n",
    "    print(df1[col].value_counts().nlargest(20))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd634754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174 columns: ['D1FastingPlasmaGlucDD1_1', 'D1FastingPlasmaGlucMM1_1', 'D1FastingPlasmaGlucYYYY1_1', 'D1FastingPlasmaGlucDD2_1', 'D1FastingPlasmaGlucMM2_1', 'D1FastingPlasmaGlucYYYY2_1', 'D1FastingPlasmaGlucDD3_1', 'D1FastingPlasmaGlucMM3_1', 'D1FastingPlasmaGlucYYYY3_1', 'D2aOGTTFastingGlucDD1_1', 'D2aOGTTFastingGlucMM1_1', 'D2aOGTTFastingGlucYYYY1_1', 'D2aOGTTFastingGlucDD2_1', 'D2aOGTTFastingGlucMM2_1', 'D2aOGTTFastingGlucYYYY2_1', 'D2b2HourGlucDD1_1', 'D2b2HourGlucMM1_1', 'D2b2HourGlucYYYY1_1', 'D2b2HourGlucDD2_1', 'D2b2HourGlucMM2_1', 'D2b2HourGlucYYYY2_1', 'D3RandomPlasmaGlucDD1_1', 'D3RandomPlasmaGlucMM1_1', 'D3RandomPlasmaGlucYYYY1_1', 'D4HbA1cDD1_1', 'D4HbA1cMM1_1', 'D4HbA1cYYYY1_1', 'D4HbA1cDD2_1', 'D4HbA1cMM2_1', 'D4HbA1cYYYY2_1', 'D5MeterReadingDD1_1', 'D5MeterReadingMM1_1', 'D5MeterReadingYYYY1_1', 'D5MeterReadingDD2_1', 'D5MeterReadingMM2_1', 'D5MeterReadingYYYY2_1', 'D5MeterReadingDD3_1', 'D5MeterReadingMM3_1', 'D5MeterReadingYYYY3_1', 'D5MeterReadingDD4_1', 'D5MeterReadingMM4_1', 'D5MeterReadingYYYY4_1', 'D5OtherTestResultDD1_1', 'D5OtherTestResultMM1_1', 'D5OtherTestResultYYYY1_1', 'D5OtherTestResultDD2_1', 'D5OtherTestResultMM2_1', 'D5OtherTestResultYYYY2_1', 'D5OtherTestResultDD3_1', 'D5OtherTestResultMM3_1', 'D5OtherTestResultYYYY3_1', 'D5OtherTestResultDD4_1', 'D5OtherTestResultMM4_1', 'D5OtherTestResultYYYY4_1', 'D5OtherTestResultDD5_1', 'D5OtherTestResultMM5_1', 'D5OtherTestResultYYYY5_1', 'E1aDateOfOnsetDD', 'E1aDateOfOnsetMM', 'E1aDateofOnsetYYYY', 'E1bDateOfOnsetDD', 'E1bDateOfOnsetMM', 'E1bDateOfOnsetYYYY', 'E1cDateofOnsetDD', 'E1cDateofOnsetMM', 'E1cDateofOnsetYYYY', 'E1dDateOfOnsetDD', 'E1dDateOfOnsetMM', 'E1dDateOfOnsetYYYY', 'f1availableWeightDD', 'f1availableWeightMM', 'f1availableWeightYYYY', 'f2availableWeightDD', 'f2availableWeightMM', 'f2availableWeightYYYY', 'f3availableWeightDD', 'f3availableWeightMM', 'f3availableWeightYYYY', 'F1DateStudyEndpointMetDD', 'F1DateStudyEndpointMetMM', 'F1DateStudyEndpointMetYYYY', 'F2ClinicalDiagnosisDateDD', 'F2ClinicalDiagnosisDateMM', 'F2ClinicalDiagnosisDateYYYY', 'DatestudyendpointmetDD', 'DatestudyendpointmetMM', 'DatestudyendpointmetYYYY', 'D2aFastingPlasmaGlucMM3_1', 'D2b2HourGlucMM3_1', 'D3RandomPlasmaGlucMM2_1', 'D3RandomPlasmaGlucMM3_1', 'D3RandomPlasmaGlucMM4_1', 'D3RandomPlasmaGlucMM5_1', 'D4HbA1cMM3_1', 'D4HbA1cMM4_1', 'D5MeterReadingMM5_1', 'D5MeterReadingMM6_1', 'D5MeterReadingMM7_1', 'D5OtherTestResultMM6_1', 'D5OtherTestResultMM7_1', 'D5OtherTestResultMM8_1', 'D5OtherTestResultMM9_1', 'D5OtherTestResultMM10_1', 'D5OtherTestResultMM11_1', 'D5OtherTestResultMM12_1', 'D5OtherTestResultMM13_1', 'D5OtherTestResultMM14_1', 'D5OtherTestResultMM15_1', 'D5OtherTestResultMM16_1', 'D5OtherTestResultMM17_1', 'D5OtherTestResultMM18_1', 'D5OtherTestResultMM19_1', 'D5OtherTestResultMM20_1', 'D5OtherTestResultMM21_1', 'D5OtherTestResultMM22_1', 'D5OtherTestResultMM23_1', 'D2aFastingPlasmaGlucDD3_1', 'D2aFastingPlasmaGlucYYYY3_1', 'D2b2HourGlucDD3_1', 'D2b2HourGlucYYYY3_1', 'D3RandomPlasmaGlucDD2_1', 'D3RandomPlasmaGlucDD3_1', 'D3RandomPlasmaGlucDD4_1', 'D3RandomPlasmaGlucDD5_1', 'D3RandomPlasmaGlucYYYY2_1', 'D3RandomPlasmaGlucYYYY3_1', 'D3RandomPlasmaGlucYYYY4_1', 'D3RandomPlasmaGlucYYYY5_1', 'D4HbA1cDD3_1', 'D4HbA1cDD4_1', 'D4HbA1cYYYY3_1', 'D4HbA1cYYYY4_1', 'D5MeterReadingDD5_1', 'D5MeterReadingDD6_1', 'D5MeterReadingDD7_1', 'D5MeterReadingYYYY5_1', 'D5MeterReadingYYYY6_1', 'D5MeterReadingYYYY7_1', 'D5OtherTestResultDD6_1', 'D5OtherTestResultDD7_1', 'D5OtherTestResultDD8_1', 'D5OtherTestResultDD9_1', 'D5OtherTestResultDD10_1', 'D5OtherTestResultDD11_1', 'D5OtherTestResultDD12_1', 'D5OtherTestResultDD13_1', 'D5OtherTestResultDD14_1', 'D5OtherTestResultDD15_1', 'D5OtherTestResultDD16_1', 'D5OtherTestResultDD17_1', 'D5OtherTestResultDD18_1', 'D5OtherTestResultDD19_1', 'D5OtherTestResultDD20_1', 'D5OtherTestResultDD21_1', 'D5OtherTestResultDD22_1', 'D5OtherTestResultDD23_1', 'D5OtherTestResultYYYY6_1', 'D5OtherTestResultYYYY7_1', 'D5OtherTestResultYYYY8_1', 'D5OtherTestResultYYYY9_1', 'D5OtherTestResultYYYY10_1', 'D5OtherTestResultYYYY11_1', 'D5OtherTestResultYYYY12_1', 'D5OtherTestResultYYYY13_1', 'D5OtherTestResultYYYY14_1', 'D5OtherTestResultYYYY15_1', 'D5OtherTestResultYYYY16_1', 'D5OtherTestResultYYYY17_1', 'D5OtherTestResultYYYY18_1', 'D5OtherTestResultYYYY19_1', 'D5OtherTestResultYYYY20_1', 'D5OtherTestResultYYYY21_1', 'D5OtherTestResultYYYY22_1', 'D5OtherTestResultYYYY23_1']\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 426 entries, 0 to 425\n",
      "Columns: 176 entries, MaskID to D5OtherTestResultYYYY23_1\n",
      "dtypes: float64(116), int64(1), object(59)\n",
      "memory usage: 585.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Filter all columns with \"DD\", \"MM\", or \"YYYY\"\n",
    "search_strings = [\"DD\", \"MM\", \"YYYY\"]  # List of strings to search for\n",
    "\n",
    "# Find columns that contain any of the search strings\n",
    "matching_columns = [col for col in df1.columns if any(string in col for string in search_strings)]\n",
    "print(\"Found\", len(matching_columns), \"columns:\", matching_columns)\n",
    "\n",
    "matching_columns2 = ['MaskID', 'Visit_Dt'] + matching_columns\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "filtered_df = df1[matching_columns2].dropna(how='all', subset=matching_columns)\n",
    "print()\n",
    "\n",
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb3f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct headers of columns that did not follow the naming rules.\n",
    "filtered_df.rename(columns={'E1aDateofOnsetYYYY': 'E1aDateOfOnsetYYYY'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64778714",
   "metadata": {},
   "source": [
    "#### If the value of the day is missing while the month and the year values are present, impute the day value as 15 (middle of the month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec02e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 426 entries, 0 to 425\n",
      "Columns: 234 entries, MaskID to D5OtherTestResultDate23_1\n",
      "dtypes: float64(116), int64(1), object(117)\n",
      "memory usage: 778.9+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n",
      "/tmp/ipykernel_33883/3406821247.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[date_col] = df.apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaskID</th>\n",
       "      <th>Visit_Dt</th>\n",
       "      <th>D1FastingPlasmaGlucDD1_1</th>\n",
       "      <th>D1FastingPlasmaGlucMM1_1</th>\n",
       "      <th>D1FastingPlasmaGlucYYYY1_1</th>\n",
       "      <th>D1FastingPlasmaGlucDD2_1</th>\n",
       "      <th>D1FastingPlasmaGlucMM2_1</th>\n",
       "      <th>D1FastingPlasmaGlucYYYY2_1</th>\n",
       "      <th>D1FastingPlasmaGlucDD3_1</th>\n",
       "      <th>D1FastingPlasmaGlucMM3_1</th>\n",
       "      <th>...</th>\n",
       "      <th>D5OtherTestResultDate14_1</th>\n",
       "      <th>D5OtherTestResultDate15_1</th>\n",
       "      <th>D5OtherTestResultDate16_1</th>\n",
       "      <th>D5OtherTestResultDate17_1</th>\n",
       "      <th>D5OtherTestResultDate18_1</th>\n",
       "      <th>D5OtherTestResultDate19_1</th>\n",
       "      <th>D5OtherTestResultDate20_1</th>\n",
       "      <th>D5OtherTestResultDate21_1</th>\n",
       "      <th>D5OtherTestResultDate22_1</th>\n",
       "      <th>D5OtherTestResultDate23_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>202411</td>\n",
       "      <td>11/06/2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202455</td>\n",
       "      <td>10/08/2019</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203313</td>\n",
       "      <td>05/19/2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>203899</td>\n",
       "      <td>09/15/2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>207308</td>\n",
       "      <td>02/04/2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 234 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MaskID    Visit_Dt  D1FastingPlasmaGlucDD1_1 D1FastingPlasmaGlucMM1_1  \\\n",
       "0  202411  11/06/2018                       NaN                      NaN   \n",
       "1  202455  10/08/2019                      16.0                      Sep   \n",
       "2  203313  05/19/2020                       NaN                      NaN   \n",
       "3  203899  09/15/2020                       NaN                      NaN   \n",
       "4  207308  02/04/2019                       NaN                      NaN   \n",
       "\n",
       "   D1FastingPlasmaGlucYYYY1_1  D1FastingPlasmaGlucDD2_1  \\\n",
       "0                         NaN                       NaN   \n",
       "1                      2019.0                       NaN   \n",
       "2                         NaN                       NaN   \n",
       "3                         NaN                       NaN   \n",
       "4                         NaN                       NaN   \n",
       "\n",
       "  D1FastingPlasmaGlucMM2_1  D1FastingPlasmaGlucYYYY2_1  \\\n",
       "0                      NaN                         NaN   \n",
       "1                      NaN                         NaN   \n",
       "2                      NaN                         NaN   \n",
       "3                      NaN                         NaN   \n",
       "4                      NaN                         NaN   \n",
       "\n",
       "   D1FastingPlasmaGlucDD3_1 D1FastingPlasmaGlucMM3_1  ...  \\\n",
       "0                       NaN                      NaN  ...   \n",
       "1                       NaN                      NaN  ...   \n",
       "2                       NaN                      NaN  ...   \n",
       "3                       NaN                      NaN  ...   \n",
       "4                       NaN                      NaN  ...   \n",
       "\n",
       "   D5OtherTestResultDate14_1  D5OtherTestResultDate15_1  \\\n",
       "0                        NaN                        NaN   \n",
       "1                        NaN                        NaN   \n",
       "2                        NaN                        NaN   \n",
       "3                        NaN                        NaN   \n",
       "4                        NaN                        NaN   \n",
       "\n",
       "  D5OtherTestResultDate16_1  D5OtherTestResultDate17_1  \\\n",
       "0                       NaN                        NaN   \n",
       "1                       NaN                        NaN   \n",
       "2                       NaN                        NaN   \n",
       "3                       NaN                        NaN   \n",
       "4                       NaN                        NaN   \n",
       "\n",
       "   D5OtherTestResultDate18_1 D5OtherTestResultDate19_1  \\\n",
       "0                        NaN                       NaN   \n",
       "1                        NaN                       NaN   \n",
       "2                        NaN                       NaN   \n",
       "3                        NaN                       NaN   \n",
       "4                        NaN                       NaN   \n",
       "\n",
       "   D5OtherTestResultDate20_1  D5OtherTestResultDate21_1  \\\n",
       "0                        NaN                        NaN   \n",
       "1                        NaN                        NaN   \n",
       "2                        NaN                        NaN   \n",
       "3                        NaN                        NaN   \n",
       "4                        NaN                        NaN   \n",
       "\n",
       "  D5OtherTestResultDate22_1  D5OtherTestResultDate23_1  \n",
       "0                       NaN                        NaN  \n",
       "1                       NaN                        NaN  \n",
       "2                       NaN                        NaN  \n",
       "3                       NaN                        NaN  \n",
       "4                       NaN                        NaN  \n",
       "\n",
       "[5 rows x 234 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert 'NA' strings to np.nan\n",
    "def convert_na(value):\n",
    "    if value == 'NA':\n",
    "        return np.nan\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "# Function to convert month from string to number, handling missing values\n",
    "def month_to_num(month_str):\n",
    "    if pd.isna(month_str):\n",
    "        return np.nan\n",
    "    try:\n",
    "        datetime_object = pd.to_datetime(month_str, format='%b')\n",
    "        return datetime_object.month\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "# Function to create a date column from day, month, and year columns, handling missing values\n",
    "def create_date_column(df, year_col, original_cols):\n",
    "    parts = year_col.split('YYYY')\n",
    "    base_col_name = parts[0]\n",
    "    suffix = parts[1] if len(parts) > 1 else ''\n",
    "\n",
    "    day_col = base_col_name + 'DD' + suffix\n",
    "    month_col = base_col_name + 'MM' + suffix\n",
    "\n",
    "    if day_col in original_cols and month_col in original_cols:\n",
    "        date_col = base_col_name + 'Date' + suffix\n",
    "        df[date_col] = df.apply(\n",
    "            lambda row: f\"{int(month_to_num(row[month_col])):02d}/{int(row[day_col]) if pd.notna(row[day_col]) else 15:02d}/{int(row[year_col])}\"\n",
    "            if pd.notna(row[month_col]) and pd.notna(row[year_col])\n",
    "            else np.nan, axis=1)\n",
    "    else:\n",
    "        print(f\"Matching columns for '{year_col}' not found.\")\n",
    "\n",
    "# Convert 'NA' strings to np.nan\n",
    "for col in filtered_df.columns:\n",
    "    filtered_df[col] = filtered_df[col].apply(convert_na)\n",
    "\n",
    "original_cols = filtered_df.columns.tolist()\n",
    "\n",
    "# Iterating over columns and creating date columns\n",
    "for col in filtered_df.columns:\n",
    "    if 'YYYY' in col:\n",
    "        create_date_column(filtered_df, col, original_cols)\n",
    "\n",
    "print(filtered_df.info())\n",
    "filtered_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b45dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort columns alphabetically\n",
    "filtered_df_sorted = filtered_df.sort_index(axis=1)\n",
    "\n",
    "filtered_df_sorted.to_csv('date_merging.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb9d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60d38717",
   "metadata": {},
   "source": [
    "## 1.2 date and time merging from other files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a3f6e",
   "metadata": {},
   "source": [
    "### 1.2.1 Patterns of date\n",
    "\n",
    "We merged data based on their formats. Before the mergement, we need filter out columns for year, month and day. There are two ways: one based on column header; the other based on values (using months) of columns."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2b2f21d",
   "metadata": {},
   "source": [
    "1). With month and year:\n",
    "BirthMonth and BirthYear\n",
    "'UnexplainedWeightLossMonth', 'UnexplainedWeightLossYear'\n",
    "'PolyuriaMonth', 'PolyuriaYear'\n",
    "'PolyphagiaMonth', 'PolyphagiaYear'\n",
    "'PolydipsiaMonth', 'Polydipsiayear'\n",
    "'KetoacidosisMonth23', 'KetoacidosisYear23'\n",
    "'FatigueMonth', 'FatigueYear'\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53c851d2",
   "metadata": {},
   "source": [
    "2) With day, month and year\n",
    "'SamplesCollectionDay', 'SamplesCollectionMonth', 'SamplesCollectionYear'\n",
    "......."
   ]
  },
  {
   "cell_type": "raw",
   "id": "35286f7a",
   "metadata": {},
   "source": [
    "3) With day, mon, yea\n",
    "'MechanisticSampleCollectionDay', 'MechanisticSampleCollectionMon', 'MechanisticSampleCollectionYea'\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "943f636f",
   "metadata": {},
   "source": [
    "4) With D, M, Y\n",
    "DateSignedInformedConsentFormM', 'DateSignedInformedConsentFormD', 'DateSignedInformedConsentFormY'\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1152f70",
   "metadata": {},
   "source": [
    "5) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0abea0a",
   "metadata": {},
   "source": [
    "6) Without day, month, year\n",
    "MonitoringSchedulingChangeEffe\n",
    "IfyesDateCOVID19TestAdminister\n",
    "DateParticipantwascontacted\n",
    "DateSignedNEWInformedConsentFo\n",
    "DateControlBecameAutoantibodyP"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b156290a",
   "metadata": {},
   "source": [
    "7) Correct date format\n",
    "ProtocolVersion\n",
    "....."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e139d55d",
   "metadata": {},
   "source": [
    "8) Time formating with hh, mm\n",
    "'TimeOfCollectionHH', 'TimeOfCollectionMM'as well as date-time.\n",
    "'TimeResultsProcessedByLabHH', 'TimeResultsProcessedByLabMM' as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7bab40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf1ce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "# The following code is to load the first part of merged dataset\n",
    "import pandas as pd\n",
    "from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "file_path = \"/home/ec2-user/SageMaker/Team-5/TN_01_merged_data_1.csv\"\n",
    "merged_first_part = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d8c47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  136 columns:  ['_22Jul2009ConsentSignedMonth', '_15Aug2011ConsentDateMonth', 'Fall2017ConsentDateMonth', 'Spring2019ConsentDateMonth', 'StudyDrugStartDateMonth1_1', 'StudyDrugStopDateMonth1_1', 'StudyDrugStartDateMonth2_1', 'StudyDrugStopDateMonth2_1', 'DateofAssessmentMM', 'DateOfBloodDrawMonth', 'DateReportedMonth', 'MonitoringSchedulingChangeEffe2', 'IfyesDateCOVID19TestAdminister2', 'DateParticipantwascontacted1_2', 'DatebloodsamplewasdrawnMonth', 'ScreeningMonth', 'BirthMonth', 'ConsentSignedMonth', 'DateSignedInformedConsentFormM', 'DateSignedNEWInformedConsentFo2', 'Spring2019ConsentDateMonth_dup12', 'DateConsentSignedMonth', 'DateSignedInformedConsentFormM_dup14', 'DateSignedNEWInformedConsentFo2_dup14', 'MechanisticSampleCollectionMon', 'SamplesCollectionMonth', 'Spring2019ConsentDateMonth_dup14', 'MechanisticSampleCollectionMon_dup15', 'SamplesCollectionMonth_dup15', 'DateSignedNEWInformedConsentFo2_dup15', 'DateConsentSignedMonth_dup15', 'DateSignedNEWInformedConsentFo2_dup16', 'MechanisticSampleCollectionMon_dup16', 'SamplesCollectionMonth_dup16', 'DateCompletionMM', 'DateStatusChangeMonth', 'DoRMonth', 'DoWMonth', 'PregnancyCompletionMonth', 'DiagnosisMonth', 'InsulinTreatmentStartedMonth', 'AdmissionMonth', 'DischargeMonth', 'PolyuriaMonth', 'PolydipsiaMonth', 'PolyphagiaMonth', 'FatigueMonth', 'UnexplainedWeightLossMonth', 'KetoacidosisMonth23', 'PlasmaGlucoseMonth', 'pHMonth', 'SerumKetonesMonth', 'AnionGapMonth', 'BicarbonateMonth', 'UrineKetonesMonth', 'GlucoseMonth1_1', 'GlucoseMonth2_1', 'GlucoseMonth3_1', 'GlucoseMonth4_1', 'GlucoseMonth5_1', 'GlucoseMonth6_1', 'GlucoseMonth7_1', 'GlucoseMonth8_1', 'GlucoseMonth9_1', 'GlucoseMonth10_1', 'HbA1cMonth', 'D1FastingPlasmaGlucMM1_1', 'D1FastingPlasmaGlucMM2_1', 'D1FastingPlasmaGlucMM3_1', 'D2aOGTTFastingGlucMM1_1', 'D2aOGTTFastingGlucMM2_1', 'D2b2HourGlucMM1_1', 'D2b2HourGlucMM2_1', 'D3RandomPlasmaGlucMM1_1', 'D4HbA1cMM1_1', 'D4HbA1cMM2_1', 'D5MeterReadingMM1_1', 'D5MeterReadingMM2_1', 'D5MeterReadingMM3_1', 'D5MeterReadingMM4_1', 'D5OtherTestResultMM1_1', 'D5OtherTestResultMM2_1', 'D5OtherTestResultMM3_1', 'D5OtherTestResultMM4_1', 'D5OtherTestResultMM5_1', 'E1aDateOfOnsetMM', 'E1bDateOfOnsetMM', 'E1cDateofOnsetMM', 'E1dDateOfOnsetMM', 'f1availableWeightMM', 'f2availableWeightMM', 'f3availableWeightMM', 'F1DateStudyEndpointMetMM', 'F2ClinicalDiagnosisDateMM', 'DatestudyendpointmetMM', 'D2aFastingPlasmaGlucMM3_1', 'D2b2HourGlucMM3_1', 'D3RandomPlasmaGlucMM2_1', 'D3RandomPlasmaGlucMM3_1', 'D3RandomPlasmaGlucMM4_1', 'D3RandomPlasmaGlucMM5_1', 'D4HbA1cMM3_1', 'D4HbA1cMM4_1', 'D5MeterReadingMM5_1', 'D5MeterReadingMM6_1', 'D5MeterReadingMM7_1', 'D5OtherTestResultMM6_1', 'D5OtherTestResultMM7_1', 'D5OtherTestResultMM8_1', 'D5OtherTestResultMM9_1', 'D5OtherTestResultMM10_1', 'D5OtherTestResultMM11_1', 'D5OtherTestResultMM12_1', 'D5OtherTestResultMM13_1', 'D5OtherTestResultMM14_1', 'D5OtherTestResultMM15_1', 'D5OtherTestResultMM16_1', 'D5OtherTestResultMM17_1', 'D5OtherTestResultMM18_1', 'D5OtherTestResultMM19_1', 'D5OtherTestResultMM20_1', 'D5OtherTestResultMM21_1', 'D5OtherTestResultMM22_1', 'D5OtherTestResultMM23_1', 'DateConsentSignedMonth_dup21', 'BirthMonth_dup21', 'DateSignedNEWInformedConsentFo2_dup21', 'MechanisticSampleCollectionMon_dup21', 'SamplesCollectionMonth_dup21', 'DateSignedInformedConsentFormM_dup22', 'DateConsentSignedMonth_dup22', 'DateSignedNEWInformedConsentFo2_dup22', 'MechanisticSampleCollectionMon_dup22', 'SamplesCollectionMonth_dup22', 'DateControlBecameAutoantibodyP2', 'ProtcolDeviationMonth']\n"
     ]
    }
   ],
   "source": [
    "# Lookup with shorts of months\n",
    "string_values_list = ['Dec', 'Aug', 'Apr', 'Jan', 'Feb', 'Jun', 'Jul', 'Sep', 'Oct', 'May', 'Nov', 'Mar']  \n",
    "string_values_list_lower = [s.lower() for s in string_values_list]\n",
    "\n",
    "# Filter columns that contain at least one of the string values (case-insensitive)\n",
    "selected_columns = [col for col in merged_first_part.columns if merged_first_part[col].astype(str).str.lower().isin(string_values_list_lower).any()]\n",
    "\n",
    "print(\"Found \", len(selected_columns), \"columns: \", selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0cf597e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IfyesDateCOVID19TestAdminister2\n",
      "DateParticipantwascontacted1_2\n",
      "DateSignedInformedConsentFormM\n",
      "DateSignedNEWInformedConsentFo2\n",
      "DateSignedInformedConsentFormM_dup14\n",
      "DateSignedNEWInformedConsentFo2_dup14\n",
      "DateSignedNEWInformedConsentFo2_dup15\n",
      "DateSignedNEWInformedConsentFo2_dup16\n",
      "DateSignedNEWInformedConsentFo2_dup21\n",
      "DateSignedInformedConsentFormM_dup22\n",
      "DateSignedNEWInformedConsentFo2_dup22\n",
      "DateControlBecameAutoantibodyP2\n"
     ]
    }
   ],
   "source": [
    "for i in selected_columns:\n",
    "    if 'Month' not in i and 'MM' not in i and 'Mon' not in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "132c8e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AEDateDeathMonth', 'AEFollowupDateMonth', 'AEOccurDtMonth', 'AEReportDtMonth', 'AEResolveDtMonth', 'AdmissionMonth', 'AnionGapMonth', 'BicarbonateMonth', 'BirthMonth', 'BirthMonth_dup20', 'ConsentSignedMonth', 'DateConsentSignedMonth', 'DateConsentSignedMonth_dup15', 'DateConsentSignedMonth_dup20', 'DateConsentSignedMonth_dup21', 'DateOfBloodDrawMonth', 'DateReportedMonth', 'DateStatusChangeMonth', 'DatebloodsamplewasdrawnMonth', 'DiagnosisMonth', 'DischargeMonth', 'DoRMonth', 'DoWMonth', 'FatigueMonth', 'GlucoseMonth10_1', 'GlucoseMonth1_1', 'GlucoseMonth2_1', 'GlucoseMonth3_1', 'GlucoseMonth4_1', 'GlucoseMonth5_1', 'GlucoseMonth6_1', 'GlucoseMonth7_1', 'GlucoseMonth8_1', 'GlucoseMonth9_1', 'HbA1cMonth', 'InsulinTreatmentStartedMonth', 'KetoacidosisMonth23', 'PlasmaGlucoseMonth', 'PolydipsiaMonth', 'PolyphagiaMonth', 'PolyuriaMonth', 'PregnancyCompletionMonth', 'ProtcolDeviationMonth', 'SamplesCollectionMonth', 'SamplesCollectionMonth_dup14', 'SamplesCollectionMonth_dup15', 'SamplesCollectionMonth_dup20', 'SamplesCollectionMonth_dup21', 'ScreeningMonth', 'SerumKetonesMonth', 'Spring2019ConsentDateMonth', 'Spring2019ConsentDateMonth_dup13', 'StudyDrugStartDateMonth1_1', 'StudyDrugStartDateMonth2_1', 'StudyDrugStopDateMonth1_1', 'StudyDrugStopDateMonth2_1', 'UnexplainedWeightLossMonth', 'UrineKetonesMonth', 'pHMonth']\n"
     ]
    }
   ],
   "source": [
    "month_column_101 = ['AEReportDtMonth', 'AEOccurDtMonth', 'AEResolveDtMonth', 'AEDateDeathMonth', 'StudyDrugStartDateMonth1_1', 'StudyDrugStopDateMonth1_1', 'StudyDrugStartDateMonth2_1', 'StudyDrugStopDateMonth2_1', 'AEFollowupDateMonth', 'DateOfBloodDrawMonth', 'DateReportedMonth', 'ReasonsNotInterested_MonthlyInfu', 'MonthlyInfusionVisitScheduleRa', 'ReasonsNotInterested_MonthlyInfu_dup8', 'DatebloodsamplewasdrawnMonth', 'ScreeningMonth', 'BirthMonth', 'ConsentSignedMonth', 'Spring2019ConsentDateMonth', 'DateConsentSignedMonth', 'SamplesCollectionMonth', 'Spring2019ConsentDateMonth_dup13', 'AddisonsDiseaseWithinLast6Months', 'AllergiesWithinLast6Months', 'AlopeciaWithinLast6Months', 'AsthmaWithinLast6Months', 'CancerWithinLast6Months', 'CeliacDiseaseWithinLast6Months', 'PsoriasisWithinLast6Months', 'RheumatologicWithinLast6Months', 'ThyroidDiseaseWithinLast6Months', 'VitiligoWithinLast6Months', 'UlcerWithinLast6Months', 'HighBPwithinLast6Months', 'HighCholestrolWithinLast6Months', 'SamplesCollectionMonth_dup14', 'AsthmaWithinLast6Months_dup15', 'CeliacDiseaseWithinLast6Months_dup15', 'AddisonsDiseaseWithinLast6Months_dup15', 'VitiligoWithinLast6Months_dup15', 'ThyroidDiseaseWithinLast6Months_dup15', 'PsoriasisWithinLast6Months_dup15', 'AlopeciaWithinLast6Months_dup15', 'RheumatologicWithinLast6Months_dup15', 'AllergiesWithinLast6Months_dup15', 'CancerWithinLast6Months_dup15', 'UlcerWithinLast6Months_dup15', 'HighBPwithinLast6Months_dup15', 'SamplesCollectionMonth_dup15', 'DateConsentSignedMonth_dup15', 'HighCholesterolWithinLast6Months', 'DateStatusChangeMonth', 'DoRMonth', 'DoWMonth', 'PregnancyCompletionMonth', 'DiagnosisMonth', 'InsulinTreatmentStartedMonth', 'AdmissionMonth', 'DischargeMonth', 'PolyuriaMonth', 'PolydipsiaMonth', 'PolyphagiaMonth', 'FatigueMonth', 'UnexplainedWeightLossMonth', 'KetoacidosisMonth23', 'PlasmaGlucoseMonth', 'pHMonth', 'SerumKetonesMonth', 'AnionGapMonth', 'BicarbonateMonth', 'UrineKetonesMonth', 'GlucoseMonth1_1', 'GlucoseMonth2_1', 'GlucoseMonth3_1', 'GlucoseMonth4_1', 'GlucoseMonth5_1', 'GlucoseMonth6_1', 'GlucoseMonth7_1', 'GlucoseMonth8_1', 'GlucoseMonth9_1', 'GlucoseMonth10_1', 'HbA1cMonth', 'DateConsentSignedMonth_dup20', 'BirthMonth_dup20', 'AsthmaWithinLast6Months_dup20', 'CeliacDiseaseWithinLast6Months_dup20', 'AddisonsDiseaseWithinLast6Months_dup20', 'VitiligoWithinLast6Months_dup20', 'ThyroidDiseaseWithinLast6Months_dup20', 'PsoriasisWithinLast6Months_dup20', 'AlopeciaWithinLast6Months_dup20', 'RheumatologicWithinLast6Months_dup20', 'AllergiesWithinLast6Months_dup20', 'CancerWithinLast6Months_dup20', 'HighBPwithinLast6Months_dup20', 'HighCholesterolWithinLast6Months_dup20', 'UlcerWithinLast6Months_dup20', 'SamplesCollectionMonth_dup20', 'DateConsentSignedMonth_dup21', 'SamplesCollectionMonth_dup21', 'ProtcolDeviationMonth']\n",
    "# 3 with \"monthly\", 39 with \"6months\"\n",
    "\n",
    "month_column_59 =['AEReportDtMonth', 'AEOccurDtMonth', 'AEResolveDtMonth', 'AEDateDeathMonth', 'StudyDrugStartDateMonth1_1', 'StudyDrugStopDateMonth1_1', 'StudyDrugStartDateMonth2_1', 'StudyDrugStopDateMonth2_1', 'AEFollowupDateMonth', 'DateOfBloodDrawMonth', 'DateReportedMonth', 'DatebloodsamplewasdrawnMonth', 'ScreeningMonth', 'BirthMonth', 'ConsentSignedMonth', 'Spring2019ConsentDateMonth', 'DateConsentSignedMonth', 'SamplesCollectionMonth', 'Spring2019ConsentDateMonth_dup13', 'SamplesCollectionMonth_dup14', 'SamplesCollectionMonth_dup15', 'DateConsentSignedMonth_dup15', 'DateStatusChangeMonth', 'DoRMonth', 'DoWMonth', 'PregnancyCompletionMonth', 'DiagnosisMonth', 'InsulinTreatmentStartedMonth', 'AdmissionMonth', 'DischargeMonth', 'PolyuriaMonth', 'PolydipsiaMonth', 'PolyphagiaMonth', 'FatigueMonth', 'UnexplainedWeightLossMonth', 'KetoacidosisMonth23', 'PlasmaGlucoseMonth', 'pHMonth', 'SerumKetonesMonth', 'AnionGapMonth', 'BicarbonateMonth', 'UrineKetonesMonth', 'GlucoseMonth1_1', 'GlucoseMonth2_1', 'GlucoseMonth3_1', 'GlucoseMonth4_1', 'GlucoseMonth5_1', 'GlucoseMonth6_1', 'GlucoseMonth7_1', 'GlucoseMonth8_1', 'GlucoseMonth9_1', 'GlucoseMonth10_1', 'HbA1cMonth', 'DateConsentSignedMonth_dup20', 'BirthMonth_dup20', 'SamplesCollectionMonth_dup20', 'DateConsentSignedMonth_dup21', 'SamplesCollectionMonth_dup21', 'ProtcolDeviationMonth']\n",
    "\n",
    "print(sorted(month_column_59))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bdfba53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AEDateDeathYear', 'AEFollowupDateYear', 'AEOccurDtYear', 'AEReportDtYear', 'AEResolveDtYear', 'AdmissionYear', 'AnionGapYear', 'BicarbonateYear', 'BirthYear', 'BirthYear_dup20', 'ConsentSignedYear', 'DateConsentSignedYear', 'DateConsentSignedYear_dup15', 'DateConsentSignedYear_dup20', 'DateConsentSignedYear_dup21', 'DateOfBloodDrawYear', 'DateReportedYear', 'DateStatusChangeYear', 'DatebloodsamplewasdrawnYear', 'DiagnosisYear', 'DischargeYear', 'DoRYear', 'DoWYear', 'FatigueYear', 'GlucoseYear10_1', 'GlucoseYear1_1', 'GlucoseYear2_1', 'GlucoseYear3_1', 'GlucoseYear4_1', 'GlucoseYear5_1', 'GlucoseYear6_1', 'GlucoseYear7_1', 'GlucoseYear8_1', 'GlucoseYear9_1', 'HbA1cYear', 'InsulinTreatmentStartedYear', 'KetoacidosisYear23', 'PlasmaGlucoseYear', 'Polydipsiayear', 'PolyphagiaYear', 'PolyuriaYear', 'PregnancyCompletionYear', 'ProtcolDeviationYear', 'SamplesCollectionYear', 'SamplesCollectionYear_dup14', 'SamplesCollectionYear_dup15', 'SamplesCollectionYear_dup20', 'SamplesCollectionYear_dup21', 'ScreeningYear', 'SerumKetonesYear', 'Spring2019ConsentDateYear', 'Spring2019ConsentDateYear_dup13', 'StudyDrugStartDateYear1_1', 'StudyDrugStartDateYear2_1', 'StudyDrugStopDateYear1_1', 'StudyDrugStopDateYear2_1', 'UnexplainedWeightLossYear', 'UrineKetonesYear', 'pHYear']\n"
     ]
    }
   ],
   "source": [
    "year_column_86 =['AEReportDtYear', 'AEOccurDtYear', 'AEResolveDtYear', 'AEDateDeathYear', 'StudyDrugStartDateYear1_1', 'StudyDrugStopDateYear1_1', 'StudyDrugStartDateYear2_1', 'StudyDrugStopDateYear2_1', 'AEFollowupDateYear', 'DateOfBloodDrawYear', 'DateReportedYear', 'DatebloodsamplewasdrawnYear', 'ScreeningYear', 'BirthYear', 'ConsentSignedYear', 'Spring2019ConsentDateYear', 'DateConsentSignedYear', 'AsthmaWithinLastYear', 'CeliacDiseaseWithinLastYear', 'AddisonsDiseaseWithinLastYear', 'VitiligoWithinLastYear', 'ThyroidDiseaseWithinLastYear', 'PsoriasisWithinLastYear', 'AlopeciaWithinLastYear', 'RheumatologicWithinLastYear', 'AllergiesWithinLastYear', 'CancerWithinLastYear', 'HighBPwithinLastYear', 'PerniciousAnemiaWithinLastYear', 'UlcerWithinLastYear', 'SamplesCollectionYear', 'Spring2019ConsentDateYear_dup13', 'SamplesCollectionYear_dup14', 'SamplesCollectionYear_dup15', 'DateConsentSignedYear_dup15', 'DateStatusChangeYear', 'DoRYear', 'DoWYear', 'PregnancyCompletionYear', 'DiagnosisYear', 'InsulinTreatmentStartedYear', 'AdmissionYear', 'DischargeYear', 'PolyuriaYear', 'Polydipsiayear', 'PolyphagiaYear', 'FatigueYear', 'UnexplainedWeightLossYear', 'KetoacidosisYear23', 'PlasmaGlucoseYear', 'pHYear', 'SerumKetonesYear', 'AnionGapYear', 'BicarbonateYear', 'UrineKetonesYear', 'GlucoseYear1_1', 'GlucoseYear2_1', 'GlucoseYear3_1', 'GlucoseYear4_1', 'GlucoseYear5_1', 'GlucoseYear6_1', 'GlucoseYear7_1', 'GlucoseYear8_1', 'GlucoseYear9_1', 'GlucoseYear10_1', 'HbA1cYear', 'DateConsentSignedYear_dup20', 'BirthYear_dup20', 'SamplesCollectionYear_dup20', 'AsthmaWithinLastYear_dup21', 'CeliacDiseaseWithinLastYear_dup21', 'AddisonsDiseaseWithinLastYear_dup21', 'VitiligoWithinLastYear_dup21', 'ThyroidDiseaseWithinLastYear_dup21', 'PsoriasisWithinLastYear_dup21', 'AlopeciaWithinLastYear_dup21', 'RheumatologicWithinLastYear_dup21', 'AllergiesWithinLastYear_dup21', 'CancerWithinLastYear_dup21', 'HighBPwithinLastYear_dup21', 'HighCholesterolWithinLastYear', 'PerniciousAnemiaWithinLastYear_dup21', 'UlcerWithinLastYear_dup21', 'DateConsentSignedYear_dup21', 'SamplesCollectionYear_dup21', 'ProtcolDeviationYear']\n",
    "# 27 with \"LastYear\"\n",
    "\n",
    "year_column_59 = ['AEReportDtYear', 'AEOccurDtYear', 'AEResolveDtYear', 'AEDateDeathYear', 'StudyDrugStartDateYear1_1', 'StudyDrugStopDateYear1_1', 'StudyDrugStartDateYear2_1', 'StudyDrugStopDateYear2_1', 'AEFollowupDateYear', 'DateOfBloodDrawYear', 'DateReportedYear', 'DatebloodsamplewasdrawnYear', 'ScreeningYear', 'BirthYear', 'ConsentSignedYear', 'Spring2019ConsentDateYear', 'DateConsentSignedYear', 'SamplesCollectionYear', 'Spring2019ConsentDateYear_dup13', 'SamplesCollectionYear_dup14', 'SamplesCollectionYear_dup15', 'DateConsentSignedYear_dup15', 'DateStatusChangeYear', 'DoRYear', 'DoWYear', 'PregnancyCompletionYear', 'DiagnosisYear', 'InsulinTreatmentStartedYear', 'AdmissionYear', 'DischargeYear', 'PolyuriaYear', 'Polydipsiayear', 'PolyphagiaYear', 'FatigueYear', 'UnexplainedWeightLossYear', 'KetoacidosisYear23', 'PlasmaGlucoseYear', 'pHYear', 'SerumKetonesYear', 'AnionGapYear', 'BicarbonateYear', 'UrineKetonesYear', 'GlucoseYear1_1', 'GlucoseYear2_1', 'GlucoseYear3_1', 'GlucoseYear4_1', 'GlucoseYear5_1', 'GlucoseYear6_1', 'GlucoseYear7_1', 'GlucoseYear8_1', 'GlucoseYear9_1', 'GlucoseYear10_1', 'HbA1cYear', 'DateConsentSignedYear_dup20', 'BirthYear_dup20', 'SamplesCollectionYear_dup20', 'DateConsentSignedYear_dup21', 'SamplesCollectionYear_dup21', 'ProtcolDeviationYear']\n",
    "\n",
    "print(sorted(year_column_59))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e48e24fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AEDateDeathDay', 'AEFollowupDateDay', 'AEOccurDtDay', 'AEReportDtDay', 'AEResolveDtDay', 'AdmissionDay', 'AnionGapDay', 'BicarbonateDay', 'ConsentSignedDay', 'DateConsentSignedDay', 'DateConsentSignedDay_dup15', 'DateConsentSignedDay_dup20', 'DateConsentSignedDay_dup21', 'DateOfBloodDrawDay', 'DateReportedDay', 'DateStatusChangeDay', 'DatebloodsamplewasdrawnDay', 'DiagnosisDay', 'DischargeDay', 'DoRDay', 'DoWDay', 'GlucoseDay10_1', 'GlucoseDay1_1', 'GlucoseDay2_1', 'GlucoseDay3_1', 'GlucoseDay4_1', 'GlucoseDay5_1', 'GlucoseDay6_1', 'GlucoseDay7_1', 'GlucoseDay8_1', 'GlucoseDay9_1', 'HbA1cDay', 'InsulinTreatmentStartedDay', 'MechanisticSampleCollectionDay', 'MechanisticSampleCollectionDay_dup14', 'MechanisticSampleCollectionDay_dup15', 'MechanisticSampleCollectionDay_dup20', 'MechanisticSampleCollectionDay_dup21', 'PlasmaGlucoseday', 'PregnancyCompletionDay', 'ProtcolDeviationDay', 'SamplesCollectionDay', 'SamplesCollectionDay_dup14', 'SamplesCollectionDay_dup15', 'SamplesCollectionDay_dup20', 'SamplesCollectionDay_dup21', 'ScreeningDay', 'SerumKetonesday', 'Spring2019ConsentDateDay', 'Spring2019ConsentDateDay_dup13', 'StudyDrugStartDateDay1_1', 'StudyDrugStartDateDay2_1', 'StudyDrugStopDateDay1_1', 'StudyDrugStopDateDay2_1', 'UrineKetonesDay', 'pHday']\n"
     ]
    }
   ],
   "source": [
    "day_column_56 = ['AEReportDtDay', 'AEOccurDtDay', 'AEResolveDtDay', 'AEDateDeathDay', 'StudyDrugStartDateDay1_1', 'StudyDrugStopDateDay1_1', 'StudyDrugStartDateDay2_1', 'StudyDrugStopDateDay2_1', 'AEFollowupDateDay', 'DateOfBloodDrawDay', 'DateReportedDay', 'DatebloodsamplewasdrawnDay', 'ScreeningDay', 'ConsentSignedDay', 'Spring2019ConsentDateDay', 'DateConsentSignedDay', 'MechanisticSampleCollectionDay', 'SamplesCollectionDay', 'Spring2019ConsentDateDay_dup13', 'MechanisticSampleCollectionDay_dup14', 'SamplesCollectionDay_dup14', 'MechanisticSampleCollectionDay_dup15', 'SamplesCollectionDay_dup15', 'DateConsentSignedDay_dup15', 'DateStatusChangeDay', 'DoRDay', 'DoWDay', 'PregnancyCompletionDay', 'DiagnosisDay', 'InsulinTreatmentStartedDay', 'AdmissionDay', 'DischargeDay', 'PlasmaGlucoseday', 'pHday', 'SerumKetonesday', 'AnionGapDay', 'BicarbonateDay', 'UrineKetonesDay', 'GlucoseDay1_1', 'GlucoseDay2_1', 'GlucoseDay3_1', 'GlucoseDay4_1', 'GlucoseDay5_1', 'GlucoseDay6_1', 'GlucoseDay7_1', 'GlucoseDay8_1', 'GlucoseDay9_1', 'GlucoseDay10_1', 'HbA1cDay', 'DateConsentSignedDay_dup20', 'MechanisticSampleCollectionDay_dup20', 'SamplesCollectionDay_dup20', 'DateConsentSignedDay_dup21', 'MechanisticSampleCollectionDay_dup21', 'SamplesCollectionDay_dup21', 'ProtcolDeviationDay']\n",
    "print(sorted(day_column_56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05e3a95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  3 columns:  ['DateControlBecameAutoantibodyP', 'DateControlBecameAutoantibodyP2', 'DateControlBecameAutoantibodyP3']\n",
      "DataFrame with filtered columns (rows with all missing values dropped):\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 34 entries, 5379 to 400875\n",
      "Data columns (total 3 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   DateControlBecameAutoantibodyP   34 non-null     float64\n",
      " 1   DateControlBecameAutoantibodyP2  34 non-null     object \n",
      " 2   DateControlBecameAutoantibodyP3  34 non-null     float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 1.1+ KB\n",
      "None\n",
      "Top 20 values for column DateControlBecameAutoantibodyP:\n",
      "DateControlBecameAutoantibodyP\n",
      "12.0    5\n",
      "13.0    3\n",
      "28.0    2\n",
      "18.0    2\n",
      "27.0    2\n",
      "15.0    2\n",
      "5.0     2\n",
      "23.0    2\n",
      "21.0    1\n",
      "7.0     1\n",
      "9.0     1\n",
      "19.0    1\n",
      "14.0    1\n",
      "22.0    1\n",
      "16.0    1\n",
      "26.0    1\n",
      "4.0     1\n",
      "25.0    1\n",
      "11.0    1\n",
      "10.0    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column DateControlBecameAutoantibodyP2:\n",
      "DateControlBecameAutoantibodyP2\n",
      "Dec    5\n",
      "Nov    4\n",
      "Mar    3\n",
      "Oct    3\n",
      "Jan    3\n",
      "Jun    3\n",
      "Jul    3\n",
      "Aug    3\n",
      "Feb    2\n",
      "Apr    2\n",
      "May    2\n",
      "Sep    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column DateControlBecameAutoantibodyP3:\n",
      "DateControlBecameAutoantibodyP3\n",
      "2015.0    10\n",
      "2017.0     8\n",
      "2018.0     6\n",
      "2016.0     4\n",
      "2019.0     3\n",
      "2010.0     1\n",
      "2014.0     1\n",
      "2013.0     1\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string = \"DateControlBecameAutoantibodyP\"  # change this to the string you're searching for\n",
    "# Convert the search string to lower case (or upper case)\n",
    "search_string_lower = string.lower()\n",
    "\n",
    "# Find columns that contain the search string, case-insensitive\n",
    "matching_columns = [col for col in merged_first_part.columns if search_string_lower in col.lower()]\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "filtered_df = merged_first_part[matching_columns].dropna(how='all')\n",
    "print(\"DataFrame with filtered columns (rows with all missing values dropped):\")\n",
    "print()\n",
    "print(filtered_df.info())\n",
    "#print(filtered_df.head(30))\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "for col in matching_columns:\n",
    "    print(f\"Top 20 values for column {col}:\")\n",
    "    print(merged_first_part[col].value_counts().nlargest(20))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c597c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three Strings to search for: NOT\n",
    "string1 = \"age\"  # First string\n",
    "string2 = \"relative\"  # Second string, change this to your second string\n",
    "string3 = \"agent\"\n",
    "\n",
    "# Convert the search strings to lower case\n",
    "search_string1_lower = string1.lower()\n",
    "search_string2_lower = string2.lower()\n",
    "search_string3_lower = string3.lower()\n",
    "\n",
    "# Find columns that contain both search strings, case-insensitive\n",
    "matching_columns = [col for col in merged_first_part.columns \n",
    "                    if search_string1_lower in col.lower() and search_string2_lower not in col.lower() and search_string3_lower not in col.lower()]\n",
    "\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "for col in matching_columns:\n",
    "    print(f\"Top 20 values for column {col}:\")\n",
    "    print(merged_first_part[col].value_counts().nlargest(20))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a809b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two Strings to search for: NOT\n",
    "string1 = \"year\"  # First string\n",
    "string2 = \"lastyear\"  # Second string, change this to your second string\n",
    "\n",
    "# Convert the search strings to lower case\n",
    "search_string1_lower = string1.lower()\n",
    "search_string2_lower = string2.lower()\n",
    "\n",
    "# Find columns that contain both search strings, case-insensitive\n",
    "matching_columns = [col for col in merged_first_part.columns \n",
    "                    if search_string1_lower in col.lower() and search_string2_lower not in col.lower()]\n",
    "\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "for col in matching_columns:\n",
    "    print(f\"Top 20 values for column {col}:\")\n",
    "    print(merged_first_part[col].value_counts().nlargest(20))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0633f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39a028de",
   "metadata": {},
   "source": [
    "### 1.2.2 Date merge based on month and year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fc990c",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "1. With filtered column pairs: one for month and one for year.\n",
    "2. Do not impute. Only merge when both values are present.\n",
    "3. Create new column with the following naming fomat: NameMonthYear"
   ]
  },
  {
   "cell_type": "raw",
   "id": "318c7fef",
   "metadata": {},
   "source": [
    "Column pairs:\n",
    "'BirthMonth', 'BirthYear'\n",
    "'UnexplainedWeightLossMonth', 'UnexplainedWeightLossYear'\n",
    "'PolyuriaMonth', 'PolyuriaYear'\n",
    "'PolyphagiaMonth', 'PolyphagiaYear'\n",
    "'PolydipsiaMonth', 'Polydipsiayear'\n",
    "'KetoacidosisMonth23', 'KetoacidosisYear23'\n",
    "'FatigueMonth', 'FatigueYear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48042c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "column_list = ['BirthMonth', 'BirthYear',\n",
    "'UnexplainedWeightLossMonth', 'UnexplainedWeightLossYear',\n",
    "'PolyuriaMonth', 'PolyuriaYear',\n",
    "'PolyphagiaMonth', 'PolyphagiaYear',\n",
    "'PolydipsiaMonth', 'Polydipsiayear',\n",
    "'KetoacidosisMonth23', 'KetoacidosisYear23',\n",
    "'FatigueMonth', 'FatigueYear']\n",
    "\n",
    "column_list2 = ['MaskID'] + column_list\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "df_date_pair = merged_first_part[column_list2].dropna(how='all', subset=column_list)\n",
    "\n",
    "# Merging the columns by pair\n",
    "column_pairs = [\n",
    "    ('BirthMonth', 'BirthYear'),\n",
    "    ('UnexplainedWeightLossMonth', 'UnexplainedWeightLossYear'),\n",
    "    ('PolyuriaMonth', 'PolyuriaYear'),\n",
    "    ('PolyphagiaMonth', 'PolyphagiaYear'),\n",
    "    ('PolydipsiaMonth', 'Polydipsiayear'),\n",
    "    ('KetoacidosisMonth23', 'KetoacidosisYear23'),\n",
    "    ('FatigueMonth', 'FatigueYear')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76f9cab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MaskID': MaskID\n",
      "200906    5\n",
      "259152    3\n",
      "754205    3\n",
      "432775    3\n",
      "923263    3\n",
      "896375    3\n",
      "564312    3\n",
      "978029    3\n",
      "567504    3\n",
      "660426    2\n",
      "735175    2\n",
      "596567    2\n",
      "249000    2\n",
      "884328    2\n",
      "682988    2\n",
      "269699    2\n",
      "732025    2\n",
      "677631    2\n",
      "598360    2\n",
      "771993    2\n",
      "Name: count, dtype: int64, 'BirthMonth': BirthMonth\n",
      "Aug    20599\n",
      "Jul    20218\n",
      "Mar    20152\n",
      "Sep    20081\n",
      "Oct    19910\n",
      "May    19712\n",
      "Jun    19512\n",
      "Apr    19365\n",
      "Dec    19154\n",
      "Jan    19012\n",
      "Nov    18705\n",
      "Feb    18131\n",
      "Name: count, dtype: int64, 'BirthYear': BirthYear\n",
      "2003.0    7649\n",
      "2004.0    7605\n",
      "2002.0    7543\n",
      "2005.0    7527\n",
      "2006.0    7341\n",
      "2001.0    7253\n",
      "2007.0    6910\n",
      "2000.0    6804\n",
      "1999.0    6496\n",
      "2008.0    6472\n",
      "1998.0    6058\n",
      "2009.0    5950\n",
      "1997.0    5666\n",
      "2010.0    5323\n",
      "1996.0    5120\n",
      "2011.0    4862\n",
      "1995.0    4592\n",
      "1973.0    4500\n",
      "1974.0    4499\n",
      "2012.0    4472\n",
      "Name: count, dtype: int64, 'UnexplainedWeightLossMonth': UnexplainedWeightLossMonth\n",
      "Aug    27\n",
      "Jan    26\n",
      "May    21\n",
      "Dec    18\n",
      "Jul    17\n",
      "Apr    17\n",
      "Nov    15\n",
      "Oct    14\n",
      "Feb    13\n",
      "Sep    13\n",
      "Jun    10\n",
      "Mar    10\n",
      "Name: count, dtype: int64, 'UnexplainedWeightLossYear': UnexplainedWeightLossYear\n",
      "2017.0    25\n",
      "2016.0    23\n",
      "2014.0    20\n",
      "2012.0    19\n",
      "2008.0    17\n",
      "2010.0    16\n",
      "2015.0    16\n",
      "2009.0    15\n",
      "2007.0    14\n",
      "2011.0    13\n",
      "2013.0    11\n",
      "2006.0     7\n",
      "2018.0     4\n",
      "2005.0     4\n",
      "Name: count, dtype: int64, 'PolyuriaMonth': PolyuriaMonth\n",
      "Apr    62\n",
      "Jul    59\n",
      "Jan    59\n",
      "Feb    58\n",
      "Dec    57\n",
      "Aug    48\n",
      "Jun    47\n",
      "Oct    46\n",
      "Sep    41\n",
      "May    41\n",
      "Nov    37\n",
      "Mar    35\n",
      "Name: count, dtype: int64, 'PolyuriaYear': PolyuriaYear\n",
      "2017.0    81\n",
      "2016.0    61\n",
      "2010.0    51\n",
      "2011.0    50\n",
      "2007.0    49\n",
      "2012.0    46\n",
      "2014.0    46\n",
      "2008.0    46\n",
      "2009.0    44\n",
      "2015.0    37\n",
      "2013.0    36\n",
      "2006.0    19\n",
      "2018.0    13\n",
      "2005.0    10\n",
      "2004.0     2\n",
      "Name: count, dtype: int64, 'PolyphagiaMonth': PolyphagiaMonth\n",
      "Dec    20\n",
      "Aug    20\n",
      "Apr    16\n",
      "Jan    16\n",
      "Feb    16\n",
      "Jun    13\n",
      "Jul    13\n",
      "Sep    12\n",
      "Oct    10\n",
      "May     8\n",
      "Nov     8\n",
      "Mar     7\n",
      "Name: count, dtype: int64, 'PolyphagiaYear': PolyphagiaYear\n",
      "2017.0    22\n",
      "2012.0    21\n",
      "2014.0    18\n",
      "2007.0    17\n",
      "2011.0    12\n",
      "2016.0    12\n",
      "2008.0    12\n",
      "2015.0    10\n",
      "2010.0     9\n",
      "2006.0     8\n",
      "2009.0     6\n",
      "2013.0     5\n",
      "2005.0     5\n",
      "2018.0     2\n",
      "Name: count, dtype: int64, 'PolydipsiaMonth': PolydipsiaMonth\n",
      "Jan    56\n",
      "Jul    55\n",
      "Apr    54\n",
      "Feb    51\n",
      "Dec    51\n",
      "Oct    47\n",
      "Aug    42\n",
      "May    39\n",
      "Jun    38\n",
      "Sep    37\n",
      "Nov    36\n",
      "Mar    33\n",
      "Name: count, dtype: int64, 'Polydipsiayear': Polydipsiayear\n",
      "2017.0    68\n",
      "2016.0    58\n",
      "2010.0    49\n",
      "2014.0    46\n",
      "2012.0    44\n",
      "2007.0    43\n",
      "2011.0    42\n",
      "2015.0    39\n",
      "2008.0    39\n",
      "2009.0    37\n",
      "2013.0    34\n",
      "2006.0    17\n",
      "2018.0    13\n",
      "2005.0     8\n",
      "2004.0     1\n",
      "Name: count, dtype: int64, 'KetoacidosisMonth23': KetoacidosisMonth23\n",
      "Dec    1\n",
      "Name: count, dtype: int64, 'KetoacidosisYear23': KetoacidosisYear23\n",
      "2017.0    1\n",
      "Name: count, dtype: int64, 'FatigueMonth': FatigueMonth\n",
      "Apr    35\n",
      "Jan    35\n",
      "Aug    33\n",
      "Dec    32\n",
      "Jul    27\n",
      "Feb    26\n",
      "May    25\n",
      "Jun    25\n",
      "Sep    22\n",
      "Nov    18\n",
      "Mar    18\n",
      "Oct    16\n",
      "Name: count, dtype: int64, 'FatigueYear': FatigueYear\n",
      "2017.0    39\n",
      "2016.0    35\n",
      "2012.0    29\n",
      "2015.0    26\n",
      "2014.0    25\n",
      "2008.0    24\n",
      "2011.0    23\n",
      "2007.0    22\n",
      "2013.0    21\n",
      "2010.0    19\n",
      "2009.0    15\n",
      "2006.0    11\n",
      "2018.0     6\n",
      "2005.0     6\n",
      "2004.0     1\n",
      "Name: count, dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "# Show top 20 values by their value counts for all columns in the dataframe\n",
    "top_20_values_by_column = {}\n",
    "\n",
    "for column in df_date_pair.columns:\n",
    "    top_20_values_by_column[column] = df_date_pair[column].value_counts().head(20)\n",
    "\n",
    "print(top_20_values_by_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e046ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 235276 entries, 8860 to 398797\n",
      "Data columns (total 22 columns):\n",
      " #   Column                          Non-Null Count   Dtype  \n",
      "---  ------                          --------------   -----  \n",
      " 0   MaskID                          235276 non-null  int64  \n",
      " 1   BirthMonth                      234550 non-null  object \n",
      " 2   BirthYear                       234556 non-null  float64\n",
      " 3   UnexplainedWeightLossMonth      199 non-null     object \n",
      " 4   UnexplainedWeightLossYear       202 non-null     float64\n",
      " 5   PolyuriaMonth                   583 non-null     object \n",
      " 6   PolyuriaYear                    584 non-null     float64\n",
      " 7   PolyphagiaMonth                 156 non-null     object \n",
      " 8   PolyphagiaYear                  156 non-null     float64\n",
      " 9   PolydipsiaMonth                 533 non-null     object \n",
      " 10  Polydipsiayear                  532 non-null     float64\n",
      " 11  KetoacidosisMonth23             1 non-null       object \n",
      " 12  KetoacidosisYear23              1 non-null       float64\n",
      " 13  FatigueMonth                    307 non-null     object \n",
      " 14  FatigueYear                     297 non-null     float64\n",
      " 15  BirthMonthYear                  234550 non-null  object \n",
      " 16  UnexplainedWeightLossMonthYear  199 non-null     object \n",
      " 17  PolyuriaMonthYear               579 non-null     object \n",
      " 18  PolyphagiaMonthYear             154 non-null     object \n",
      " 19  PolydipsiaMonthYear             528 non-null     object \n",
      " 20  KetoacidosisMonthYear23         1 non-null       object \n",
      " 21  FatigueMonthYear                295 non-null     object \n",
      "dtypes: float64(7), int64(1), object(14)\n",
      "memory usage: 41.3+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaskID</th>\n",
       "      <th>BirthMonth</th>\n",
       "      <th>BirthYear</th>\n",
       "      <th>UnexplainedWeightLossMonth</th>\n",
       "      <th>UnexplainedWeightLossYear</th>\n",
       "      <th>PolyuriaMonth</th>\n",
       "      <th>PolyuriaYear</th>\n",
       "      <th>PolyphagiaMonth</th>\n",
       "      <th>PolyphagiaYear</th>\n",
       "      <th>PolydipsiaMonth</th>\n",
       "      <th>...</th>\n",
       "      <th>KetoacidosisYear23</th>\n",
       "      <th>FatigueMonth</th>\n",
       "      <th>FatigueYear</th>\n",
       "      <th>BirthMonthYear</th>\n",
       "      <th>UnexplainedWeightLossMonthYear</th>\n",
       "      <th>PolyuriaMonthYear</th>\n",
       "      <th>PolyphagiaMonthYear</th>\n",
       "      <th>PolydipsiaMonthYear</th>\n",
       "      <th>KetoacidosisMonthYear23</th>\n",
       "      <th>FatigueMonthYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8860</th>\n",
       "      <td>362328</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>Apr</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>Apr</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr 2007</td>\n",
       "      <td>Apr 2007</td>\n",
       "      <td>Apr 2007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10960</th>\n",
       "      <td>402858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Feb</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Feb 2010</td>\n",
       "      <td>Feb 2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Feb 2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17659</th>\n",
       "      <td>526619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24328</th>\n",
       "      <td>648931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr 2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apr 2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26397</th>\n",
       "      <td>686820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jan</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jan 2012</td>\n",
       "      <td>Jan 2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jan 2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jan 2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MaskID BirthMonth  BirthYear UnexplainedWeightLossMonth  \\\n",
       "8860   362328        NaN        NaN                        NaN   \n",
       "10960  402858        NaN        NaN                        Feb   \n",
       "17659  526619        NaN        NaN                        NaN   \n",
       "24328  648931        NaN        NaN                        NaN   \n",
       "26397  686820        NaN        NaN                        Jan   \n",
       "\n",
       "       UnexplainedWeightLossYear PolyuriaMonth  PolyuriaYear PolyphagiaMonth  \\\n",
       "8860                         NaN           Apr        2007.0             Apr   \n",
       "10960                     2010.0           Feb        2010.0             NaN   \n",
       "17659                        NaN           Jul        2017.0             NaN   \n",
       "24328                        NaN           Apr        2012.0             NaN   \n",
       "26397                     2012.0           Jan        2012.0             NaN   \n",
       "\n",
       "       PolyphagiaYear PolydipsiaMonth  ...  KetoacidosisYear23 FatigueMonth  \\\n",
       "8860           2007.0             Apr  ...                 NaN          Apr   \n",
       "10960             NaN             Feb  ...                 NaN          NaN   \n",
       "17659             NaN             NaN  ...                 NaN          Jul   \n",
       "24328             NaN             Apr  ...                 NaN          NaN   \n",
       "26397             NaN             Jan  ...                 NaN          Jan   \n",
       "\n",
       "       FatigueYear BirthMonthYear  UnexplainedWeightLossMonthYear  \\\n",
       "8860        2007.0            NaN                             NaN   \n",
       "10960          NaN            NaN                        Feb 2010   \n",
       "17659       2017.0            NaN                             NaN   \n",
       "24328          NaN            NaN                             NaN   \n",
       "26397       2012.0            NaN                        Jan 2012   \n",
       "\n",
       "      PolyuriaMonthYear PolyphagiaMonthYear PolydipsiaMonthYear  \\\n",
       "8860           Apr 2007            Apr 2007            Apr 2007   \n",
       "10960          Feb 2010                 NaN            Feb 2010   \n",
       "17659          Jul 2017                 NaN                 NaN   \n",
       "24328          Apr 2012                 NaN            Apr 2012   \n",
       "26397          Jan 2012                 NaN            Jan 2012   \n",
       "\n",
       "      KetoacidosisMonthYear23 FatigueMonthYear  \n",
       "8860                      NaN         Apr 2007  \n",
       "10960                     NaN              NaN  \n",
       "17659                     NaN         Jul 2017  \n",
       "24328                     NaN              NaN  \n",
       "26397                     NaN         Jan 2012  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Assuming 'df_date_pair' is your DataFrame and 'column_pairs' is defined as before\n",
    "\n",
    "# Function to convert month and year to a datetime string\n",
    "def convert_to_datetime(row, month_col, year_col):\n",
    "    try:\n",
    "        # Check if the month and year are not missing\n",
    "        if pd.notna(row[month_col]) and pd.notna(row[year_col]):\n",
    "            month = row[month_col].capitalize()  # Capitalizing to match standard abbreviations like Jan, Feb, etc.\n",
    "            year = int(row[year_col])\n",
    "            return pd.to_datetime(f\"{month} {year}\", format='%b %Y').strftime('%b %Y')\n",
    "        else:\n",
    "            return np.nan\n",
    "    except (ValueError, TypeError):\n",
    "        return np.nan  # Return np.nan if there's an error in conversion\n",
    "\n",
    "for month_col, year_col in column_pairs:\n",
    "    new_col_name = month_col + \"Year\" if \"23\" not in month_col else month_col.replace(\"23\", \"\") + \"Year23\"\n",
    "\n",
    "    # Apply the conversion function to each row\n",
    "    df_date_pair[new_col_name] = df_date_pair.apply(convert_to_datetime, axis=1, args=(month_col, year_col))\n",
    "\n",
    "df_date_pair = df_date_pair.drop_duplicates()\n",
    "print(df_date_pair.info())\n",
    "df_date_pair.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d05b3365",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_pair = df_date_pair.drop_duplicates()\n",
    "df_date_pair_sorted = df_date_pair.sort_index(axis=1)\n",
    "df_date_pair_sorted.to_csv('date_pair_month_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf5eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5de4221",
   "metadata": {},
   "source": [
    "### 1.2.3 Date merge for column with day, mon, yea"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3e6e446",
   "metadata": {},
   "source": [
    "3) With day, mon, yea\n",
    "'MechanisticSampleCollectionDay', 'MechanisticSampleCollectionMon', 'MechanisticSampleCollectionYea'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "294dce43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  15 columns:  ['MechanisticSampleCollectionDay', 'MechanisticSampleCollectionMon', 'MechanisticSampleCollectionYea', 'MechanisticSampleCollectionDay_dup15', 'MechanisticSampleCollectionMon_dup15', 'MechanisticSampleCollectionYea_dup15', 'MechanisticSampleCollectionDay_dup16', 'MechanisticSampleCollectionMon_dup16', 'MechanisticSampleCollectionYea_dup16', 'MechanisticSampleCollectionDay_dup21', 'MechanisticSampleCollectionMon_dup21', 'MechanisticSampleCollectionYea_dup21', 'MechanisticSampleCollectionDay_dup22', 'MechanisticSampleCollectionMon_dup22', 'MechanisticSampleCollectionYea_dup22']\n",
      "Top 20 values for column MechanisticSampleCollectionDay:\n",
      "MechanisticSampleCollectionDay\n",
      "17.0    34\n",
      "15.0    30\n",
      "10.0    29\n",
      "16.0    28\n",
      "28.0    26\n",
      "11.0    23\n",
      "4.0     23\n",
      "7.0     23\n",
      "21.0    23\n",
      "19.0    23\n",
      "12.0    21\n",
      "18.0    21\n",
      "14.0    19\n",
      "20.0    19\n",
      "1.0     19\n",
      "8.0     18\n",
      "9.0     18\n",
      "6.0     18\n",
      "25.0    18\n",
      "5.0     17\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column MechanisticSampleCollectionMon:\n",
      "MechanisticSampleCollectionMon\n",
      "Nov    57\n",
      "Apr    55\n",
      "Jun    55\n",
      "Aug    53\n",
      "Jul    53\n",
      "Mar    52\n",
      "Feb    51\n",
      "May    51\n",
      "Oct    48\n",
      "Sep    47\n",
      "Jan    46\n",
      "Dec    38\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column MechanisticSampleCollectionYea:\n",
      "MechanisticSampleCollectionYea\n",
      "2007.0    162\n",
      "2008.0    156\n",
      "2006.0    142\n",
      "2005.0    129\n",
      "2004.0     17\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column MechanisticSampleCollectionDay_dup15:\n",
      "MechanisticSampleCollectionDay_dup15\n",
      "7.0     145\n",
      "20.0    144\n",
      "21.0    139\n",
      "27.0    138\n",
      "22.0    137\n",
      "19.0    136\n",
      "16.0    135\n",
      "18.0    134\n",
      "23.0    131\n",
      "9.0     129\n",
      "5.0     127\n",
      "10.0    126\n",
      "11.0    126\n",
      "17.0    126\n",
      "3.0     121\n",
      "29.0    121\n",
      "1.0     120\n",
      "2.0     120\n",
      "6.0     120\n",
      "8.0     119\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column MechanisticSampleCollectionMon_dup15:\n",
      "MechanisticSampleCollectionMon_dup15\n",
      "Apr    364\n",
      "Mar    342\n",
      "Dec    339\n",
      "May    336\n",
      "Jun    329\n",
      "Jul    323\n",
      "Feb    313\n",
      "Jan    303\n",
      "Aug    301\n",
      "Nov    288\n",
      "Oct    288\n",
      "Sep    191\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column MechanisticSampleCollectionYea_dup15:\n",
      "MechanisticSampleCollectionYea_dup15\n",
      "2011.0    749\n",
      "2010.0    661\n",
      "2009.0    582\n",
      "2008.0    556\n",
      "2007.0    440\n",
      "2012.0    358\n",
      "2006.0    268\n",
      "2005.0     92\n",
      "2004.0      3\n",
      "2040.0      1\n",
      "2001.0      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column MechanisticSampleCollectionDay_dup16:\n",
      "MechanisticSampleCollectionDay_dup16\n",
      "26.0    10\n",
      "11.0     8\n",
      "23.0     8\n",
      "5.0      7\n",
      "13.0     7\n",
      "21.0     7\n",
      "7.0      7\n",
      "20.0     7\n",
      "14.0     7\n",
      "22.0     6\n",
      "28.0     6\n",
      "25.0     6\n",
      "30.0     6\n",
      "17.0     6\n",
      "9.0      6\n",
      "6.0      5\n",
      "16.0     5\n",
      "3.0      5\n",
      "8.0      5\n",
      "10.0     5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column MechanisticSampleCollectionMon_dup16:\n",
      "MechanisticSampleCollectionMon_dup16\n",
      "Apr    28\n",
      "Jun    19\n",
      "May    19\n",
      "Mar    16\n",
      "Feb    13\n",
      "Jul    12\n",
      "Aug    10\n",
      "Sep     9\n",
      "Oct     9\n",
      "Dec     9\n",
      "Nov     6\n",
      "Jan     3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column MechanisticSampleCollectionYea_dup16:\n",
      "MechanisticSampleCollectionYea_dup16\n",
      "2009.0    53\n",
      "2010.0    45\n",
      "2011.0    34\n",
      "2012.0    17\n",
      "2008.0     3\n",
      "2006.0     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column MechanisticSampleCollectionDay_dup21:\n",
      "MechanisticSampleCollectionDay_dup21\n",
      "15.0    4\n",
      "28.0    4\n",
      "6.0     3\n",
      "3.0     3\n",
      "25.0    3\n",
      "5.0     3\n",
      "27.0    3\n",
      "21.0    3\n",
      "9.0     2\n",
      "7.0     2\n",
      "29.0    2\n",
      "12.0    2\n",
      "17.0    2\n",
      "11.0    2\n",
      "1.0     2\n",
      "22.0    2\n",
      "18.0    2\n",
      "13.0    2\n",
      "4.0     2\n",
      "19.0    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column MechanisticSampleCollectionMon_dup21:\n",
      "MechanisticSampleCollectionMon_dup21\n",
      "Oct    8\n",
      "Jul    8\n",
      "Aug    7\n",
      "Dec    6\n",
      "Mar    6\n",
      "Feb    5\n",
      "Apr    5\n",
      "Jan    4\n",
      "Nov    4\n",
      "Sep    3\n",
      "Jun    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column MechanisticSampleCollectionYea_dup21:\n",
      "MechanisticSampleCollectionYea_dup21\n",
      "2004.0    15\n",
      "2005.0    12\n",
      "2009.0    10\n",
      "2006.0     9\n",
      "2010.0     5\n",
      "2008.0     4\n",
      "2007.0     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column MechanisticSampleCollectionDay_dup22:\n",
      "MechanisticSampleCollectionDay_dup22\n",
      "23.0    7\n",
      "24.0    6\n",
      "25.0    5\n",
      "4.0     5\n",
      "20.0    5\n",
      "3.0     4\n",
      "17.0    4\n",
      "22.0    4\n",
      "13.0    3\n",
      "19.0    3\n",
      "5.0     3\n",
      "1.0     3\n",
      "14.0    2\n",
      "2.0     2\n",
      "9.0     2\n",
      "10.0    2\n",
      "21.0    2\n",
      "15.0    2\n",
      "8.0     2\n",
      "27.0    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column MechanisticSampleCollectionMon_dup22:\n",
      "MechanisticSampleCollectionMon_dup22\n",
      "Feb    11\n",
      "Jun     9\n",
      "Apr     9\n",
      "Oct     9\n",
      "Aug     8\n",
      "May     8\n",
      "Jul     7\n",
      "Mar     6\n",
      "Jan     3\n",
      "Dec     2\n",
      "Nov     2\n",
      "Sep     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column MechanisticSampleCollectionYea_dup22:\n",
      "MechanisticSampleCollectionYea_dup22\n",
      "2008.0    73\n",
      "2009.0     2\n",
      "2011.0     1\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Three Strings to search for: NOT\n",
    "string1 = \"MechanisticSampleCollectionDay\"  # First string\n",
    "string2 = \"MechanisticSampleCollectionMon\"  # Second string, change this to your second string\n",
    "string3 = \"MechanisticSampleCollectionYea\"\n",
    "\n",
    "# Convert the search strings to lower case\n",
    "search_string1_lower = string1.lower()\n",
    "search_string2_lower = string2.lower()\n",
    "search_string3_lower = string3.lower()\n",
    "\n",
    "# Find columns that contain both search strings, case-insensitive\n",
    "matching_columns = [col for col in merged_first_part.columns \n",
    "                    if search_string1_lower in col.lower() or search_string2_lower in col.lower() or search_string3_lower in col.lower()]\n",
    "\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "\n",
    "column_list2 = ['MaskID'] + matching_columns\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "filtered_df = merged_first_part[column_list2].dropna(how='all', subset=matching_columns)\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "for col in matching_columns:\n",
    "    print(f\"Top 20 values for column {col}:\")\n",
    "    print(merged_first_part[col].value_counts().nlargest(20))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1181d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert 'NA' strings to np.nan\n",
    "def convert_na(value):\n",
    "    if value == 'NA':\n",
    "        return np.nan\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "# Function to convert month from string to number, handling missing values\n",
    "def month_to_num(month_str):\n",
    "    if pd.isna(month_str):\n",
    "        return np.nan\n",
    "    try:\n",
    "        datetime_object = pd.to_datetime(month_str, format='%b')\n",
    "        return datetime_object.month\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "# Function to create a date column from day, month, and year columns, handling missing values\n",
    "def create_date_column(df, year_col, original_cols):\n",
    "    parts = year_col.split('Yea')\n",
    "    base_col_name = parts[0]\n",
    "    suffix = parts[1] if len(parts) > 1 else ''\n",
    "\n",
    "    day_col = base_col_name + 'Day' + suffix\n",
    "    month_col = base_col_name + 'Mon' + suffix\n",
    "\n",
    "    if day_col in original_cols and month_col in original_cols:\n",
    "        date_col = base_col_name + 'Date' + suffix\n",
    "        df[date_col] = df.apply(\n",
    "            lambda row: f\"{int(month_to_num(row[month_col])):02d}/{int(row[day_col]) if pd.notna(row[day_col]) else 15:02d}/{int(row[year_col])}\"\n",
    "            if pd.notna(row[month_col]) and pd.notna(row[year_col])\n",
    "            else np.nan, axis=1)\n",
    "    else:\n",
    "        print(f\"Matching columns for '{year_col}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af1699c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4606 entries, 9498 to 400806\n",
      "Data columns (total 21 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   MaskID                                 4606 non-null   int64  \n",
      " 1   MechanisticSampleCollectionDay         606 non-null    float64\n",
      " 2   MechanisticSampleCollectionMon         606 non-null    object \n",
      " 3   MechanisticSampleCollectionYea         606 non-null    float64\n",
      " 4   MechanisticSampleCollectionDay_dup15   3715 non-null   float64\n",
      " 5   MechanisticSampleCollectionMon_dup15   3717 non-null   object \n",
      " 6   MechanisticSampleCollectionYea_dup15   3711 non-null   float64\n",
      " 7   MechanisticSampleCollectionDay_dup16   153 non-null    float64\n",
      " 8   MechanisticSampleCollectionMon_dup16   153 non-null    object \n",
      " 9   MechanisticSampleCollectionYea_dup16   153 non-null    float64\n",
      " 10  MechanisticSampleCollectionDay_dup21   57 non-null     float64\n",
      " 11  MechanisticSampleCollectionMon_dup21   57 non-null     object \n",
      " 12  MechanisticSampleCollectionYea_dup21   57 non-null     float64\n",
      " 13  MechanisticSampleCollectionDay_dup22   76 non-null     float64\n",
      " 14  MechanisticSampleCollectionMon_dup22   76 non-null     object \n",
      " 15  MechanisticSampleCollectionYea_dup22   76 non-null     float64\n",
      " 16  MechanisticSampleCollectionDate        606 non-null    object \n",
      " 17  MechanisticSampleCollectionDate_dup15  3711 non-null   object \n",
      " 18  MechanisticSampleCollectionDate_dup16  153 non-null    object \n",
      " 19  MechanisticSampleCollectionDate_dup21  57 non-null     object \n",
      " 20  MechanisticSampleCollectionDate_dup22  76 non-null     object \n",
      "dtypes: float64(10), int64(1), object(10)\n",
      "memory usage: 791.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaskID</th>\n",
       "      <th>MechanisticSampleCollectionDay</th>\n",
       "      <th>MechanisticSampleCollectionMon</th>\n",
       "      <th>MechanisticSampleCollectionYea</th>\n",
       "      <th>MechanisticSampleCollectionDay_dup15</th>\n",
       "      <th>MechanisticSampleCollectionMon_dup15</th>\n",
       "      <th>MechanisticSampleCollectionYea_dup15</th>\n",
       "      <th>MechanisticSampleCollectionDay_dup16</th>\n",
       "      <th>MechanisticSampleCollectionMon_dup16</th>\n",
       "      <th>MechanisticSampleCollectionYea_dup16</th>\n",
       "      <th>...</th>\n",
       "      <th>MechanisticSampleCollectionMon_dup21</th>\n",
       "      <th>MechanisticSampleCollectionYea_dup21</th>\n",
       "      <th>MechanisticSampleCollectionDay_dup22</th>\n",
       "      <th>MechanisticSampleCollectionMon_dup22</th>\n",
       "      <th>MechanisticSampleCollectionYea_dup22</th>\n",
       "      <th>MechanisticSampleCollectionDate</th>\n",
       "      <th>MechanisticSampleCollectionDate_dup15</th>\n",
       "      <th>MechanisticSampleCollectionDate_dup16</th>\n",
       "      <th>MechanisticSampleCollectionDate_dup21</th>\n",
       "      <th>MechanisticSampleCollectionDate_dup22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9498</th>\n",
       "      <td>374069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Apr</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04/20/2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9604</th>\n",
       "      <td>376229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "      <td>May</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/31/2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11012</th>\n",
       "      <td>404247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Feb</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02/20/2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12923</th>\n",
       "      <td>440176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Mar</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03/23/2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13168</th>\n",
       "      <td>445279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>May</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>05/23/2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MaskID  MechanisticSampleCollectionDay MechanisticSampleCollectionMon  \\\n",
       "9498   374069                             NaN                            NaN   \n",
       "9604   376229                             NaN                            NaN   \n",
       "11012  404247                             NaN                            NaN   \n",
       "12923  440176                             NaN                            NaN   \n",
       "13168  445279                             NaN                            NaN   \n",
       "\n",
       "       MechanisticSampleCollectionYea  MechanisticSampleCollectionDay_dup15  \\\n",
       "9498                              NaN                                  20.0   \n",
       "9604                              NaN                                  31.0   \n",
       "11012                             NaN                                  20.0   \n",
       "12923                             NaN                                  23.0   \n",
       "13168                             NaN                                  23.0   \n",
       "\n",
       "      MechanisticSampleCollectionMon_dup15  \\\n",
       "9498                                   Apr   \n",
       "9604                                   May   \n",
       "11012                                  Feb   \n",
       "12923                                  Mar   \n",
       "13168                                  May   \n",
       "\n",
       "       MechanisticSampleCollectionYea_dup15  \\\n",
       "9498                                 2012.0   \n",
       "9604                                 2012.0   \n",
       "11012                                2012.0   \n",
       "12923                                2012.0   \n",
       "13168                                2012.0   \n",
       "\n",
       "       MechanisticSampleCollectionDay_dup16  \\\n",
       "9498                                    NaN   \n",
       "9604                                    NaN   \n",
       "11012                                   NaN   \n",
       "12923                                   NaN   \n",
       "13168                                   NaN   \n",
       "\n",
       "      MechanisticSampleCollectionMon_dup16  \\\n",
       "9498                                   NaN   \n",
       "9604                                   NaN   \n",
       "11012                                  NaN   \n",
       "12923                                  NaN   \n",
       "13168                                  NaN   \n",
       "\n",
       "       MechanisticSampleCollectionYea_dup16  ...  \\\n",
       "9498                                    NaN  ...   \n",
       "9604                                    NaN  ...   \n",
       "11012                                   NaN  ...   \n",
       "12923                                   NaN  ...   \n",
       "13168                                   NaN  ...   \n",
       "\n",
       "       MechanisticSampleCollectionMon_dup21  \\\n",
       "9498                                    NaN   \n",
       "9604                                    NaN   \n",
       "11012                                   NaN   \n",
       "12923                                   NaN   \n",
       "13168                                   NaN   \n",
       "\n",
       "      MechanisticSampleCollectionYea_dup21  \\\n",
       "9498                                   NaN   \n",
       "9604                                   NaN   \n",
       "11012                                  NaN   \n",
       "12923                                  NaN   \n",
       "13168                                  NaN   \n",
       "\n",
       "       MechanisticSampleCollectionDay_dup22  \\\n",
       "9498                                    NaN   \n",
       "9604                                    NaN   \n",
       "11012                                   NaN   \n",
       "12923                                   NaN   \n",
       "13168                                   NaN   \n",
       "\n",
       "       MechanisticSampleCollectionMon_dup22  \\\n",
       "9498                                    NaN   \n",
       "9604                                    NaN   \n",
       "11012                                   NaN   \n",
       "12923                                   NaN   \n",
       "13168                                   NaN   \n",
       "\n",
       "      MechanisticSampleCollectionYea_dup22  MechanisticSampleCollectionDate  \\\n",
       "9498                                   NaN                              NaN   \n",
       "9604                                   NaN                              NaN   \n",
       "11012                                  NaN                              NaN   \n",
       "12923                                  NaN                              NaN   \n",
       "13168                                  NaN                              NaN   \n",
       "\n",
       "      MechanisticSampleCollectionDate_dup15  \\\n",
       "9498                             04/20/2012   \n",
       "9604                             05/31/2012   \n",
       "11012                            02/20/2012   \n",
       "12923                            03/23/2012   \n",
       "13168                            05/23/2012   \n",
       "\n",
       "      MechanisticSampleCollectionDate_dup16  \\\n",
       "9498                                    NaN   \n",
       "9604                                    NaN   \n",
       "11012                                   NaN   \n",
       "12923                                   NaN   \n",
       "13168                                   NaN   \n",
       "\n",
       "      MechanisticSampleCollectionDate_dup21  \\\n",
       "9498                                    NaN   \n",
       "9604                                    NaN   \n",
       "11012                                   NaN   \n",
       "12923                                   NaN   \n",
       "13168                                   NaN   \n",
       "\n",
       "      MechanisticSampleCollectionDate_dup22  \n",
       "9498                                    NaN  \n",
       "9604                                    NaN  \n",
       "11012                                   NaN  \n",
       "12923                                   NaN  \n",
       "13168                                   NaN  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'NA' strings to np.nan\n",
    "for col in filtered_df.columns:\n",
    "    filtered_df[col] = filtered_df[col].apply(convert_na)\n",
    "\n",
    "original_cols = filtered_df.columns.tolist()\n",
    "\n",
    "# Iterating over columns and creating date columns\n",
    "for col in filtered_df.columns:\n",
    "    if 'Yea' in col:\n",
    "        create_date_column(filtered_df, col, original_cols)\n",
    "\n",
    "print(filtered_df.info())\n",
    "filtered_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f48cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.drop_duplicates()\n",
    "filtered_df.to_csv('date_Day_Mon_Yea.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a03380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e5baf72",
   "metadata": {},
   "source": [
    "### 1.2.4 date merge for day, month and year"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ee7edad",
   "metadata": {},
   "source": [
    "With day, month and year\n",
    "'SamplesCollectionDay', 'SamplesCollectionMonth', 'SamplesCollectionYear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "664541de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  15 columns:  ['MechanisticSampleCollectionDay', 'MechanisticSampleCollectionMon', 'MechanisticSampleCollectionYea', 'MechanisticSampleCollectionDay_dup15', 'MechanisticSampleCollectionMon_dup15', 'MechanisticSampleCollectionYea_dup15', 'MechanisticSampleCollectionDay_dup16', 'MechanisticSampleCollectionMon_dup16', 'MechanisticSampleCollectionYea_dup16', 'MechanisticSampleCollectionDay_dup21', 'MechanisticSampleCollectionMon_dup21', 'MechanisticSampleCollectionYea_dup21', 'MechanisticSampleCollectionDay_dup22', 'MechanisticSampleCollectionMon_dup22', 'MechanisticSampleCollectionYea_dup22']\n"
     ]
    }
   ],
   "source": [
    "# Three Strings to search for: NOT\n",
    "string1 = \"MechanisticSampleCollection\"  # First string\n",
    "#string2 = \"MechanisticSampleCollectionMon\"  # Second string, change this to your second string\n",
    "#string3 = \"MechanisticSampleCollectionYea\"\n",
    "\n",
    "# Convert the search strings to lower case\n",
    "search_string1_lower = string1.lower()\n",
    "#search_string2_lower = string2.lower()\n",
    "#search_string3_lower = string3.lower()\n",
    "\n",
    "# Find columns that contain both search strings, case-insensitive\n",
    "matching_columns = [col for col in merged_first_part.columns \n",
    "                    if search_string1_lower in col.lower()]\n",
    "\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "\n",
    "column_list2 = ['MaskID'] + matching_columns\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "filtered_df = merged_first_part[column_list2].dropna(how='all', subset=matching_columns)\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "#for col in matching_columns:\n",
    "#    print(f\"Top 20 values for column {col}:\")\n",
    "#    print(merged_first_part[col].value_counts().nlargest(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b16e99",
   "metadata": {},
   "source": [
    "#### Correct names of the following columns:\n",
    "1. PlasmaGlucoseday: PlasmaGlucoseDay\n",
    "2. pHday: pHDay\n",
    "3. SerumKetonesday: SerumKetonesDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "826cad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_first_part.rename(columns={'PlasmaGlucoseday': 'PlasmaGlucoseDay', 'pHday': 'pHDay', 'SerumKetonesday': 'SerumKetonesDay'}, inplace=True)\n",
    "file_path = \"/home/ec2-user/SageMaker/Team-5/TN_01_merged_data_1.csv\"\n",
    "merged_first_part.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f48929da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of the day columes to be merged:  55\n"
     ]
    }
   ],
   "source": [
    "day_column_list = [col for col in merged_first_part.columns \n",
    "                    if \"Day\" in col and 'MechanisticSampleCollectionDay' not in col]\n",
    "\n",
    "print(\"No. of the day columes to be merged: \", len(day_column_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74b13083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of month columes in wrong naming format:  0\n",
      "No. of year columes in wrong naming format:  0\n"
     ]
    }
   ],
   "source": [
    "# Create lists of column headers for the month and year based on the day\n",
    "month_column_list = [name.replace(\"Day\", \"Month\") for name in day_column_list]\n",
    "year_column_list = [name.replace(\"Day\", \"Year\") for name in day_column_list]\n",
    "\n",
    "#Check whether there are any columns in the dataframe not following the naming rules\n",
    "unmatched_month_columns = [name for name in month_column_list if name not in merged_first_part.columns]\n",
    "print(\"No. of month columes in wrong naming format: \", len(unmatched_month_columns))\n",
    "\n",
    "unmatched_year_columns = [name for name in year_column_list if name not in merged_first_part.columns]\n",
    "print(\"No. of year columes in wrong naming format: \", len(unmatched_year_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0a5d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = day_column_list + month_column_list + year_column_list\n",
    "column_list.sort()\n",
    "\n",
    "column_list2 = ['MaskID'] + column_list\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "filtered_df = merged_first_part[column_list2].dropna(how='all', subset=column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cca0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert 'NA' strings to np.nan\n",
    "def convert_na(value):\n",
    "    if value == 'NA':\n",
    "        return np.nan\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "# Function to convert month from string or number to numeric month, handling missing values\n",
    "def month_to_num(month):\n",
    "    if pd.isna(month):\n",
    "        return np.nan\n",
    "    try:\n",
    "        # Handle string representation\n",
    "        if isinstance(month, str):\n",
    "            datetime_object = pd.to_datetime(month, format='%b')\n",
    "            return datetime_object.month\n",
    "        # Handle numeric representation\n",
    "        elif isinstance(month, (int, float)):\n",
    "            return int(month)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "# Function to create a date column from day, month, and year columns, handling missing values\n",
    "def create_date_column(df, year_col, original_cols):\n",
    "    parts = year_col.split('Year')\n",
    "    base_col_name = parts[0]\n",
    "    suffix = parts[1] if len(parts) > 1 else ''\n",
    "\n",
    "    day_col = base_col_name + 'Day' + suffix\n",
    "    month_col = base_col_name + 'Month' + suffix\n",
    "\n",
    "    if day_col in original_cols and month_col in original_cols:\n",
    "        date_col = base_col_name + 'Date' + suffix\n",
    "        df[date_col] = df.apply(\n",
    "            lambda row: f\"{int(month_to_num(row[month_col])):02d}/{int(row[day_col]):02d}/{int(row[year_col])}\"\n",
    "            if pd.notna(month_to_num(row[month_col])) and pd.notna(row[day_col]) and pd.notna(row[year_col])\n",
    "            else np.nan, axis=1)\n",
    "    else:\n",
    "        print(f\"Matching columns for '{year_col}' not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cb8f669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 304383 entries, 0 to 463770\n",
      "Columns: 221 entries, MaskID to pHDate\n",
      "dtypes: float64(115), int64(1), object(105)\n",
      "memory usage: 515.5+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaskID</th>\n",
       "      <th>AEDateDeathDay</th>\n",
       "      <th>AEDateDeathMonth</th>\n",
       "      <th>AEDateDeathYear</th>\n",
       "      <th>AEFollowupDateDay</th>\n",
       "      <th>AEFollowupDateMonth</th>\n",
       "      <th>AEFollowupDateYear</th>\n",
       "      <th>AEOccurDtDay</th>\n",
       "      <th>AEOccurDtMonth</th>\n",
       "      <th>AEOccurDtYear</th>\n",
       "      <th>...</th>\n",
       "      <th>Spring2019ConsentDateDate_dup12</th>\n",
       "      <th>Spring2019ConsentDateDate_dup14</th>\n",
       "      <th>StudyDrugStartDateDate1_1</th>\n",
       "      <th>StudyDrugStartDateDate2_1</th>\n",
       "      <th>StudyDrugStopDateDate1_1</th>\n",
       "      <th>StudyDrugStopDateDate2_1</th>\n",
       "      <th>UrineKetonesDate</th>\n",
       "      <th>_15Aug2011ConsentDateDate</th>\n",
       "      <th>_22Jul2009ConsentSignedDate</th>\n",
       "      <th>pHDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07/01/2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200074</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04/23/2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200082</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06/13/2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07/23/2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 221 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MaskID  AEDateDeathDay  AEDateDeathMonth  AEDateDeathYear  \\\n",
       "0  200023             NaN               NaN              NaN   \n",
       "1  200071             NaN               NaN              NaN   \n",
       "2  200074             NaN               NaN              NaN   \n",
       "4  200082             NaN               NaN              NaN   \n",
       "5  200086             NaN               NaN              NaN   \n",
       "\n",
       "   AEFollowupDateDay  AEFollowupDateMonth  AEFollowupDateYear  AEOccurDtDay  \\\n",
       "0                NaN                  NaN                 NaN           NaN   \n",
       "1                NaN                  NaN                 NaN           NaN   \n",
       "2                NaN                  NaN                 NaN           NaN   \n",
       "4                NaN                  NaN                 NaN           NaN   \n",
       "5                NaN                  NaN                 NaN           NaN   \n",
       "\n",
       "   AEOccurDtMonth  AEOccurDtYear  ...  Spring2019ConsentDateDate_dup12  \\\n",
       "0             NaN            NaN  ...                              NaN   \n",
       "1             NaN            NaN  ...                              NaN   \n",
       "2             NaN            NaN  ...                              NaN   \n",
       "4             NaN            NaN  ...                              NaN   \n",
       "5             NaN            NaN  ...                              NaN   \n",
       "\n",
       "   Spring2019ConsentDateDate_dup14  StudyDrugStartDateDate1_1  \\\n",
       "0                              NaN                        NaN   \n",
       "1                              NaN                        NaN   \n",
       "2                              NaN                        NaN   \n",
       "4                              NaN                        NaN   \n",
       "5                              NaN                        NaN   \n",
       "\n",
       "   StudyDrugStartDateDate2_1  StudyDrugStopDateDate1_1  \\\n",
       "0                        NaN                       NaN   \n",
       "1                        NaN                       NaN   \n",
       "2                        NaN                       NaN   \n",
       "4                        NaN                       NaN   \n",
       "5                        NaN                       NaN   \n",
       "\n",
       "   StudyDrugStopDateDate2_1  UrineKetonesDate _15Aug2011ConsentDateDate  \\\n",
       "0                       NaN               NaN                07/01/2015   \n",
       "1                       NaN               NaN                       NaN   \n",
       "2                       NaN               NaN                04/23/2012   \n",
       "4                       NaN               NaN                06/13/2012   \n",
       "5                       NaN               NaN                07/23/2012   \n",
       "\n",
       "   _22Jul2009ConsentSignedDate  pHDate  \n",
       "0                          NaN     NaN  \n",
       "1                          NaN     NaN  \n",
       "2                          NaN     NaN  \n",
       "4                          NaN     NaN  \n",
       "5                          NaN     NaN  \n",
       "\n",
       "[5 rows x 221 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'NA' strings to np.nan\n",
    "for col in filtered_df.columns:\n",
    "    filtered_df[col] = filtered_df[col].apply(convert_na)\n",
    "\n",
    "original_cols = filtered_df.columns.tolist()\n",
    "\n",
    "# Iterating over columns and creating date columns\n",
    "for col in filtered_df.columns:\n",
    "    if 'Year' in col:\n",
    "        create_date_column(filtered_df, col, original_cols)\n",
    "\n",
    "print(filtered_df.info())\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f807ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.drop_duplicates()\n",
    "filtered_df_sorted = filtered_df.sort_index(axis=1)\n",
    "filtered_df_sorted.to_csv('date_Day_Month_Year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494ecc0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a3841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69ba86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ea0a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b2f47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c597e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c4cbce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656617d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f0c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e0eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782021f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0084415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e55aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256766e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a3fd45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f676b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a341a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db9e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bd93f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d97f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313f8508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f5cba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a943a59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b7bd47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e1da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a219b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c5b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3435e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0330e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d0279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5002a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c10d295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9546334f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d38d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f69028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4271fcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002ea71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81255d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c496df76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f10fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91797d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a43a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52cb1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1265e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c0d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc767d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685eae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db2a0705",
   "metadata": {},
   "source": [
    "### 1.2.5 time foramting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1a584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7824e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b0993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acde7ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d46de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8ac4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76aceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036d0cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f0d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca09ab16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429794b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a964ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35efe757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31882cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de3660d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596f097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67d855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b74b8df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62badf26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775edc87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb9849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9d2c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4531d9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56155ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82565803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9358bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ba2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02781d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d688c1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21cc9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa12e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35177245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd9378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde9aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d9878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c619e4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b11ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5f8497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5080eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ccfa7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c0221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd1e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ecf1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415a8e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42afa5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce998c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217060e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bec8917",
   "metadata": {},
   "source": [
    "# 2. Arrant values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017d12d1",
   "metadata": {},
   "source": [
    "## 2.1 height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19fa9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is to load the first part of merged dataset\n",
    "import pandas as pd\n",
    "from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "file_path = \"/home/ec2-user/SageMaker/Team-5/TN_01_merged_data_1.csv\"\n",
    "merged_first_part = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46ecb707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 519673 entries, 0 to 519672\n",
      "Columns: 1562 entries, MaskID to SignedConsentFor2017Protocol\n",
      "dtypes: float64(664), int64(1), object(897)\n",
      "memory usage: 6.0+ GB\n"
     ]
    }
   ],
   "source": [
    "merged_first_part.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "511463d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path_23 = \"/home/ec2-user/SageMaker/TEAM-5/TN_01_merged_data_23.csv\"\n",
    "#merged_part_23 = pd.read_csv(file_path_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a98da923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  10 columns:  ['Heightcm', 'Heightin', 'Heightcm_dup15', 'Heightin_dup15', 'Heightcm_dup16', 'Heightin_dup16', 'Heightcm_dup21', 'Heightin_dup21', 'Heightcm_dup22', 'Heightin_dup22']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 26425 entries, 368 to 400806\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   MaskID           26425 non-null  int64  \n",
      " 1   Visit_Dt         26424 non-null  object \n",
      " 2   BirthYear_dup21  114 non-null    float64\n",
      " 3   Heightcm         6322 non-null   float64\n",
      " 4   Heightin         777 non-null    float64\n",
      " 5   Heightcm_dup15   17899 non-null  float64\n",
      " 6   Heightin_dup15   1670 non-null   float64\n",
      " 7   Heightcm_dup16   471 non-null    float64\n",
      " 8   Heightin_dup16   22 non-null     float64\n",
      " 9   Heightcm_dup21   98 non-null     float64\n",
      " 10  Heightin_dup21   26 non-null     float64\n",
      " 11  Heightcm_dup22   150 non-null    float64\n",
      " 12  Heightin_dup22   4 non-null      float64\n",
      "dtypes: float64(11), int64(1), object(1)\n",
      "memory usage: 2.8+ MB\n",
      "None\n",
      "Top 20 values for column Heightcm:\n",
      "Heightcm\n",
      "160.0    48\n",
      "165.0    46\n",
      "170.0    44\n",
      "164.0    39\n",
      "168.0    37\n",
      "172.0    36\n",
      "163.0    36\n",
      "171.0    36\n",
      "161.0    35\n",
      "162.5    35\n",
      "158.0    34\n",
      "178.0    34\n",
      "167.0    34\n",
      "162.0    32\n",
      "169.0    32\n",
      "127.0    31\n",
      "173.0    30\n",
      "130.0    30\n",
      "174.0    29\n",
      "126.0    28\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Heightin:\n",
      "Heightin\n",
      "67.0    35\n",
      "64.0    25\n",
      "65.0    24\n",
      "62.0    22\n",
      "63.0    21\n",
      "68.0    21\n",
      "66.0    21\n",
      "70.0    18\n",
      "72.0    18\n",
      "69.0    17\n",
      "71.0    17\n",
      "61.0    11\n",
      "50.0    11\n",
      "48.0    10\n",
      "46.0    10\n",
      "67.5     8\n",
      "65.5     8\n",
      "68.5     8\n",
      "60.0     8\n",
      "63.5     7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Heightcm_dup15:\n",
      "Heightcm_dup15\n",
      "170.0    191\n",
      "165.0    164\n",
      "160.0    157\n",
      "162.0    157\n",
      "163.0    143\n",
      "173.0    139\n",
      "164.0    139\n",
      "167.0    136\n",
      "175.0    132\n",
      "169.0    127\n",
      "178.0    127\n",
      "161.0    126\n",
      "166.0    122\n",
      "158.0    120\n",
      "171.0    119\n",
      "172.0    118\n",
      "174.0    112\n",
      "176.0    105\n",
      "162.5    102\n",
      "168.0    101\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Heightin_dup15:\n",
      "Heightin_dup15\n",
      "67.0    72\n",
      "63.0    70\n",
      "66.0    67\n",
      "64.0    58\n",
      "68.0    51\n",
      "69.0    50\n",
      "72.0    46\n",
      "62.0    45\n",
      "70.0    38\n",
      "65.0    37\n",
      "71.0    33\n",
      "73.0    30\n",
      "60.0    22\n",
      "61.0    22\n",
      "65.5    21\n",
      "68.5    20\n",
      "53.0    19\n",
      "74.0    19\n",
      "66.5    19\n",
      "59.0    17\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Heightcm_dup16:\n",
      "Heightcm_dup16\n",
      "169.0    10\n",
      "163.0     7\n",
      "182.0     6\n",
      "164.6     4\n",
      "164.5     4\n",
      "173.0     4\n",
      "131.0     4\n",
      "163.8     4\n",
      "181.0     4\n",
      "163.3     4\n",
      "182.8     3\n",
      "144.2     3\n",
      "158.5     3\n",
      "166.4     3\n",
      "165.5     3\n",
      "168.0     3\n",
      "168.9     3\n",
      "168.3     3\n",
      "161.0     3\n",
      "166.8     3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Heightin_dup16:\n",
      "Heightin_dup16\n",
      "66.50    3\n",
      "41.00    1\n",
      "59.33    1\n",
      "51.50    1\n",
      "44.57    1\n",
      "66.00    1\n",
      "62.00    1\n",
      "63.66    1\n",
      "65.00    1\n",
      "66.10    1\n",
      "68.10    1\n",
      "71.10    1\n",
      "72.00    1\n",
      "57.00    1\n",
      "60.00    1\n",
      "40.90    1\n",
      "48.90    1\n",
      "59.80    1\n",
      "53.74    1\n",
      "52.80    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Heightcm_dup21:\n",
      "Heightcm_dup21\n",
      "160.0    4\n",
      "183.0    3\n",
      "187.5    2\n",
      "163.0    2\n",
      "162.0    2\n",
      "172.9    2\n",
      "166.0    2\n",
      "160.6    2\n",
      "176.0    2\n",
      "176.4    1\n",
      "163.9    1\n",
      "146.0    1\n",
      "176.2    1\n",
      "191.0    1\n",
      "180.2    1\n",
      "180.9    1\n",
      "170.5    1\n",
      "159.5    1\n",
      "171.0    1\n",
      "181.0    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Heightin_dup21:\n",
      "Heightin_dup21\n",
      "73.00    3\n",
      "63.00    2\n",
      "66.00    2\n",
      "64.00    1\n",
      "65.50    1\n",
      "70.00    1\n",
      "65.00    1\n",
      "45.00    1\n",
      "55.00    1\n",
      "69.75    1\n",
      "75.00    1\n",
      "67.50    1\n",
      "63.50    1\n",
      "64.25    1\n",
      "58.75    1\n",
      "72.50    1\n",
      "53.25    1\n",
      "69.00    1\n",
      "72.00    1\n",
      "63.30    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Heightcm_dup22:\n",
      "Heightcm_dup22\n",
      "166.8    3\n",
      "163.5    3\n",
      "154.8    2\n",
      "170.5    2\n",
      "142.0    2\n",
      "164.1    2\n",
      "133.5    2\n",
      "180.0    2\n",
      "158.5    2\n",
      "155.5    2\n",
      "163.0    2\n",
      "178.2    2\n",
      "164.4    2\n",
      "119.5    2\n",
      "159.5    2\n",
      "129.0    2\n",
      "157.9    2\n",
      "120.0    1\n",
      "129.6    1\n",
      "145.5    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Heightin_dup22:\n",
      "Heightin_dup22\n",
      "60.9    1\n",
      "59.5    1\n",
      "53.0    1\n",
      "63.0    1\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Strings to search for: OR\n",
    "string1 = \"heightcm\"  # First string\n",
    "string2 = \"heightin\"  # Second string, change this to your second string\n",
    "\n",
    "# Convert the search strings to lower case\n",
    "search_string1_lower = string1.lower()\n",
    "search_string2_lower = string2.lower()\n",
    "\n",
    "# Find columns that contain both search strings, case-insensitive\n",
    "matching_columns = [col for col in merged_first_part.columns \n",
    "                    if search_string1_lower in col.lower() or search_string2_lower in col.lower()]\n",
    "\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "matching_columns2 = ['MaskID', 'Visit_Dt', 'BirthYear_dup21'] + matching_columns\n",
    "filtered_df = merged_first_part[matching_columns2].dropna(how='all', subset=matching_columns)\n",
    "print(filtered_df.info())\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "for col in matching_columns:\n",
    "    print(f\"Top 20 values for column {col}:\")\n",
    "    print(merged_first_part[col].value_counts().nlargest(20))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ab693e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 26291 entries, 368 to 400806\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   MaskID           26291 non-null  int64  \n",
      " 1   Visit_Dt         26290 non-null  object \n",
      " 2   BirthYear_dup21  114 non-null    float64\n",
      " 3   Heightcm         6318 non-null   float64\n",
      " 4   Heightin         777 non-null    float64\n",
      " 5   Heightcm_dup15   17789 non-null  float64\n",
      " 6   Heightin_dup15   1650 non-null   float64\n",
      " 7   Heightcm_dup16   470 non-null    float64\n",
      " 8   Heightin_dup16   22 non-null     float64\n",
      " 9   Heightcm_dup21   98 non-null     float64\n",
      " 10  Heightin_dup21   26 non-null     float64\n",
      " 11  Heightcm_dup22   149 non-null    float64\n",
      " 12  Heightin_dup22   4 non-null      float64\n",
      "dtypes: float64(11), int64(1), object(1)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "filtered_df = filtered_df.drop_duplicates()\n",
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41a419f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('height_AddBirthYear.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e4821",
   "metadata": {},
   "source": [
    "## 2.2 weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ecbcb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  10 columns:  ['Weightkg', 'Weightlbs', 'Weightkg_dup15', 'Weightlbs_dup15', 'Weightkg_dup16', 'Weightlbs_dup16', 'Weightkg_dup21', 'Weightlbs_dup21', 'Weightkg_dup22', 'Weightlbs_dup22']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 28512 entries, 368 to 400806\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   MaskID           28512 non-null  int64  \n",
      " 1   Visit_Dt         28511 non-null  object \n",
      " 2   BirthYear_dup21  115 non-null    float64\n",
      " 3   Weightkg         6428 non-null   float64\n",
      " 4   Weightlbs        882 non-null    float64\n",
      " 5   Weightkg_dup15   19590 non-null  float64\n",
      " 6   Weightlbs_dup15  2319 non-null   float64\n",
      " 7   Weightkg_dup16   486 non-null    float64\n",
      " 8   Weightlbs_dup16  30 non-null     float64\n",
      " 9   Weightkg_dup21   95 non-null     float64\n",
      " 10  Weightlbs_dup21  34 non-null     float64\n",
      " 11  Weightkg_dup22   152 non-null    float64\n",
      " 12  Weightlbs_dup22  6 non-null      float64\n",
      "dtypes: float64(11), int64(1), object(1)\n",
      "memory usage: 3.0+ MB\n",
      "None\n",
      "Top 20 values for column Weightkg:\n",
      "Weightkg\n",
      "27.8    23\n",
      "20.0    22\n",
      "23.0    22\n",
      "33.0    22\n",
      "20.5    22\n",
      "22.0    22\n",
      "21.0    22\n",
      "40.0    21\n",
      "39.0    20\n",
      "70.0    20\n",
      "17.0    20\n",
      "55.0    19\n",
      "34.0    19\n",
      "24.5    19\n",
      "18.6    19\n",
      "18.5    19\n",
      "24.0    18\n",
      "25.0    18\n",
      "58.0    18\n",
      "57.0    18\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Weightlbs:\n",
      "Weightlbs\n",
      "55.0     7\n",
      "170.0    6\n",
      "180.0    6\n",
      "148.0    5\n",
      "35.0     5\n",
      "114.0    5\n",
      "162.0    5\n",
      "125.0    5\n",
      "145.0    5\n",
      "46.5     5\n",
      "135.0    5\n",
      "160.0    5\n",
      "64.0     5\n",
      "128.0    4\n",
      "41.0     4\n",
      "50.0     4\n",
      "181.0    4\n",
      "140.0    4\n",
      "127.0    4\n",
      "124.0    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Weightkg_dup15:\n",
      "Weightkg_dup15\n",
      "60.0    101\n",
      "58.0     77\n",
      "56.0     73\n",
      "65.0     72\n",
      "57.0     71\n",
      "66.0     66\n",
      "63.0     65\n",
      "53.0     62\n",
      "62.0     62\n",
      "54.0     60\n",
      "70.0     59\n",
      "55.0     56\n",
      "72.0     55\n",
      "71.0     55\n",
      "75.0     52\n",
      "52.0     52\n",
      "67.0     51\n",
      "51.0     50\n",
      "68.0     50\n",
      "61.0     48\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Weightlbs_dup15:\n",
      "Weightlbs_dup15\n",
      "130.0    26\n",
      "140.0    24\n",
      "160.0    22\n",
      "145.0    22\n",
      "180.0    21\n",
      "190.0    20\n",
      "175.0    19\n",
      "200.0    18\n",
      "135.0    18\n",
      "165.0    17\n",
      "170.0    17\n",
      "155.0    16\n",
      "150.0    15\n",
      "110.0    15\n",
      "138.0    15\n",
      "120.0    14\n",
      "210.0    13\n",
      "128.0    13\n",
      "125.0    12\n",
      "220.0    12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Weightkg_dup16:\n",
      "Weightkg_dup16\n",
      "52.2    6\n",
      "56.0    5\n",
      "72.8    4\n",
      "68.2    4\n",
      "28.2    4\n",
      "62.0    4\n",
      "55.0    4\n",
      "57.0    3\n",
      "45.6    3\n",
      "38.3    3\n",
      "36.2    3\n",
      "69.2    3\n",
      "73.1    3\n",
      "81.6    3\n",
      "66.4    3\n",
      "73.0    3\n",
      "52.4    3\n",
      "31.2    3\n",
      "63.2    3\n",
      "67.3    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Weightlbs_dup16:\n",
      "Weightlbs_dup16\n",
      "41.00     1\n",
      "150.13    1\n",
      "84.40     1\n",
      "51.10     1\n",
      "39.02     1\n",
      "55.00     1\n",
      "82.70     1\n",
      "286.38    1\n",
      "62.17     1\n",
      "175.10    1\n",
      "188.40    1\n",
      "355.80    1\n",
      "136.75    1\n",
      "146.40    1\n",
      "115.30    1\n",
      "182.00    1\n",
      "109.30    1\n",
      "65.00     1\n",
      "62.50     1\n",
      "59.50     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Weightkg_dup21:\n",
      "Weightkg_dup21\n",
      "44.8     2\n",
      "67.5     2\n",
      "56.5     2\n",
      "58.0     2\n",
      "49.0     2\n",
      "72.1     2\n",
      "60.6     2\n",
      "60.8     2\n",
      "67.2     2\n",
      "51.8     2\n",
      "83.5     1\n",
      "62.0     1\n",
      "47.6     1\n",
      "110.0    1\n",
      "86.1     1\n",
      "77.3     1\n",
      "66.3     1\n",
      "85.6     1\n",
      "49.2     1\n",
      "75.6     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Weightlbs_dup21:\n",
      "Weightlbs_dup21\n",
      "164.0    2\n",
      "66.0     2\n",
      "44.0     1\n",
      "174.6    1\n",
      "302.0    1\n",
      "175.0    1\n",
      "146.0    1\n",
      "121.0    1\n",
      "186.0    1\n",
      "142.0    1\n",
      "280.4    1\n",
      "124.0    1\n",
      "201.0    1\n",
      "116.0    1\n",
      "172.0    1\n",
      "129.0    1\n",
      "80.0     1\n",
      "258.0    1\n",
      "155.0    1\n",
      "109.0    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Weightkg_dup22:\n",
      "Weightkg_dup22\n",
      "35.3    3\n",
      "63.4    3\n",
      "50.6    3\n",
      "56.7    3\n",
      "45.0    3\n",
      "33.1    2\n",
      "92.1    2\n",
      "53.7    2\n",
      "69.8    2\n",
      "30.8    2\n",
      "56.0    2\n",
      "44.5    2\n",
      "24.6    2\n",
      "35.9    2\n",
      "65.1    2\n",
      "71.8    2\n",
      "53.1    1\n",
      "53.9    1\n",
      "56.1    1\n",
      "38.4    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column Weightlbs_dup22:\n",
      "Weightlbs_dup22\n",
      "96.6     1\n",
      "83.3     1\n",
      "118.6    1\n",
      "129.8    1\n",
      "62.1     1\n",
      "206.0    1\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Strings to search for: OR\n",
    "string1 = \"Weightkg\"  # First string\n",
    "string2 = \"Weightlbs\"  # Second string, change this to your second string\n",
    "\n",
    "# Convert the search strings to lower case\n",
    "search_string1_lower = string1.lower()\n",
    "search_string2_lower = string2.lower()\n",
    "\n",
    "# Find columns that contain both search strings, case-insensitive\n",
    "matching_columns = [col for col in merged_first_part.columns \n",
    "                    if search_string1_lower in col.lower() or search_string2_lower in col.lower()]\n",
    "\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "matching_columns2 = ['MaskID', 'Visit_Dt', 'BirthYear_dup21'] + matching_columns\n",
    "filtered_df = merged_first_part[matching_columns2].dropna(how='all', subset=matching_columns)\n",
    "print(filtered_df.info())\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "for col in matching_columns:\n",
    "    print(f\"Top 20 values for column {col}:\")\n",
    "    print(merged_first_part[col].value_counts().nlargest(20))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc25eb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 28367 entries, 368 to 400806\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   MaskID           28367 non-null  int64  \n",
      " 1   Visit_Dt         28366 non-null  object \n",
      " 2   BirthYear_dup21  115 non-null    float64\n",
      " 3   Weightkg         6423 non-null   float64\n",
      " 4   Weightlbs        882 non-null    float64\n",
      " 5   Weightkg_dup15   19471 non-null  float64\n",
      " 6   Weightlbs_dup15  2296 non-null   float64\n",
      " 7   Weightkg_dup16   485 non-null    float64\n",
      " 8   Weightlbs_dup16  30 non-null     float64\n",
      " 9   Weightkg_dup21   95 non-null     float64\n",
      " 10  Weightlbs_dup21  34 non-null     float64\n",
      " 11  Weightkg_dup22   151 non-null    float64\n",
      " 12  Weightlbs_dup22  6 non-null      float64\n",
      "dtypes: float64(11), int64(1), object(1)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "filtered_df = filtered_df.drop_duplicates()\n",
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98c91e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('weight_1_AddBirthYear.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab4a70f",
   "metadata": {},
   "source": [
    "#### create another table for availableWeightWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "539c8beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  6 columns:  ['f1availableWeightWeight', 'f1availableWeightUnits', 'f2availableWeightWeight', 'f2availableWeightUnits', 'f3availableWeightWeight', 'f3availableWeightUnits']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 46 entries, 24774 to 399096\n",
      "Data columns (total 8 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   MaskID                   46 non-null     int64  \n",
      " 1   Visit_Dt                 46 non-null     object \n",
      " 2   f1availableWeightWeight  46 non-null     float64\n",
      " 3   f1availableWeightUnits   40 non-null     object \n",
      " 4   f2availableWeightWeight  44 non-null     float64\n",
      " 5   f2availableWeightUnits   40 non-null     object \n",
      " 6   f3availableWeightWeight  11 non-null     float64\n",
      " 7   f3availableWeightUnits   10 non-null     object \n",
      "dtypes: float64(3), int64(1), object(4)\n",
      "memory usage: 3.2+ KB\n",
      "None\n",
      "Top 20 values for column f1availableWeightWeight:\n",
      "f1availableWeightWeight\n",
      "150.0    2\n",
      "0.0      2\n",
      "50.9     2\n",
      "143.5    1\n",
      "153.0    1\n",
      "124.0    1\n",
      "55.2     1\n",
      "198.0    1\n",
      "34.6     1\n",
      "103.0    1\n",
      "202.0    1\n",
      "160.0    1\n",
      "43.5     1\n",
      "12.9     1\n",
      "31.8     1\n",
      "76.0     1\n",
      "34.0     1\n",
      "38.5     1\n",
      "230.0    1\n",
      "39.8     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column f1availableWeightUnits:\n",
      "f1availableWeightUnits\n",
      "Kg     27\n",
      "lbs    13\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column f2availableWeightWeight:\n",
      "f2availableWeightWeight\n",
      "0.0      3\n",
      "192.0    2\n",
      "137.5    1\n",
      "146.0    1\n",
      "59.5     1\n",
      "124.0    1\n",
      "49.9     1\n",
      "84.0     1\n",
      "82.0     1\n",
      "180.0    1\n",
      "49.2     1\n",
      "39.0     1\n",
      "30.6     1\n",
      "74.0     1\n",
      "30.0     1\n",
      "34.8     1\n",
      "195.0    1\n",
      "39.7     1\n",
      "37.7     1\n",
      "52.1     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column f2availableWeightUnits:\n",
      "f2availableWeightUnits\n",
      "Kg     26\n",
      "lbs    14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column f3availableWeightWeight:\n",
      "f3availableWeightWeight\n",
      "77.40     1\n",
      "39.15     1\n",
      "35.00     1\n",
      "46.70     1\n",
      "36.90     1\n",
      "25.40     1\n",
      "76.30     1\n",
      "13.50     1\n",
      "37.10     1\n",
      "74.00     1\n",
      "212.80    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column f3availableWeightUnits:\n",
      "f3availableWeightUnits\n",
      "Kg     8\n",
      "lbs    2\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Strings to search for: OR\n",
    "string1 = \"availableWeightWeight\"  # First string\n",
    "string2 = \"availableWeightUnits\"  # Second string, change this to your second string\n",
    "\n",
    "# Convert the search strings to lower case\n",
    "search_string1_lower = string1.lower()\n",
    "search_string2_lower = string2.lower()\n",
    "\n",
    "# Find columns that contain both search strings, case-insensitive\n",
    "matching_columns = [col for col in merged_first_part.columns \n",
    "                    if search_string1_lower in col.lower() or search_string2_lower in col.lower()]\n",
    "\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "matching_columns2 = ['MaskID', 'Visit_Dt'] + matching_columns\n",
    "filtered_df = merged_first_part[matching_columns2].dropna(how='all', subset=matching_columns)\n",
    "print(filtered_df.info())\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "for col in matching_columns:\n",
    "    print(f\"Top 20 values for column {col}:\")\n",
    "    print(merged_first_part[col].value_counts().nlargest(20))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "448e298f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 46 entries, 24774 to 399096\n",
      "Data columns (total 8 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   MaskID                   46 non-null     int64  \n",
      " 1   Visit_Dt                 46 non-null     object \n",
      " 2   f1availableWeightWeight  46 non-null     float64\n",
      " 3   f1availableWeightUnits   40 non-null     object \n",
      " 4   f2availableWeightWeight  44 non-null     float64\n",
      " 5   f2availableWeightUnits   40 non-null     object \n",
      " 6   f3availableWeightWeight  11 non-null     float64\n",
      " 7   f3availableWeightUnits   10 non-null     object \n",
      "dtypes: float64(3), int64(1), object(4)\n",
      "memory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "filtered_df = filtered_df.drop_duplicates()\n",
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e761410",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('weight_2_availableWeight.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f09a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72236ba9",
   "metadata": {},
   "source": [
    "## 2.3 Gluocose values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d17efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "# The following code is to load the first part of merged dataset\n",
    "import pandas as pd\n",
    "from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "file_path = \"/home/ec2-user/SageMaker/Team-5/TN_01_merged_data_1.csv\"\n",
    "merged_first_part = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4bc3fb9",
   "metadata": {},
   "source": [
    "Approximate conversion factor of 18 between mg/dl and mmol/L: 1 mmol/L = 18 mg/dl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23fd37eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  78 columns:  ['PlasmaGlucoseResult', 'PlasmaGlucoseUnits', 'Result1_1', 'Units1_1', 'LowerReferenceRange1_1', 'HigherReferenceRange1_1', 'GlucoseType1_1', 'Result2_1', 'Units2_1', 'LowerReferenceRange2_1', 'HigherReferenceRange2_1', 'GlucoseType2_1', 'Result3_1', 'Units3_1', 'LowerReferenceRange3_1', 'HigherReferenceRange3_1', 'GlucoseType3_1', 'Result4_1', 'Units4_1', 'LowerReferenceRange4_1', 'HigherReferenceRange4_1', 'GlucoseType4_1', 'Result5_1', 'Units5_1', 'LowerReferenceRange5_1', 'HigherReferenceRange5_1', 'GlucoseType5_1', 'Result6_1', 'Units6_1', 'LowerReferenceRange6_1', 'HigherReferenceRange6_1', 'GlucoseType6_1', 'Result7_1', 'Units7_1', 'LowerReferenceRange7_1', 'HigherReferenceRange7_1', 'GlucoseType7_1', 'Result8_1', 'Units8_1', 'LowerReferenceRange8_1', 'HigherReferenceRange8_1', 'GlucoseType8_1', 'Result9_1', 'Units9_1', 'LowerReferenceRange9_1', 'HigherReferenceRange9_1', 'GlucoseType9_1', 'Result10_1', 'Units10_1', 'GlucoseType10_1', 'D1FastingPlasmaGlucResult1_1', 'D1FastingPlasmaGlucUnits1_1', 'D1FastingPlasmaGlucResult2_1', 'D1FastingPlasmaGlucUnits2_1', 'D1FastingPlasmaGlucResult3_1', 'D1FastingPlasmaGlucUnits3_1', 'D2aOGTTFastingGlucResult1_1', 'D2aOGTTFastingGlucUnits1_1', 'D2aOGTTFastingGlucResult2_1', 'D2aOGTTFastingGlucUnits2_1', 'D2b2HourGlucUnits2_1', 'D2b2HourGlucResult1_1', 'D2b2HourGlucUnits1_1', 'D2b2HourGlucResult2_1', 'D3RandomPlasmaGlucResult1_1', 'D3RandomPlasmaGlucUnits1_1', 'D2aFastingPlasmaGlucUnits3_1', 'D2b2HourGlucUnits3_1', 'D3RandomPlasmaGlucUnits2_1', 'D3RandomPlasmaGlucUnits3_1', 'D3RandomPlasmaGlucUnits4_1', 'D3RandomPlasmaGlucUnits5_1', 'D2aFastingPlasmaGlucResult3_1', 'D2b2HourGlucResult3_1', 'D3RandomPlasmaGlucResult2_1', 'D3RandomPlasmaGlucResult3_1', 'D3RandomPlasmaGlucResult4_1', 'D3RandomPlasmaGlucResult5_1']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1536 entries, 31 to 399148\n",
      "Data columns (total 78 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   PlasmaGlucoseResult            181 non-null    float64\n",
      " 1   PlasmaGlucoseUnits             182 non-null    object \n",
      " 2   Result1_1                      1124 non-null   float64\n",
      " 3   Units1_1                       1117 non-null   object \n",
      " 4   LowerReferenceRange1_1         428 non-null    float64\n",
      " 5   HigherReferenceRange1_1        460 non-null    float64\n",
      " 6   GlucoseType1_1                 1121 non-null   object \n",
      " 7   Result2_1                      821 non-null    float64\n",
      " 8   Units2_1                       802 non-null    object \n",
      " 9   LowerReferenceRange2_1         291 non-null    float64\n",
      " 10  HigherReferenceRange2_1        327 non-null    float64\n",
      " 11  GlucoseType2_1                 814 non-null    object \n",
      " 12  Result3_1                      414 non-null    float64\n",
      " 13  Units3_1                       401 non-null    object \n",
      " 14  LowerReferenceRange3_1         157 non-null    float64\n",
      " 15  HigherReferenceRange3_1        173 non-null    float64\n",
      " 16  GlucoseType3_1                 398 non-null    object \n",
      " 17  Result4_1                      227 non-null    float64\n",
      " 18  Units4_1                       218 non-null    object \n",
      " 19  LowerReferenceRange4_1         85 non-null     float64\n",
      " 20  HigherReferenceRange4_1        91 non-null     float64\n",
      " 21  GlucoseType4_1                 209 non-null    object \n",
      " 22  Result5_1                      53 non-null     float64\n",
      " 23  Units5_1                       51 non-null     object \n",
      " 24  LowerReferenceRange5_1         24 non-null     float64\n",
      " 25  HigherReferenceRange5_1        28 non-null     float64\n",
      " 26  GlucoseType5_1                 50 non-null     object \n",
      " 27  Result6_1                      16 non-null     float64\n",
      " 28  Units6_1                       16 non-null     object \n",
      " 29  LowerReferenceRange6_1         7 non-null      float64\n",
      " 30  HigherReferenceRange6_1        7 non-null      float64\n",
      " 31  GlucoseType6_1                 16 non-null     object \n",
      " 32  Result7_1                      11 non-null     float64\n",
      " 33  Units7_1                       11 non-null     object \n",
      " 34  LowerReferenceRange7_1         6 non-null      float64\n",
      " 35  HigherReferenceRange7_1        7 non-null      float64\n",
      " 36  GlucoseType7_1                 11 non-null     object \n",
      " 37  Result8_1                      4 non-null      float64\n",
      " 38  Units8_1                       4 non-null      object \n",
      " 39  LowerReferenceRange8_1         3 non-null      float64\n",
      " 40  HigherReferenceRange8_1        3 non-null      float64\n",
      " 41  GlucoseType8_1                 4 non-null      object \n",
      " 42  Result9_1                      2 non-null      float64\n",
      " 43  Units9_1                       2 non-null      object \n",
      " 44  LowerReferenceRange9_1         1 non-null      float64\n",
      " 45  HigherReferenceRange9_1        1 non-null      float64\n",
      " 46  GlucoseType9_1                 2 non-null      object \n",
      " 47  Result10_1                     1 non-null      float64\n",
      " 48  Units10_1                      1 non-null      object \n",
      " 49  GlucoseType10_1                1 non-null      object \n",
      " 50  D1FastingPlasmaGlucResult1_1   67 non-null     float64\n",
      " 51  D1FastingPlasmaGlucUnits1_1    73 non-null     object \n",
      " 52  D1FastingPlasmaGlucResult2_1   4 non-null      float64\n",
      " 53  D1FastingPlasmaGlucUnits2_1    5 non-null      object \n",
      " 54  D1FastingPlasmaGlucResult3_1   1 non-null      float64\n",
      " 55  D1FastingPlasmaGlucUnits3_1    2 non-null      object \n",
      " 56  D2aOGTTFastingGlucResult1_1    30 non-null     float64\n",
      " 57  D2aOGTTFastingGlucUnits1_1     33 non-null     object \n",
      " 58  D2aOGTTFastingGlucResult2_1    6 non-null      float64\n",
      " 59  D2aOGTTFastingGlucUnits2_1     6 non-null      object \n",
      " 60  D2b2HourGlucUnits2_1           5 non-null      object \n",
      " 61  D2b2HourGlucResult1_1          35 non-null     float64\n",
      " 62  D2b2HourGlucUnits1_1           36 non-null     object \n",
      " 63  D2b2HourGlucResult2_1          5 non-null      float64\n",
      " 64  D3RandomPlasmaGlucResult1_1    131 non-null    float64\n",
      " 65  D3RandomPlasmaGlucUnits1_1     136 non-null    object \n",
      " 66  D2aFastingPlasmaGlucUnits3_1   1 non-null      object \n",
      " 67  D2b2HourGlucUnits3_1           1 non-null      object \n",
      " 68  D3RandomPlasmaGlucUnits2_1     12 non-null     object \n",
      " 69  D3RandomPlasmaGlucUnits3_1     7 non-null      object \n",
      " 70  D3RandomPlasmaGlucUnits4_1     3 non-null      object \n",
      " 71  D3RandomPlasmaGlucUnits5_1     2 non-null      object \n",
      " 72  D2aFastingPlasmaGlucResult3_1  1 non-null      float64\n",
      " 73  D2b2HourGlucResult3_1          1 non-null      float64\n",
      " 74  D3RandomPlasmaGlucResult2_1    12 non-null     float64\n",
      " 75  D3RandomPlasmaGlucResult3_1    7 non-null      float64\n",
      " 76  D3RandomPlasmaGlucResult4_1    2 non-null      float64\n",
      " 77  D3RandomPlasmaGlucResult5_1    1 non-null      float64\n",
      "dtypes: float64(43), object(35)\n",
      "memory usage: 948.0+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor col in filtered_columns:\\n    print(f\"Top 20 values for column {col}:\")\\n    print(merged_first_part[col].value_counts().nlargest(20))\\n    print()\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in lower case for case-insensitive comparison\n",
    "# Example strings to include\n",
    "contain_list = ['result', 'unit', 'glucosetype', 'lowerreferencerange', 'higherreferencerange'] \n",
    "# Example strings to exclude\n",
    "not_contain_list = ['date', 'znt', 'time', 'sig', 'lymph', 'rbc', 'hemo', 'hema', 'mcv', 'plate', \n",
    "                    'neutro', 'eosino', 'gada', 'baso', 'ia2', 'urine', 'd5','d6', 'mch', 'wbc', \n",
    "                    'mono','miaa', 'within', 'ica', 'serum', 'aniong','hba','f3','f1','f2','ph']     \n",
    "\n",
    "# Filtering columns with case-insensitive comparison\n",
    "filtered_columns = [col for col in merged_first_part.columns if \n",
    "                    any(contain.lower() in col.lower() for contain in contain_list) and \n",
    "                    all(not_contain.lower() not in col.lower() for not_contain in not_contain_list)]\n",
    "\n",
    "print(\"Found \", len(filtered_columns), \"columns: \", filtered_columns)\n",
    "\n",
    "filtered_df = merged_first_part[filtered_columns].dropna(how='all')\n",
    "\n",
    "# Output the filtered DataFrame\n",
    "print(filtered_df.info())\n",
    "\"\"\"\n",
    "for col in filtered_columns:\n",
    "    print(f\"Top 20 values for column {col}:\")\n",
    "    print(merged_first_part[col].value_counts().nlargest(20))\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "253e2fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('GlucResult.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736225d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8615eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72d2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2326fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d6c32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502e5b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b35f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2670a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c5726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539030a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e84aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58112715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55583f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34406c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bd5a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282996f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26eaccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ba7a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e648437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35788613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ccd01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd079d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8383ce42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b82bda7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb7fc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee04c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be371c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f278f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af63751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ed22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918a73f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49675f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e0d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5e41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0051287e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5fc2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb13e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e478b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793def60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b60ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc7ac01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593c44f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b62d2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b71f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0512db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3614db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b566f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986315b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30efcbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4680ee1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5c0d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be440cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3744d9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee66feac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee6cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c825a8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0271510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45fbca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5773cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f866bba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8f498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482fe7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e196c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473bdf41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d86e556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2105a649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c73299b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd78b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878e930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b62d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d431b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64658d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d945973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420a31c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1034e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a79374e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb0bccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19433c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae98d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e46701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebb86d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "895b6cc1",
   "metadata": {},
   "source": [
    "# 3. testName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46ece5a",
   "metadata": {},
   "source": [
    "1. Values from both parts of merged datasets of TN_01.\n",
    "2. Find possible duplications, typos, synonyms, etc. \n",
    "3. Check whether we would like to turn that into category values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41a567",
   "metadata": {},
   "source": [
    "## 3.1 Collect values for test name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785261f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "# The following code is to load the first part of merged dataset\n",
    "import pandas as pd\n",
    "from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "file_path = \"/home/ec2-user/SageMaker/Team-5/TN_01_merged_data_1.csv\"\n",
    "merged_first_part = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9be529b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  23 columns:  ['testName1_1', 'testName2_1', 'testName3_1', 'testName4_1', 'testName5_1', 'testName6_1', 'testName7_1', 'testName8_1', 'testName9_1', 'testName10_1', 'testName11_1', 'testName12_1', 'testName13_1', 'testName14_1', 'testName15_1', 'testName16_1', 'testName17_1', 'testName18_1', 'testName19_1', 'testName20_1', 'testName21_1', 'testName22_1', 'testName23_1']\n",
      "DataFrame with filtered columns (rows with all missing values dropped):\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 47 entries, 53825 to 399148\n",
      "Data columns (total 23 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   testName1_1   46 non-null     object\n",
      " 1   testName2_1   14 non-null     object\n",
      " 2   testName3_1   8 non-null      object\n",
      " 3   testName4_1   6 non-null      object\n",
      " 4   testName5_1   5 non-null      object\n",
      " 5   testName6_1   4 non-null      object\n",
      " 6   testName7_1   4 non-null      object\n",
      " 7   testName8_1   3 non-null      object\n",
      " 8   testName9_1   3 non-null      object\n",
      " 9   testName10_1  3 non-null      object\n",
      " 10  testName11_1  2 non-null      object\n",
      " 11  testName12_1  2 non-null      object\n",
      " 12  testName13_1  2 non-null      object\n",
      " 13  testName14_1  2 non-null      object\n",
      " 14  testName15_1  1 non-null      object\n",
      " 15  testName16_1  1 non-null      object\n",
      " 16  testName17_1  1 non-null      object\n",
      " 17  testName18_1  1 non-null      object\n",
      " 18  testName19_1  1 non-null      object\n",
      " 19  testName20_1  1 non-null      object\n",
      " 20  testName21_1  1 non-null      object\n",
      " 21  testName22_1  1 non-null      object\n",
      " 22  testName23_1  1 non-null      object\n",
      "dtypes: object(23)\n",
      "memory usage: 8.8+ KB\n",
      "None\n",
      "Top 20 values for column testName1_1:\n",
      "testName1_1\n",
      "Ketones                           6\n",
      "Sodium                            2\n",
      "blood ketones                     2\n",
      "pH                                2\n",
      "Anion Gap                         2\n",
      "C-Peptide                         2\n",
      "Blood Ketones                     2\n",
      "Random plasma glucose             1\n",
      "Home test                         1\n",
      "S-IA2Ab                           1\n",
      "A1c at home                       1\n",
      "CGM                               1\n",
      "glycated Protein serum            1\n",
      "HbA1c (point of care analyser)    1\n",
      "HCO3                              1\n",
      "Urine, Ketones                    1\n",
      "urine glucose                     1\n",
      "Urine Screen                      1\n",
      "Urine Glucose                     1\n",
      "Glucose-Lab                       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName2_1:\n",
      "testName2_1\n",
      "whole blood glucose    1\n",
      "P-OHBUT                1\n",
      "PCo2                   1\n",
      "Urine                  1\n",
      "creatinine             1\n",
      "CO2                    1\n",
      "ketones                1\n",
      "potassium              1\n",
      "Urine Glucose          1\n",
      "Bicarbonate            1\n",
      "Potassium              1\n",
      "c-peptide              1\n",
      "Anti GAD Antibodies    1\n",
      "pH CAP                 1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName3_1:\n",
      "testName3_1\n",
      "Bicarb                      1\n",
      "Beta Hydroxybutyric Acid    1\n",
      "sodium                      1\n",
      "HCO3                        1\n",
      "chloride                    1\n",
      "Creatinine                  1\n",
      "TTG IgA Abs                 1\n",
      "PCO2 Cap                    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName4_1:\n",
      "testName4_1\n",
      "Lactate                           1\n",
      "Potassium                         1\n",
      "BHB                               1\n",
      "CO2                               1\n",
      "TSH                               1\n",
      "Islet Cell Cytoplasmic Autoabs    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName5_1:\n",
      "testName5_1\n",
      "serum ketones     1\n",
      "Chloride          1\n",
      "Calcium           1\n",
      "Free T4           1\n",
      "IA2 Antibodies    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName6_1:\n",
      "testName6_1\n",
      "Bicarbonate               1\n",
      "BUN                       1\n",
      "WBC                       1\n",
      "Blood gases pO2, blood    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName7_1:\n",
      "testName7_1\n",
      "Anion Gap            1\n",
      "Creatinine           1\n",
      "RBC                  1\n",
      "Meta / Myelocytes    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName8_1:\n",
      "testName8_1\n",
      "Urea       1\n",
      "Albumin    1\n",
      "HgB        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName9_1:\n",
      "testName9_1\n",
      "Beta Hydroxybutyrate    1\n",
      "Anion Gap               1\n",
      "HcT                     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName10_1:\n",
      "testName10_1\n",
      "venous gases PH    1\n",
      "Phosphorus         1\n",
      "MCV                1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName11_1:\n",
      "testName11_1\n",
      "PC02    1\n",
      "MCH     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName12_1:\n",
      "testName12_1\n",
      "HCO3    1\n",
      "MCHC    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName13_1:\n",
      "testName13_1\n",
      "Base Excess    1\n",
      "RDW            1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName14_1:\n",
      "testName14_1\n",
      "Urea         1\n",
      "Platelets    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName15_1:\n",
      "testName15_1\n",
      "IgA    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName16_1:\n",
      "testName16_1\n",
      "urine ketones    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName17_1:\n",
      "testName17_1\n",
      "CBC and diff normal    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName18_1:\n",
      "testName18_1\n",
      "BetaHydroxy    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName19_1:\n",
      "testName19_1\n",
      "venous gases PH    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName20_1:\n",
      "testName20_1\n",
      "PCO2    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName21_1:\n",
      "testName21_1\n",
      "HCO3    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName22_1:\n",
      "testName22_1\n",
      "Base excess    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column testName23_1:\n",
      "testName23_1\n",
      "urine glucose    1\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string = \"testName\"  # change this to the string you're searching for\n",
    "# Convert the search string to lower case (or upper case)\n",
    "search_string_lower = string.lower()\n",
    "\n",
    "# Find columns that contain the search string, case-insensitive\n",
    "matching_columns = [col for col in merged_first_part.columns if search_string_lower in col.lower()]\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "filtered_df = merged_first_part[matching_columns].dropna(how='all')\n",
    "print(\"DataFrame with filtered columns (rows with all missing values dropped):\")\n",
    "print()\n",
    "print(filtered_df.info())\n",
    "#print(filtered_df.head(30))\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "for col in matching_columns:\n",
    "    print(f\"Top 20 values for column {col}:\")\n",
    "    print(merged_first_part[col].value_counts().nlargest(20))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25ef0e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A totoal of  86 unique values across columns containing 'testName':\n",
      "['A1c at home', 'Albumin', 'Anion Gap', 'Anti GAD Antibodies', 'BHB', 'BUN', 'Base Excess', 'Base excess', 'Beta Hydroxybutyrate', 'Beta Hydroxybutyric Acid', 'Beta-Hydroxybutyrate', 'BetaHydroxy', 'Bicarb', 'Bicarbonate', 'Blood Ketones', 'Blood gases pO2, blood', 'Blood ketones', 'C-Peptide', 'CBC and diff normal', 'CGM', 'CGM data', 'CO2', 'Calcium', 'Chloride', 'Creatinine', 'Free T4', 'Fructosamine', 'Glucose-Lab', 'HCO3', 'HbA1c (point of care analyser)', 'HcT', 'HgB', 'Home test', 'IA2 Antibodies', 'IgA', 'Insulin, Fasting', 'Islet Cell Cytoplasmic Autoabs', 'Ketones', 'Keyton', 'Lactate', 'MCH', 'MCHC', 'MCV', 'Meta / Myelocytes', 'P-OHBUT', 'P-OHButyr', 'PC02', 'PCO2', 'PCO2 Cap', 'PCo2', 'Phosphorus', 'Platelets', 'Potassium', 'RANDOM PLASMA GLUCOSE', 'RBC', 'RDW', 'Random plasma glucose', 'S-IA2Ab', 'Sodium', 'TSH', 'TTG IgA Abs', 'U-Gluk-O', 'Urea', 'Urine', 'Urine Glucose', 'Urine Screen', 'Urine, Ketones', 'WBC', 'blood ketones', 'c-peptide', 'cB-pH', 'chloride', 'creatinine', 'glycated Protein serum', 'ketone', 'ketones', 'pH', 'pH CAP', 'ph', 'potassium', 'serum ketones', 'sodium', 'urine glucose', 'urine ketones', 'venous gases PH', 'whole blood glucose']\n"
     ]
    }
   ],
   "source": [
    "# Flatten the values from these columns into a single list, removing NaNs\n",
    "test_name_list_1 = filtered_df.apply(lambda x: x.dropna().tolist(), axis=1).sum()\n",
    "\n",
    "# Extract unique values\n",
    "test_name_1 = pd.unique(test_name_list_1)\n",
    "\n",
    "print(\"A totoal of \", len(test_name_1), \"unique values across columns containing 'testName':\")\n",
    "print(sorted(test_name_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3789229d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1968807 entries, 0 to 1968806\n",
      "Data columns (total 17 columns):\n",
      " #   Column                         Dtype  \n",
      "---  ------                         -----  \n",
      " 0   MaskID                         int64  \n",
      " 1   Date_of_Registration           object \n",
      " 2   Status                         object \n",
      " 3   Date_of_Study_Start            object \n",
      " 4   Date_of_Draw                   object \n",
      " 5   Event_Title                    object \n",
      " 6   Spec_Name                      object \n",
      " 7   SampleMaskID                   object \n",
      " 8   Test_Name                      object \n",
      " 9   Result                         object \n",
      " 10  Result_Type                    object \n",
      " 11  Visit                          object \n",
      " 12  Date_at_Test_Results_Reported  object \n",
      " 13  Date_at_Evaluation             object \n",
      " 14  Date_Received                  object \n",
      " 15  Date_Shipped                   object \n",
      " 16  LabID                          float64\n",
      "dtypes: float64(1), int64(1), object(15)\n",
      "memory usage: 255.4+ MB\n"
     ]
    }
   ],
   "source": [
    "#Check test_name in the second part of the data\n",
    "file_path_23 = \"/home/ec2-user/SageMaker/Team-5/TN_01_merged_data_23.csv\"\n",
    "merged_part_23 = pd.read_csv(file_path_23)\n",
    "\n",
    "merged_part_23.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97e165b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values and their counts in 'column_name':\n",
      "Test_Name\n",
      "MIAA       371707\n",
      "GAD65H     265604\n",
      "IA-2H      254721\n",
      "GAD65      118167\n",
      "ICA512     118166\n",
      "ICA         53008\n",
      "ZNT8        40283\n",
      "HbA1c       28798\n",
      "GLU120      26535\n",
      "GLU-10      25881\n",
      "GLU0        25866\n",
      "PEP-10      25861\n",
      "PEP0        25857\n",
      "PEP120      25720\n",
      "GLU60       25524\n",
      "PEP60       25497\n",
      "GLU30       25478\n",
      "PEP30       25451\n",
      "GLU90       25409\n",
      "PEP90       25380\n",
      "INST0       24827\n",
      "INST-10     24797\n",
      "INST120     24752\n",
      "INST60      22505\n",
      "INST90      22431\n",
      "INST30      22413\n",
      "HLA          7008\n",
      "DR4          6979\n",
      "DR3          6979\n",
      "HLAb         6978\n",
      "HLAa         6978\n",
      "INS120       1263\n",
      "INS0         1262\n",
      "INS-10       1107\n",
      "INS60          12\n",
      "INS90          11\n",
      "INS30           8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get unique values and their counts for Text_Name\n",
    "value_counts = merged_part_23['Test_Name'].value_counts()\n",
    "\n",
    "print(\"Unique values and their counts in 'column_name':\")\n",
    "print(value_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f5f3b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A totoal of  37 Unique values in 'Test_Name' as a list:\n",
      "['DR3', 'DR4', 'GAD65', 'GAD65H', 'GLU-10', 'GLU0', 'GLU120', 'GLU30', 'GLU60', 'GLU90', 'HLA', 'HLAa', 'HLAb', 'HbA1c', 'IA-2H', 'ICA', 'ICA512', 'INS-10', 'INS0', 'INS120', 'INS30', 'INS60', 'INS90', 'INST-10', 'INST0', 'INST120', 'INST30', 'INST60', 'INST90', 'MIAA', 'PEP-10', 'PEP0', 'PEP120', 'PEP30', 'PEP60', 'PEP90', 'ZNT8']\n"
     ]
    }
   ],
   "source": [
    "# Get unique values for Test_Name\n",
    "unique_test_name_2 = merged_part_23['Test_Name'].unique()\n",
    "\n",
    "# Convert to list\n",
    "unique_test_name_list_2 = list(unique_test_name_2)\n",
    "test_name_2 = [x for x in unique_test_name_list_2 if not pd.isna(x)]\n",
    "\n",
    "print(\"A totoal of \", len(test_name_2),\"Unique values in 'Test_Name' as a list:\")\n",
    "print(sorted(test_name_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4345b3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "840e9a5a",
   "metadata": {},
   "source": [
    "#### Are there any dictonaries to get standard terms for all those terms? If so, we could create mapping or algorithms to find standard terms based on similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "daf7a9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 86 entries, 0 to 85\n",
      "Data columns (total 2 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   test_name_data_1  86 non-null     object\n",
      " 1   test_name_data_2  37 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.5+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_name_data_1</th>\n",
       "      <th>test_name_data_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1c at home</td>\n",
       "      <td>DR3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albumin</td>\n",
       "      <td>DR4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anion Gap</td>\n",
       "      <td>GAD65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anti GAD Antibodies</td>\n",
       "      <td>GAD65H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BHB</td>\n",
       "      <td>GLU-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      test_name_data_1 test_name_data_2\n",
       "0          A1c at home              DR3\n",
       "1              Albumin              DR4\n",
       "2            Anion Gap            GAD65\n",
       "3  Anti GAD Antibodies           GAD65H\n",
       "4                  BHB           GLU-10"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert arrays to lists if they are NumPy arrays\n",
    "if isinstance(test_name_1, np.ndarray):\n",
    "    test_name_1 = test_name_1.tolist()\n",
    "if isinstance(test_name_2, np.ndarray):\n",
    "    test_name_2 = test_name_2.tolist()\n",
    "\n",
    "# Custom sort key function to handle np.nan\n",
    "def sort_key(item):\n",
    "    return (item is np.nan, item)\n",
    "\n",
    "# Sort the lists alphabetically, np.nan values will be placed at the end\n",
    "test_name_1_sorted = sorted(test_name_1, key=sort_key)\n",
    "test_name_2_sorted = sorted(test_name_2, key=sort_key)\n",
    "\n",
    "# Pad the shorter list with NaNs\n",
    "max_length = max(len(test_name_1_sorted), len(test_name_2_sorted))\n",
    "test_name_1_sorted += [np.nan] * (max_length - len(test_name_1_sorted))\n",
    "test_name_2_sorted += [np.nan] * (max_length - len(test_name_2_sorted))\n",
    "\n",
    "# Create DataFrame\n",
    "df_test_name = pd.DataFrame({'test_name_data_1': test_name_1_sorted, 'test_name_data_2': test_name_2_sorted})\n",
    "\n",
    "print(df_test_name.info())\n",
    "df_test_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6100ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_name.to_csv('test_name.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a61e61",
   "metadata": {},
   "source": [
    "## 3.2 Generate dictionary of standard terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46b57e",
   "metadata": {},
   "source": [
    "#### Check values with loincsnomed.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44bd55a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 86 entries, 0 to 85\n",
      "Data columns (total 2 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   test_name_data_1  86 non-null     object\n",
      " 1   test_name_data_2  37 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.5+ KB\n"
     ]
    }
   ],
   "source": [
    "test_name_df = pd.read_csv('test_name.csv')\n",
    "test_name_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fafbe33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total value count of test_name_list_1:  86\n",
      "Test name list #1:  ['A1c at home', 'Albumin', 'Anion Gap', 'Anti GAD Antibodies', 'BHB', 'BUN', 'Base Excess', 'Base excess', 'Beta Hydroxybutyrate', 'Beta Hydroxybutyric Acid', 'Beta-Hydroxybutyrate', 'BetaHydroxy', 'Bicarb', 'Bicarbonate', 'Blood Ketones', 'Blood gases pO2, blood', 'Blood ketones', 'C-Peptide', 'CBC and diff normal', 'CGM', 'CGM data', 'CO2', 'Calcium', 'Chloride', 'Creatinine', 'Free T4', 'Fructosamine', 'Glucose-Lab', 'HCO3', 'HbA1c (point of care analyser)', 'HcT', 'HgB', 'Home test', 'IA2 Antibodies', 'IgA', 'Insulin, Fasting', 'Islet Cell Cytoplasmic Autoabs', 'Ketones', 'Keyton', 'Lactate', 'MCH', 'MCHC', 'MCV', 'Meta / Myelocytes', 'P-OHBUT', 'P-OHButyr', 'PC02', 'PCO2', 'PCO2 Cap', 'PCo2', 'Phosphorus', 'Platelets', 'Potassium', 'RANDOM PLASMA GLUCOSE', 'RBC', 'RDW', 'Random plasma glucose', 'S-IA2Ab', 'Sodium', 'TSH', 'TTG IgA Abs', 'U-Gluk-O', 'Urea', 'Urine', 'Urine Glucose', 'Urine Screen', 'Urine, Ketones', 'WBC', 'blood ketones', 'c-peptide', 'cB-pH', 'chloride', 'creatinine', 'glycated Protein serum', 'ketone', 'ketones', 'pH', 'pH CAP', 'ph', 'potassium', 'serum ketones', 'sodium', 'urine glucose', 'urine ketones', 'venous gases PH', 'whole blood glucose']\n"
     ]
    }
   ],
   "source": [
    "test_name_list_1 = test_name_df['test_name_data_1'].tolist()\n",
    "print(\"Total value count of test_name_list_1: \", len(test_name_list_1))\n",
    "print(\"Test name list #1: \", test_name_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09a4e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correct those with duplications, typos, synonyms\n",
    "test_name_list_1_modified = ['HBA1c', 'Albumin', 'Anion Gap', 'Anti GAD Antibodies', 'Beta Hydroxybutyrate', 'Blood Urea Nitrogen', 'Base Excess', 'Base Excess', 'Beta Hydroxybutyrate', 'Beta Hydroxybutyrate', 'Beta Hydroxybutyrate', 'BetaHydroxy', 'HCO3', 'HCO3', 'Blood Ketones', 'Blood gases pO2, blood', 'Blood Ketones', \n",
    "                             'C-Peptide', 'CBC and diff normal', 'CGM', 'CGM', 'CO2', 'Calcium', 'Chloride', 'Creatinine', 'Free T4', 'Fructosamine', 'Glucose-Lab', 'HCO3', 'HbA1c (point of care analyser)', 'HcT', 'HgB', 'Home test', 'IA2 Antibodies', 'IgA', 'Insulin, Fasting', 'Islet Cell Cytoplasmic Autoabs', 'Ketones', 'Ketones', \n",
    "                             'Lactate', 'MCH', 'MCHC', 'MCV', 'Meta / Myelocytes', 'P-OHBUT', 'P-OHButyr', 'PCO2', 'PCO2', 'PCO2', 'PCO2', 'Phosphorus', 'Platelets', 'Potassium', 'Random Plasma Glucose', 'RBC', 'RDW', 'Random Plasma Glucose', 'S-IA2Ab', 'Sodium', 'TSH', 'TTG IgA Abs', 'U-Gluk-O', 'Urea', 'Urine', 'Urine Glucose', 'Urine', 'Urine Ketones', \n",
    "                             'WBC', 'Blood Ketones', 'C-Peptide', 'cB-pH', 'Chloride', 'Creatinine', 'Glycated Protein Serum', 'Ketones', 'Ketones', 'pH', 'pH', 'pH', 'Potassium', 'Serum Ketones', 'Sodium', 'Urine Glucose', 'Urine Ketones', 'Venous Gases PH', 'Whole Blood Glucose']\n",
    "# Unsure: A1c at home, Anti GAD Antibodies, BetaHydroxy, Blood gases pO2, blood, CGM:Continuous Glucose Monitoring, 'P-OHBUT', 'P-OHButyr', S-IA2Ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6ced761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test names in list_1:  60\n"
     ]
    }
   ],
   "source": [
    "unique_test_name_list_1 = set(test_name_list_1_modified)\n",
    "print(\"Total number of test names in list_1: \", len(unique_test_name_list_1))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c096242d",
   "metadata": {},
   "source": [
    "There 26 test names are duplicated terms or typos. The mapping will be used in the clean dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d93866b",
   "metadata": {},
   "source": [
    "## 3.3 Tests (testName) in the first part of the merged dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b54ee18",
   "metadata": {},
   "source": [
    "1). All testNames (23 in total) are from the same spread sheet: TN01_nh08_diabonset_current.csv\n",
    "2). One set of test has 6 components in the dataset: D5OtherTestResultDD, D5OtherTestResultMM, D5OtherTestResultYYYY, testName, D5OtherTestResultResult, D6OtherTestResultUnit\n",
    "3). There might be a typo. D6OtherTestResultUnit should be D5OtherTestResultUnit.\n",
    "4). There are 18 columns with D6OtherTestResultUnit, so 5 are missing: D6OtherTestResultUnit15_1, D6OtherTestResultUnit16_1, D6OtherTestResultUnit17_1, D6OtherTestResultUnit19_1, D6OtherTestResultUnit23_1.\n",
    "5). There are 22 columns with D5OtherTestResultResult, so the missing one is D5OtherTestResultResult17_1.\n",
    "6). In the data cleaning, we should combine the first 3 columns into one as the date.\n",
    "7). We might need to check arrant values for the last 3 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b32801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "# The following code is to load the first part of merged dataset\n",
    "import pandas as pd\n",
    "from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "file_path = \"/home/ec2-user/SageMaker/Team-5/TN_01_merged_data_1.csv\"\n",
    "merged_first_part = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8581760a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  22 columns:  ['D5OtherTestResultResult1_1', 'D5OtherTestResultResult2_1', 'D5OtherTestResultResult3_1', 'D5OtherTestResultResult4_1', 'D5OtherTestResultResult5_1', 'D5OtherTestResultResult6_1', 'D5OtherTestResultResult7_1', 'D5OtherTestResultResult8_1', 'D5OtherTestResultResult9_1', 'D5OtherTestResultResult10_1', 'D5OtherTestResultResult11_1', 'D5OtherTestResultResult12_1', 'D5OtherTestResultResult13_1', 'D5OtherTestResultResult14_1', 'D5OtherTestResultResult15_1', 'D5OtherTestResultResult16_1', 'D5OtherTestResultResult18_1', 'D5OtherTestResultResult19_1', 'D5OtherTestResultResult20_1', 'D5OtherTestResultResult21_1', 'D5OtherTestResultResult22_1', 'D5OtherTestResultResult23_1']\n",
      "DataFrame with filtered columns (rows with all missing values dropped):\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 47 entries, 53825 to 399148\n",
      "Data columns (total 22 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   D5OtherTestResultResult1_1   46 non-null     object \n",
      " 1   D5OtherTestResultResult2_1   14 non-null     object \n",
      " 2   D5OtherTestResultResult3_1   8 non-null      object \n",
      " 3   D5OtherTestResultResult4_1   6 non-null      object \n",
      " 4   D5OtherTestResultResult5_1   5 non-null      object \n",
      " 5   D5OtherTestResultResult6_1   4 non-null      float64\n",
      " 6   D5OtherTestResultResult7_1   4 non-null      float64\n",
      " 7   D5OtherTestResultResult8_1   3 non-null      float64\n",
      " 8   D5OtherTestResultResult9_1   3 non-null      float64\n",
      " 9   D5OtherTestResultResult10_1  3 non-null      float64\n",
      " 10  D5OtherTestResultResult11_1  2 non-null      float64\n",
      " 11  D5OtherTestResultResult12_1  2 non-null      float64\n",
      " 12  D5OtherTestResultResult13_1  2 non-null      object \n",
      " 13  D5OtherTestResultResult14_1  2 non-null      float64\n",
      " 14  D5OtherTestResultResult15_1  1 non-null      float64\n",
      " 15  D5OtherTestResultResult16_1  1 non-null      object \n",
      " 16  D5OtherTestResultResult18_1  1 non-null      float64\n",
      " 17  D5OtherTestResultResult19_1  1 non-null      float64\n",
      " 18  D5OtherTestResultResult20_1  1 non-null      float64\n",
      " 19  D5OtherTestResultResult21_1  1 non-null      float64\n",
      " 20  D5OtherTestResultResult22_1  1 non-null      float64\n",
      " 21  D5OtherTestResultResult23_1  1 non-null      object \n",
      "dtypes: float64(14), object(8)\n",
      "memory usage: 8.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "string = \"D5OtherTestResultResult\"  # change this to the string you're searching for\n",
    "# Convert the search string to lower case (or upper case)\n",
    "search_string_lower = string.lower()\n",
    "\n",
    "# Find columns that contain the search string, case-insensitive\n",
    "matching_columns = [col for col in merged_first_part.columns if search_string_lower in col.lower()]\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "filtered_df = merged_first_part[matching_columns].dropna(how='all')\n",
    "print(\"DataFrame with filtered columns (rows with all missing values dropped):\")\n",
    "print()\n",
    "print(filtered_df.info())\n",
    "#print(filtered_df.head(30))\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "#for col in matching_columns:\n",
    "#    print(f\"Top 20 values for column {col}:\")\n",
    "#    print(merged_first_part[col].value_counts().nlargest(20))\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb85a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Create DataFrames for each suffix with consistent column names\n",
    "def create_dataframe_for_suffix(df, suffix):\n",
    "    column_mapping = {\n",
    "        f'testName{suffix}': 'testName',\n",
    "        f'D5OtherTestResultResult{suffix}': 'D5OtherTestResultResult',\n",
    "        f'D6OtherTestResultUnit{suffix}': 'D6OtherTestResultUnit'\n",
    "    }\n",
    "    df_suffix = pd.DataFrame()\n",
    "    for original_col, new_col in column_mapping.items():\n",
    "        df_suffix[new_col] = df[original_col] if original_col in df.columns else np.nan\n",
    "    return df_suffix\n",
    "\n",
    "# Creating 23 DataFrames\n",
    "dataframes = []\n",
    "for i in range(23):\n",
    "    suffix = str(i+1) + \"_1\"\n",
    "    dataframes.append(create_dataframe_for_suffix(merged_first_part, suffix))\n",
    "\n",
    "# Step 2: Concatenate the corresponding columns across all DataFrames\n",
    "final_test_name = pd.concat([merged_first_part['testName'] for merged_first_part in dataframes]).reset_index(drop=True)\n",
    "final_d5 = pd.concat([merged_first_part['D5OtherTestResultResult'] for merged_first_part in dataframes]).reset_index(drop=True)\n",
    "final_d6 = pd.concat([merged_first_part['D6OtherTestResultUnit'] for merged_first_part in dataframes]).reset_index(drop=True)\n",
    "\n",
    "# Step 3: Combine into a single DataFrame\n",
    "final_df = pd.DataFrame({\n",
    "    'testName': final_test_name,\n",
    "    'D5OtherTestResultResult': final_d5,\n",
    "    'D6OtherTestResultUnit': final_d6\n",
    "})\n",
    "\n",
    "# Step 4: Clean and sort the resulting DataFrame\n",
    "final_df.dropna(how='all', inplace=True)\n",
    "final_df.sort_values(by=['testName', 'D5OtherTestResultResult'], inplace=True)\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25b96e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('tests in first part of the merged data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88886d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e9aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f206d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ecc322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564935c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db62991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff19c183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5dd062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d723960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cd081e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "406f1ab5",
   "metadata": {},
   "source": [
    "## 3.4 Tests in the second part of the merged dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3ddc49d",
   "metadata": {},
   "source": [
    "#Check test_name in the second part of the data\n",
    "file_path_23 = \"/home/ec2-user/SageMaker/Team-5/TN_01_merged_data_23.csv\"\n",
    "merged_part_23 = pd.read_csv(file_path_23)\n",
    "\n",
    "merged_part_23.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d85b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2279aa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1739223 entries, 66039 to 1962412\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Dtype \n",
      "---  ------       ----- \n",
      " 0   Test_Name    object\n",
      " 1   Result       object\n",
      " 2   Result_Type  object\n",
      "dtypes: object(3)\n",
      "memory usage: 53.1+ MB\n"
     ]
    }
   ],
   "source": [
    "selected_columns = ['Test_Name', 'Result', 'Result_Type'] \n",
    "\n",
    "# Drop rows with all missing values in the selected columns\n",
    "merged_part_23_test = merged_part_23.loc[:, selected_columns].dropna(how='all')\n",
    "\n",
    "sort_columns = ['Result_Type', 'Test_Name', 'Result']\n",
    "merged_part_23_test = merged_part_23_test.sort_values(by=sort_columns)\n",
    "merged_part_23_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c765b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_part_23_test.to_csv('tests in second part of the merged data.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12a608ac",
   "metadata": {},
   "source": [
    "Comments:\n",
    "1. find cases with arrant values.\n",
    "2. It might be possible to impute Result or Result_type based on Test_name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a12b69",
   "metadata": {},
   "source": [
    "# 4. ProtocolVersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c75d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  11 columns:  ['WhichProtocolVersion', 'ProtocolVersion', 'WhichProtocolVersion2', 'WhichProtocolVersion_dup12', 'ProtocolVersion_dup14', 'ProtocolVersion_dup15', 'ProtocolVersion_dup16', 'ProtocolVersion_dup17', 'ProtocolVersion_dup22', 'ProtocolVersion_dup24', 'ProtocolVersion_dup27']\n",
      "DataFrame with filtered columns (rows with all missing values dropped):\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 118075 entries, 0 to 519672\n",
      "Data columns (total 11 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   WhichProtocolVersion        32022 non-null  object \n",
      " 1   ProtocolVersion             18507 non-null  float64\n",
      " 2   WhichProtocolVersion2       1827 non-null   object \n",
      " 3   WhichProtocolVersion_dup12  11686 non-null  object \n",
      " 4   ProtocolVersion_dup14       1325 non-null   float64\n",
      " 5   ProtocolVersion_dup15       5963 non-null   float64\n",
      " 6   ProtocolVersion_dup16       77 non-null     float64\n",
      " 7   ProtocolVersion_dup17       3498 non-null   float64\n",
      " 8   ProtocolVersion_dup22       9 non-null      float64\n",
      " 9   ProtocolVersion_dup24       1469 non-null   float64\n",
      " 10  ProtocolVersion_dup27       68347 non-null  object \n",
      "dtypes: float64(7), object(4)\n",
      "memory usage: 10.8+ MB\n",
      "None\n",
      "Top 20 values for column WhichProtocolVersion:\n",
      "WhichProtocolVersion\n",
      "15August2011    31229\n",
      "22July2009        793\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column ProtocolVersion:\n",
      "ProtocolVersion\n",
      "3.0    15620\n",
      "4.0     2456\n",
      "2.0      431\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column WhichProtocolVersion2:\n",
      "WhichProtocolVersion2\n",
      "October 10, 2017    1241\n",
      "22February2019       586\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column WhichProtocolVersion_dup12:\n",
      "WhichProtocolVersion_dup12\n",
      "22February2019      9039\n",
      "October 10, 2017    2647\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column ProtocolVersion_dup14:\n",
      "ProtocolVersion_dup14\n",
      "3.0    798\n",
      "4.0    277\n",
      "2.0    250\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column ProtocolVersion_dup15:\n",
      "ProtocolVersion_dup15\n",
      "3.0    3325\n",
      "4.0    1571\n",
      "2.0    1067\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column ProtocolVersion_dup16:\n",
      "ProtocolVersion_dup16\n",
      "3.0    68\n",
      "4.0     5\n",
      "2.0     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column ProtocolVersion_dup17:\n",
      "ProtocolVersion_dup17\n",
      "3.0    1437\n",
      "2.0    1163\n",
      "4.0     898\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column ProtocolVersion_dup22:\n",
      "ProtocolVersion_dup22\n",
      "3.0    9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column ProtocolVersion_dup24:\n",
      "ProtocolVersion_dup24\n",
      "3.0    842\n",
      "4.0    408\n",
      "2.0    219\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 20 values for column ProtocolVersion_dup27:\n",
      "ProtocolVersion_dup27\n",
      "October 10, 2017    43855\n",
      "15August2011        24492\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string = \"ProtocolVersion\"  # change this to the string you're searching for\n",
    "# Convert the search string to lower case (or upper case)\n",
    "search_string_lower = string.lower()\n",
    "\n",
    "# Find columns that contain the search string, case-insensitive\n",
    "matching_columns = [col for col in merged_first_part.columns if search_string_lower in col.lower()]\n",
    "print(\"Found \", len(matching_columns), \"columns: \", matching_columns)\n",
    "\n",
    "# Create a DataFrame with the filtered columns and drop rows with only missing values\n",
    "filtered_df = merged_first_part[matching_columns].dropna(how='all')\n",
    "print(\"DataFrame with filtered columns (rows with all missing values dropped):\")\n",
    "print()\n",
    "print(filtered_df.info())\n",
    "#print(filtered_df.head(30))\n",
    "\n",
    "# Print top 20 values for each matching column\n",
    "for col in matching_columns:\n",
    "    print(f\"Top 20 values for column {col}:\")\n",
    "    print(merged_first_part[col].value_counts().nlargest(20))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c2ee26",
   "metadata": {},
   "source": [
    "#### Here are a couple points to be mentioned. 1). It seems there are two systems to indicate protocol versions, one using dates and the other using indices. If we could find the source of those protocol, we may generate a consistent protocol for all files in the system. 2). On the inconsistent date formats, it might be acceptable because they are actually used as labels instead of dates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616cee2f",
   "metadata": {},
   "source": [
    "# 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a7e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adafd2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8657f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fda0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa280f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4142511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010a79cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8834b528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707bec70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8333f442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f606d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6c989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd66c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2f992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0235f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41df5d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b04033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b5dbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f6ab05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa5149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb2f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159c7c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3147934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8e49d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189fd16b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d415c4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee5a7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06030f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006a8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc6d2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e985cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6baa3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec51f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b816f326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47c9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8377a62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90bd12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5df1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea4b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17434cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d2f9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa840d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1c088c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c900e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ebe04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd81465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7219537e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c10494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b85585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a88638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d728c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd55e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a4f0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f68bbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176a07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ced9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c44470b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3b7642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b824b662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cffe68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c298c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b234474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a757aed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330187e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938339ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb7f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acba561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b3850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c275e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e771d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f3378c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a3a95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb20ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba30350a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68caed4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27530321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40526a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eac9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4aa97e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e35d5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb2447c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b21db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d3f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca340a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20fdd7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a184cb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a37419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62caebc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485e7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fce634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31574d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
